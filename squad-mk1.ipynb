{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9165605,"sourceType":"datasetVersion","datasetId":5538075}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-13T15:55:57.250623Z","iopub.execute_input":"2024-08-13T15:55:57.250956Z","iopub.status.idle":"2024-08-13T15:55:59.238887Z","shell.execute_reply.started":"2024-08-13T15:55:57.250928Z","shell.execute_reply":"2024-08-13T15:55:59.237986Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/squad-translated/translated_squad2.0_ru.csv\n/kaggle/input/squad-translated/translated_squad2.0_hi.csv\n/kaggle/input/squad-translated/translated_squad2.0_ja.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install gtts\n!pip install pydub\n!pip install transformers torch\n!pip install torchaudio","metadata":{"execution":{"iopub.status.busy":"2024-08-13T15:55:59.241043Z","iopub.execute_input":"2024-08-13T15:55:59.241881Z","iopub.status.idle":"2024-08-13T15:56:50.294429Z","shell.execute_reply.started":"2024-08-13T15:55:59.241845Z","shell.execute_reply":"2024-08-13T15:56:50.293505Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting gtts\n  Downloading gTTS-2.5.2-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: requests<3,>=2.27 in /opt/conda/lib/python3.10/site-packages (from gtts) (2.32.3)\nRequirement already satisfied: click<8.2,>=7.1 in /opt/conda/lib/python3.10/site-packages (from gtts) (8.1.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->gtts) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->gtts) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->gtts) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->gtts) (2024.7.4)\nDownloading gTTS-2.5.2-py3-none-any.whl (29 kB)\nInstalling collected packages: gtts\nSuccessfully installed gtts-2.5.2\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (0.25.1)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchaudio) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchaudio) (2024.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchaudio) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchaudio) (1.3.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from gtts import gTTS\nimport io\nimport numpy as np\nfrom scipy.io import wavfile\nfrom pydub import AudioSegment\nfrom transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AdamW\nimport torch\nimport torchaudio\nfrom transformers import Wav2Vec2Processor, Wav2Vec2Model\nimport pandas as pd\nimport ast\nimport warnings\nimport logging\nimport torch.nn as nn\n\nlogging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-08-13T16:07:38.432171Z","iopub.execute_input":"2024-08-13T16:07:38.432530Z","iopub.status.idle":"2024-08-13T16:07:38.439727Z","shell.execute_reply.started":"2024-08-13T16:07:38.432502Z","shell.execute_reply":"2024-08-13T16:07:38.438846Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"\ndef tokenize_input(context, question, answer=None, max_length=100):\n    inputs = tokenizer.encode_plus(\n        question,\n        context,\n        add_special_tokens=True,  \n        return_tensors=\"pt\",       \n        padding='max_length',      \n        max_length=max_length,    \n        truncation=True,\n        return_overflowing_tokens=False\n    )\n    \n    if answer:\n        answer_dict = ast.literal_eval(answer)\n        \n        if answer_dict['text'] and answer_dict['answer_start']:\n            answer_text = answer_dict['text'][0]  \n            answer_start = answer_dict['answer_start'][0]  \n            \n            context_tokens_before_answer = tokenizer.encode(context[:answer_start], add_special_tokens=False)\n            answer_tokens = tokenizer.encode(answer_text, add_special_tokens=False)\n            \n            start_position = len(context_tokens_before_answer) + 1 \n            end_position = start_position + len(answer_tokens) - 1\n        else:\n            start_position = 0\n            end_position = 0\n        \n        inputs.update({'start_positions': torch.tensor([start_position]),\n                       'end_positions': torch.tensor([end_position])})\n    \n    return inputs","metadata":{"execution":{"iopub.status.busy":"2024-08-13T15:59:46.054114Z","iopub.execute_input":"2024-08-13T15:59:46.054469Z","iopub.status.idle":"2024-08-13T15:59:46.063225Z","shell.execute_reply.started":"2024-08-13T15:59:46.054441Z","shell.execute_reply":"2024-08-13T15:59:46.062348Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class QADataset(Dataset):\n    def __init__(self, contexts, questions, answers):\n        self.contexts = contexts\n        self.questions = questions\n        self.answers = answers\n    \n    def __len__(self):\n        return len(self.questions)\n    \n    def __getitem__(self, idx):\n        context = self.contexts[idx]\n        question = self.questions[idx]\n        answer = self.answers[idx]\n        inputs = tokenize_input(context, question, answer)\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2024-08-13T15:59:46.435781Z","iopub.execute_input":"2024-08-13T15:59:46.436119Z","iopub.status.idle":"2024-08-13T15:59:46.442109Z","shell.execute_reply.started":"2024-08-13T15:59:46.436093Z","shell.execute_reply":"2024-08-13T15:59:46.441217Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = BertForQuestionAnswering.from_pretrained('bert-base-multilingual-cased').to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T15:59:46.733748Z","iopub.execute_input":"2024-08-13T15:59:46.734486Z","iopub.status.idle":"2024-08-13T15:59:47.597947Z","shell.execute_reply.started":"2024-08-13T15:59:46.734461Z","shell.execute_reply":"2024-08-13T15:59:47.597128Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"df_hi = pd.read_csv('/kaggle/input/squad-translated/translated_squad2.0_hi.csv')\ndf_ru = pd.read_csv('/kaggle/input/squad-translated/translated_squad2.0_ru.csv')\ndf_ja =  pd.read_csv('/kaggle/input/squad-translated/translated_squad2.0_ja.csv')\n\ncontexts =  df_hi['context']\nquestions = df_hi['question']\nanswers = df_hi['answers']","metadata":{"execution":{"iopub.status.busy":"2024-08-13T15:59:47.599900Z","iopub.execute_input":"2024-08-13T15:59:47.600534Z","iopub.status.idle":"2024-08-13T15:59:47.783901Z","shell.execute_reply.started":"2024-08-13T15:59:47.600499Z","shell.execute_reply":"2024-08-13T15:59:47.782925Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"train_dataset = QADataset(contexts, questions, answers)\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n\n\noptimizer = AdamW(model.parameters(), lr=0.0001)\n\nmodel.train()\nfor epoch in range(3): \n    for i, batch in enumerate(train_dataloader):\n        \n        batch['input_ids'] = batch['input_ids'].squeeze(1)\n        batch['attention_mask'] = batch['attention_mask'].squeeze(1)\n        batch['token_type_ids'] = batch['token_type_ids'].squeeze(1)\n        batch['start_positions'] = batch['start_positions'].squeeze(1)\n        batch['end_positions'] = batch['end_positions'].squeeze(1)\n        batch.to(device)\n        optimizer.zero_grad()\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        \n        print(f'for batch {i+1}/{len(train_dataloader)}, epoch {epoch + 1}: loss = {loss.item()}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T15:59:47.785046Z","iopub.execute_input":"2024-08-13T15:59:47.785307Z","iopub.status.idle":"2024-08-13T16:02:34.021016Z","shell.execute_reply.started":"2024-08-13T15:59:47.785284Z","shell.execute_reply":"2024-08-13T16:02:34.020084Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","text":"for batch 1/326, epoch 1: loss = 4.461539268493652\nfor batch 2/326, epoch 1: loss = 4.41354513168335\nfor batch 3/326, epoch 1: loss = 4.601458549499512\nfor batch 4/326, epoch 1: loss = 4.66028356552124\nfor batch 5/326, epoch 1: loss = 4.483882904052734\nfor batch 6/326, epoch 1: loss = 4.41224479675293\nfor batch 7/326, epoch 1: loss = 4.699667453765869\nfor batch 8/326, epoch 1: loss = 4.517522811889648\nfor batch 9/326, epoch 1: loss = 4.50960111618042\nfor batch 10/326, epoch 1: loss = 4.7938127517700195\nfor batch 11/326, epoch 1: loss = 4.669885635375977\nfor batch 12/326, epoch 1: loss = 4.535154342651367\nfor batch 13/326, epoch 1: loss = 4.446372985839844\nfor batch 14/326, epoch 1: loss = 4.585618019104004\nfor batch 15/326, epoch 1: loss = 4.556934356689453\nfor batch 16/326, epoch 1: loss = 4.641994476318359\nfor batch 17/326, epoch 1: loss = 4.504547595977783\nfor batch 18/326, epoch 1: loss = 4.452451229095459\nfor batch 19/326, epoch 1: loss = 4.444398880004883\nfor batch 20/326, epoch 1: loss = 4.462831497192383\nfor batch 21/326, epoch 1: loss = 4.516706466674805\nfor batch 22/326, epoch 1: loss = 4.350987434387207\nfor batch 23/326, epoch 1: loss = 4.566498756408691\nfor batch 24/326, epoch 1: loss = 4.408441066741943\nfor batch 25/326, epoch 1: loss = 4.472630500793457\nfor batch 26/326, epoch 1: loss = 4.586846351623535\nfor batch 27/326, epoch 1: loss = 4.555422782897949\nfor batch 28/326, epoch 1: loss = 4.3480095863342285\nfor batch 29/326, epoch 1: loss = 4.443238258361816\nfor batch 30/326, epoch 1: loss = 4.465829849243164\nfor batch 31/326, epoch 1: loss = 4.485022068023682\nfor batch 32/326, epoch 1: loss = 4.473715305328369\nfor batch 33/326, epoch 1: loss = 4.585299491882324\nfor batch 34/326, epoch 1: loss = 4.508929252624512\nfor batch 35/326, epoch 1: loss = 4.581986427307129\nfor batch 36/326, epoch 1: loss = 4.489401340484619\nfor batch 37/326, epoch 1: loss = 4.3543195724487305\nfor batch 38/326, epoch 1: loss = 4.537950038909912\nfor batch 39/326, epoch 1: loss = 4.345486640930176\nfor batch 40/326, epoch 1: loss = 4.6382646560668945\nfor batch 41/326, epoch 1: loss = 4.538360118865967\nfor batch 42/326, epoch 1: loss = 4.509265422821045\nfor batch 43/326, epoch 1: loss = 4.5784454345703125\nfor batch 44/326, epoch 1: loss = 4.439431667327881\nfor batch 45/326, epoch 1: loss = 4.57643985748291\nfor batch 46/326, epoch 1: loss = 4.49046516418457\nfor batch 47/326, epoch 1: loss = 4.44905948638916\nfor batch 48/326, epoch 1: loss = 4.613706588745117\nfor batch 49/326, epoch 1: loss = 4.471025466918945\nfor batch 50/326, epoch 1: loss = 4.506285667419434\nfor batch 51/326, epoch 1: loss = 4.432896614074707\nfor batch 52/326, epoch 1: loss = 4.409030914306641\nfor batch 53/326, epoch 1: loss = 4.496723175048828\nfor batch 54/326, epoch 1: loss = 4.529453277587891\nfor batch 55/326, epoch 1: loss = 4.561542510986328\nfor batch 56/326, epoch 1: loss = 4.52421760559082\nfor batch 57/326, epoch 1: loss = 4.641672134399414\nfor batch 58/326, epoch 1: loss = 4.43826961517334\nfor batch 59/326, epoch 1: loss = 4.467057704925537\nfor batch 60/326, epoch 1: loss = 4.461928367614746\nfor batch 61/326, epoch 1: loss = 4.470667362213135\nfor batch 62/326, epoch 1: loss = 4.546786785125732\nfor batch 63/326, epoch 1: loss = 4.428853511810303\nfor batch 64/326, epoch 1: loss = 4.568399429321289\nfor batch 65/326, epoch 1: loss = 4.3917646408081055\nfor batch 66/326, epoch 1: loss = 4.412714004516602\nfor batch 67/326, epoch 1: loss = 4.502813339233398\nfor batch 68/326, epoch 1: loss = 4.416138648986816\nfor batch 69/326, epoch 1: loss = 4.514498233795166\nfor batch 70/326, epoch 1: loss = 4.541961669921875\nfor batch 71/326, epoch 1: loss = 4.474085330963135\nfor batch 72/326, epoch 1: loss = 4.449919700622559\nfor batch 73/326, epoch 1: loss = 4.544593811035156\nfor batch 74/326, epoch 1: loss = 4.513623237609863\nfor batch 75/326, epoch 1: loss = 4.555558204650879\nfor batch 76/326, epoch 1: loss = 4.493007659912109\nfor batch 77/326, epoch 1: loss = 4.499504089355469\nfor batch 78/326, epoch 1: loss = 4.443784236907959\nfor batch 79/326, epoch 1: loss = 4.547760963439941\nfor batch 80/326, epoch 1: loss = 4.599559783935547\nfor batch 81/326, epoch 1: loss = 4.492954254150391\nfor batch 82/326, epoch 1: loss = 4.43137264251709\nfor batch 83/326, epoch 1: loss = 4.607377052307129\nfor batch 84/326, epoch 1: loss = 4.5639753341674805\nfor batch 85/326, epoch 1: loss = 4.436762809753418\nfor batch 86/326, epoch 1: loss = 4.506458282470703\nfor batch 87/326, epoch 1: loss = 4.390398979187012\nfor batch 88/326, epoch 1: loss = 4.386704444885254\nfor batch 89/326, epoch 1: loss = 4.584649085998535\nfor batch 90/326, epoch 1: loss = 4.541093349456787\nfor batch 91/326, epoch 1: loss = 4.581425666809082\nfor batch 92/326, epoch 1: loss = 4.564253330230713\nfor batch 93/326, epoch 1: loss = 4.435421943664551\nfor batch 94/326, epoch 1: loss = 4.395125865936279\nfor batch 95/326, epoch 1: loss = 4.339058876037598\nfor batch 96/326, epoch 1: loss = 4.573024749755859\nfor batch 97/326, epoch 1: loss = 4.594301700592041\nfor batch 98/326, epoch 1: loss = 4.544785022735596\nfor batch 99/326, epoch 1: loss = 4.535934925079346\nfor batch 100/326, epoch 1: loss = 4.412392616271973\nfor batch 101/326, epoch 1: loss = 4.529897689819336\nfor batch 102/326, epoch 1: loss = 4.494734287261963\nfor batch 103/326, epoch 1: loss = 4.527639865875244\nfor batch 104/326, epoch 1: loss = 4.494325160980225\nfor batch 105/326, epoch 1: loss = 4.437507629394531\nfor batch 106/326, epoch 1: loss = 4.479325294494629\nfor batch 107/326, epoch 1: loss = 4.48304557800293\nfor batch 108/326, epoch 1: loss = 4.435367584228516\nfor batch 109/326, epoch 1: loss = 4.460976600646973\nfor batch 110/326, epoch 1: loss = 4.517765045166016\nfor batch 111/326, epoch 1: loss = 4.435901641845703\nfor batch 112/326, epoch 1: loss = 4.5036091804504395\nfor batch 113/326, epoch 1: loss = 4.457376480102539\nfor batch 114/326, epoch 1: loss = 4.433265686035156\nfor batch 115/326, epoch 1: loss = 4.394980430603027\nfor batch 116/326, epoch 1: loss = 4.573419094085693\nfor batch 117/326, epoch 1: loss = 4.476644039154053\nfor batch 118/326, epoch 1: loss = 4.5528178215026855\nfor batch 119/326, epoch 1: loss = 4.507919788360596\nfor batch 120/326, epoch 1: loss = 4.474555015563965\nfor batch 121/326, epoch 1: loss = 4.506725311279297\nfor batch 122/326, epoch 1: loss = 4.543615341186523\nfor batch 123/326, epoch 1: loss = 4.543975830078125\nfor batch 124/326, epoch 1: loss = 4.419914245605469\nfor batch 125/326, epoch 1: loss = 4.416142463684082\nfor batch 126/326, epoch 1: loss = 4.392421722412109\nfor batch 127/326, epoch 1: loss = 4.432875633239746\nfor batch 128/326, epoch 1: loss = 4.523237228393555\nfor batch 129/326, epoch 1: loss = 4.420399188995361\nfor batch 130/326, epoch 1: loss = 4.47817325592041\nfor batch 131/326, epoch 1: loss = 4.425784111022949\nfor batch 132/326, epoch 1: loss = 4.5410237312316895\nfor batch 133/326, epoch 1: loss = 4.618602275848389\nfor batch 134/326, epoch 1: loss = 4.509317398071289\nfor batch 135/326, epoch 1: loss = 4.367048263549805\nfor batch 136/326, epoch 1: loss = 4.54178524017334\nfor batch 137/326, epoch 1: loss = 4.579427719116211\nfor batch 138/326, epoch 1: loss = 4.437508583068848\nfor batch 139/326, epoch 1: loss = 4.445110321044922\nfor batch 140/326, epoch 1: loss = 4.5149126052856445\nfor batch 141/326, epoch 1: loss = 4.419967174530029\nfor batch 142/326, epoch 1: loss = 4.446349620819092\nfor batch 143/326, epoch 1: loss = 4.565659046173096\nfor batch 144/326, epoch 1: loss = 4.372525691986084\nfor batch 145/326, epoch 1: loss = 4.504214286804199\nfor batch 146/326, epoch 1: loss = 4.492425441741943\nfor batch 147/326, epoch 1: loss = 4.418179512023926\nfor batch 148/326, epoch 1: loss = 4.463258743286133\nfor batch 149/326, epoch 1: loss = 4.481293678283691\nfor batch 150/326, epoch 1: loss = 4.451437473297119\nfor batch 151/326, epoch 1: loss = 4.521397590637207\nfor batch 152/326, epoch 1: loss = 4.423301696777344\nfor batch 153/326, epoch 1: loss = 4.454608917236328\nfor batch 154/326, epoch 1: loss = 4.396175861358643\nfor batch 155/326, epoch 1: loss = 4.571737766265869\nfor batch 156/326, epoch 1: loss = 4.62545108795166\nfor batch 157/326, epoch 1: loss = 4.647878170013428\nfor batch 158/326, epoch 1: loss = 4.464519500732422\nfor batch 159/326, epoch 1: loss = 4.559229850769043\nfor batch 160/326, epoch 1: loss = 4.594139575958252\nfor batch 161/326, epoch 1: loss = 4.67915153503418\nfor batch 162/326, epoch 1: loss = 4.726088523864746\nfor batch 163/326, epoch 1: loss = 4.52052116394043\nfor batch 164/326, epoch 1: loss = 4.49857234954834\nfor batch 165/326, epoch 1: loss = 4.493913650512695\nfor batch 166/326, epoch 1: loss = 4.581818103790283\nfor batch 167/326, epoch 1: loss = 4.44382381439209\nfor batch 168/326, epoch 1: loss = 4.434528350830078\nfor batch 169/326, epoch 1: loss = 4.350286960601807\nfor batch 170/326, epoch 1: loss = 4.411079406738281\nfor batch 171/326, epoch 1: loss = 4.500580787658691\nfor batch 172/326, epoch 1: loss = 4.559240341186523\nfor batch 173/326, epoch 1: loss = 4.556268692016602\nfor batch 174/326, epoch 1: loss = 4.578118324279785\nfor batch 175/326, epoch 1: loss = 4.617891311645508\nfor batch 176/326, epoch 1: loss = 4.572027683258057\nfor batch 177/326, epoch 1: loss = 4.484432220458984\nfor batch 178/326, epoch 1: loss = 4.476221561431885\nfor batch 179/326, epoch 1: loss = 4.436501502990723\nfor batch 180/326, epoch 1: loss = 4.426874160766602\nfor batch 181/326, epoch 1: loss = 4.4835662841796875\nfor batch 182/326, epoch 1: loss = 4.481698036193848\nfor batch 183/326, epoch 1: loss = 4.529006004333496\nfor batch 184/326, epoch 1: loss = 4.503185272216797\nfor batch 185/326, epoch 1: loss = 4.659629821777344\nfor batch 186/326, epoch 1: loss = 4.482059478759766\nfor batch 187/326, epoch 1: loss = 4.535019397735596\nfor batch 188/326, epoch 1: loss = 4.375947952270508\nfor batch 189/326, epoch 1: loss = 4.49291467666626\nfor batch 190/326, epoch 1: loss = 4.450356483459473\nfor batch 191/326, epoch 1: loss = 4.5153350830078125\nfor batch 192/326, epoch 1: loss = 4.492092609405518\nfor batch 193/326, epoch 1: loss = 4.427824974060059\nfor batch 194/326, epoch 1: loss = 4.525141716003418\nfor batch 195/326, epoch 1: loss = 4.606614589691162\nfor batch 196/326, epoch 1: loss = 4.607232093811035\nfor batch 197/326, epoch 1: loss = 4.490365028381348\nfor batch 198/326, epoch 1: loss = 4.65548038482666\nfor batch 199/326, epoch 1: loss = 4.379619598388672\nfor batch 200/326, epoch 1: loss = 4.373573303222656\nfor batch 201/326, epoch 1: loss = 4.485560417175293\nfor batch 202/326, epoch 1: loss = 4.4741411209106445\nfor batch 203/326, epoch 1: loss = 4.438390254974365\nfor batch 204/326, epoch 1: loss = 4.579363822937012\nfor batch 205/326, epoch 1: loss = 4.492316722869873\nfor batch 206/326, epoch 1: loss = 4.345385551452637\nfor batch 207/326, epoch 1: loss = 4.39565372467041\nfor batch 208/326, epoch 1: loss = 4.542171478271484\nfor batch 209/326, epoch 1: loss = 4.576475143432617\nfor batch 210/326, epoch 1: loss = 4.450110912322998\nfor batch 211/326, epoch 1: loss = 4.54882287979126\nfor batch 212/326, epoch 1: loss = 4.5777506828308105\nfor batch 213/326, epoch 1: loss = 4.600263595581055\nfor batch 214/326, epoch 1: loss = 4.6131792068481445\nfor batch 215/326, epoch 1: loss = 4.466579437255859\nfor batch 216/326, epoch 1: loss = 4.554599761962891\nfor batch 217/326, epoch 1: loss = 4.598608493804932\nfor batch 218/326, epoch 1: loss = 4.552495002746582\nfor batch 219/326, epoch 1: loss = 4.605461120605469\nfor batch 220/326, epoch 1: loss = 4.522049903869629\nfor batch 221/326, epoch 1: loss = 4.630290985107422\nfor batch 222/326, epoch 1: loss = 4.434240341186523\nfor batch 223/326, epoch 1: loss = 4.565003395080566\nfor batch 224/326, epoch 1: loss = 4.503081321716309\nfor batch 225/326, epoch 1: loss = 4.337798118591309\nfor batch 226/326, epoch 1: loss = 4.464023590087891\nfor batch 227/326, epoch 1: loss = 4.5505781173706055\nfor batch 228/326, epoch 1: loss = 4.4698100090026855\nfor batch 229/326, epoch 1: loss = 4.4887895584106445\nfor batch 230/326, epoch 1: loss = 4.483973979949951\nfor batch 231/326, epoch 1: loss = 4.734984397888184\nfor batch 232/326, epoch 1: loss = 4.526765823364258\nfor batch 233/326, epoch 1: loss = 4.402005195617676\nfor batch 234/326, epoch 1: loss = 4.549219131469727\nfor batch 235/326, epoch 1: loss = 4.4102935791015625\nfor batch 236/326, epoch 1: loss = 4.460457801818848\nfor batch 237/326, epoch 1: loss = 4.415277004241943\nfor batch 238/326, epoch 1: loss = 4.429422378540039\nfor batch 239/326, epoch 1: loss = 4.419980525970459\nfor batch 240/326, epoch 1: loss = 4.55772066116333\nfor batch 241/326, epoch 1: loss = 4.582585334777832\nfor batch 242/326, epoch 1: loss = 4.455560207366943\nfor batch 243/326, epoch 1: loss = 4.58898401260376\nfor batch 244/326, epoch 1: loss = 4.523709297180176\nfor batch 245/326, epoch 1: loss = 4.562125205993652\nfor batch 246/326, epoch 1: loss = 4.598921775817871\nfor batch 247/326, epoch 1: loss = 4.513802528381348\nfor batch 248/326, epoch 1: loss = 4.463665962219238\nfor batch 249/326, epoch 1: loss = 4.601778030395508\nfor batch 250/326, epoch 1: loss = 4.445118427276611\nfor batch 251/326, epoch 1: loss = 4.461822509765625\nfor batch 252/326, epoch 1: loss = 4.454704284667969\nfor batch 253/326, epoch 1: loss = 4.4995341300964355\nfor batch 254/326, epoch 1: loss = 4.479734897613525\nfor batch 255/326, epoch 1: loss = 4.444634437561035\nfor batch 256/326, epoch 1: loss = 4.417400360107422\nfor batch 257/326, epoch 1: loss = 4.587889194488525\nfor batch 258/326, epoch 1: loss = 4.504113674163818\nfor batch 259/326, epoch 1: loss = 4.514525413513184\nfor batch 260/326, epoch 1: loss = 4.4369707107543945\nfor batch 261/326, epoch 1: loss = 4.488834381103516\nfor batch 262/326, epoch 1: loss = 4.506366729736328\nfor batch 263/326, epoch 1: loss = 4.451257705688477\nfor batch 264/326, epoch 1: loss = 4.522050857543945\nfor batch 265/326, epoch 1: loss = 4.449282169342041\nfor batch 266/326, epoch 1: loss = 4.474340438842773\nfor batch 267/326, epoch 1: loss = 4.474778175354004\nfor batch 268/326, epoch 1: loss = 4.4531707763671875\nfor batch 269/326, epoch 1: loss = 4.650609970092773\nfor batch 270/326, epoch 1: loss = 4.366145133972168\nfor batch 271/326, epoch 1: loss = 4.380060195922852\nfor batch 272/326, epoch 1: loss = 4.4614105224609375\nfor batch 273/326, epoch 1: loss = 4.455007076263428\nfor batch 274/326, epoch 1: loss = 4.5580244064331055\nfor batch 275/326, epoch 1: loss = 4.4757981300354\nfor batch 276/326, epoch 1: loss = 4.4363532066345215\nfor batch 277/326, epoch 1: loss = 4.468810081481934\nfor batch 278/326, epoch 1: loss = 4.59773063659668\nfor batch 279/326, epoch 1: loss = 4.462413787841797\nfor batch 280/326, epoch 1: loss = 4.425957202911377\nfor batch 281/326, epoch 1: loss = 4.457871437072754\nfor batch 282/326, epoch 1: loss = 4.550560474395752\nfor batch 283/326, epoch 1: loss = 4.492056846618652\nfor batch 284/326, epoch 1: loss = 4.56859016418457\nfor batch 285/326, epoch 1: loss = 4.471466064453125\nfor batch 286/326, epoch 1: loss = 4.4208478927612305\nfor batch 287/326, epoch 1: loss = 4.460328102111816\nfor batch 288/326, epoch 1: loss = 4.516156196594238\nfor batch 289/326, epoch 1: loss = 4.376020908355713\nfor batch 290/326, epoch 1: loss = 4.440769195556641\nfor batch 291/326, epoch 1: loss = 4.4316606521606445\nfor batch 292/326, epoch 1: loss = 4.65737771987915\nfor batch 293/326, epoch 1: loss = 4.5350871086120605\nfor batch 294/326, epoch 1: loss = 4.417746543884277\nfor batch 295/326, epoch 1: loss = 4.496513843536377\nfor batch 296/326, epoch 1: loss = 4.427517890930176\nfor batch 297/326, epoch 1: loss = 4.471244812011719\nfor batch 298/326, epoch 1: loss = 4.485610008239746\nfor batch 299/326, epoch 1: loss = 4.434942245483398\nfor batch 300/326, epoch 1: loss = 4.46782922744751\nfor batch 301/326, epoch 1: loss = 4.318287372589111\nfor batch 302/326, epoch 1: loss = 4.589447975158691\nfor batch 303/326, epoch 1: loss = 4.451987266540527\nfor batch 304/326, epoch 1: loss = 4.5236358642578125\nfor batch 305/326, epoch 1: loss = 4.57682991027832\nfor batch 306/326, epoch 1: loss = 4.607802867889404\nfor batch 307/326, epoch 1: loss = 4.520892143249512\nfor batch 308/326, epoch 1: loss = 4.499636650085449\nfor batch 309/326, epoch 1: loss = 4.393052577972412\nfor batch 310/326, epoch 1: loss = 4.51365852355957\nfor batch 311/326, epoch 1: loss = 4.468736171722412\nfor batch 312/326, epoch 1: loss = 4.5739006996154785\nfor batch 313/326, epoch 1: loss = 4.546350479125977\nfor batch 314/326, epoch 1: loss = 4.493337154388428\nfor batch 315/326, epoch 1: loss = 4.576767921447754\nfor batch 316/326, epoch 1: loss = 4.340730667114258\nfor batch 317/326, epoch 1: loss = 4.403766632080078\nfor batch 318/326, epoch 1: loss = 4.533110618591309\nfor batch 319/326, epoch 1: loss = 4.560776233673096\nfor batch 320/326, epoch 1: loss = 4.396381855010986\nfor batch 321/326, epoch 1: loss = 4.441817283630371\nfor batch 322/326, epoch 1: loss = 4.528384208679199\nfor batch 323/326, epoch 1: loss = 4.423519134521484\nfor batch 324/326, epoch 1: loss = 4.517664909362793\nfor batch 325/326, epoch 1: loss = 4.418364524841309\nfor batch 326/326, epoch 1: loss = 4.4770097732543945\nfor batch 1/326, epoch 2: loss = 4.536434650421143\nfor batch 2/326, epoch 2: loss = 4.438763618469238\nfor batch 3/326, epoch 2: loss = 4.634321212768555\nfor batch 4/326, epoch 2: loss = 4.424386978149414\nfor batch 5/326, epoch 2: loss = 4.664067268371582\nfor batch 6/326, epoch 2: loss = 4.469152450561523\nfor batch 7/326, epoch 2: loss = 4.470951557159424\nfor batch 8/326, epoch 2: loss = 4.457913398742676\nfor batch 9/326, epoch 2: loss = 4.531008720397949\nfor batch 10/326, epoch 2: loss = 4.418206214904785\nfor batch 11/326, epoch 2: loss = 4.399194717407227\nfor batch 12/326, epoch 2: loss = 4.581155300140381\nfor batch 13/326, epoch 2: loss = 4.626782417297363\nfor batch 14/326, epoch 2: loss = 4.437168121337891\nfor batch 15/326, epoch 2: loss = 4.589239120483398\nfor batch 16/326, epoch 2: loss = 4.4563398361206055\nfor batch 17/326, epoch 2: loss = 4.475015163421631\nfor batch 18/326, epoch 2: loss = 4.455597400665283\nfor batch 19/326, epoch 2: loss = 4.614129066467285\nfor batch 20/326, epoch 2: loss = 4.391215801239014\nfor batch 21/326, epoch 2: loss = 4.407231330871582\nfor batch 22/326, epoch 2: loss = 4.583868026733398\nfor batch 23/326, epoch 2: loss = 4.346942901611328\nfor batch 24/326, epoch 2: loss = 4.453263759613037\nfor batch 25/326, epoch 2: loss = 4.633791446685791\nfor batch 26/326, epoch 2: loss = 4.596463680267334\nfor batch 27/326, epoch 2: loss = 4.491061210632324\nfor batch 28/326, epoch 2: loss = 4.394826889038086\nfor batch 29/326, epoch 2: loss = 4.497109413146973\nfor batch 30/326, epoch 2: loss = 4.5125203132629395\nfor batch 31/326, epoch 2: loss = 4.43365478515625\nfor batch 32/326, epoch 2: loss = 4.473478317260742\nfor batch 33/326, epoch 2: loss = 4.442700386047363\nfor batch 34/326, epoch 2: loss = 4.532057762145996\nfor batch 35/326, epoch 2: loss = 4.403903961181641\nfor batch 36/326, epoch 2: loss = 4.399894714355469\nfor batch 37/326, epoch 2: loss = 4.543185234069824\nfor batch 38/326, epoch 2: loss = 4.435614585876465\nfor batch 39/326, epoch 2: loss = 4.600048065185547\nfor batch 40/326, epoch 2: loss = 4.6194748878479\nfor batch 41/326, epoch 2: loss = 4.384208679199219\nfor batch 42/326, epoch 2: loss = 4.548325538635254\nfor batch 43/326, epoch 2: loss = 4.461458206176758\nfor batch 44/326, epoch 2: loss = 4.576757431030273\nfor batch 45/326, epoch 2: loss = 4.485089302062988\nfor batch 46/326, epoch 2: loss = 4.474355697631836\nfor batch 47/326, epoch 2: loss = 4.431216239929199\nfor batch 48/326, epoch 2: loss = 4.423718452453613\nfor batch 49/326, epoch 2: loss = 4.565328121185303\nfor batch 50/326, epoch 2: loss = 4.4938812255859375\nfor batch 51/326, epoch 2: loss = 4.375543594360352\nfor batch 52/326, epoch 2: loss = 4.500755310058594\nfor batch 53/326, epoch 2: loss = 4.4815673828125\nfor batch 54/326, epoch 2: loss = 4.496950149536133\nfor batch 55/326, epoch 2: loss = 4.6121745109558105\nfor batch 56/326, epoch 2: loss = 4.4520063400268555\nfor batch 57/326, epoch 2: loss = 4.632266044616699\nfor batch 58/326, epoch 2: loss = 4.592783451080322\nfor batch 59/326, epoch 2: loss = 4.341335296630859\nfor batch 60/326, epoch 2: loss = 4.573692798614502\nfor batch 61/326, epoch 2: loss = 4.492227554321289\nfor batch 62/326, epoch 2: loss = 4.512273788452148\nfor batch 63/326, epoch 2: loss = 4.434581756591797\nfor batch 64/326, epoch 2: loss = 4.4815287590026855\nfor batch 65/326, epoch 2: loss = 4.59381103515625\nfor batch 66/326, epoch 2: loss = 4.436220169067383\nfor batch 67/326, epoch 2: loss = 4.575923919677734\nfor batch 68/326, epoch 2: loss = 4.322963237762451\nfor batch 69/326, epoch 2: loss = 4.422715187072754\nfor batch 70/326, epoch 2: loss = 4.477968215942383\nfor batch 71/326, epoch 2: loss = 4.475316047668457\nfor batch 72/326, epoch 2: loss = 4.447463512420654\nfor batch 73/326, epoch 2: loss = 4.382399559020996\nfor batch 74/326, epoch 2: loss = 4.570032119750977\nfor batch 75/326, epoch 2: loss = 4.411351203918457\nfor batch 76/326, epoch 2: loss = 4.548392295837402\nfor batch 77/326, epoch 2: loss = 4.421834468841553\nfor batch 78/326, epoch 2: loss = 4.510982513427734\nfor batch 79/326, epoch 2: loss = 4.5266313552856445\nfor batch 80/326, epoch 2: loss = 4.4685797691345215\nfor batch 81/326, epoch 2: loss = 4.480401515960693\nfor batch 82/326, epoch 2: loss = 4.421634674072266\nfor batch 83/326, epoch 2: loss = 4.468040466308594\nfor batch 84/326, epoch 2: loss = 4.582390785217285\nfor batch 85/326, epoch 2: loss = 4.507252216339111\nfor batch 86/326, epoch 2: loss = 4.483219146728516\nfor batch 87/326, epoch 2: loss = 4.582334518432617\nfor batch 88/326, epoch 2: loss = 4.573169708251953\nfor batch 89/326, epoch 2: loss = 4.588891983032227\nfor batch 90/326, epoch 2: loss = 4.361285209655762\nfor batch 91/326, epoch 2: loss = 4.387780666351318\nfor batch 92/326, epoch 2: loss = 4.549312591552734\nfor batch 93/326, epoch 2: loss = 4.488880157470703\nfor batch 94/326, epoch 2: loss = 4.475656986236572\nfor batch 95/326, epoch 2: loss = 4.553075790405273\nfor batch 96/326, epoch 2: loss = 4.493757724761963\nfor batch 97/326, epoch 2: loss = 4.568573474884033\nfor batch 98/326, epoch 2: loss = 4.542990207672119\nfor batch 99/326, epoch 2: loss = 4.442767143249512\nfor batch 100/326, epoch 2: loss = 4.661853313446045\nfor batch 101/326, epoch 2: loss = 4.435015678405762\nfor batch 102/326, epoch 2: loss = 4.3916239738464355\nfor batch 103/326, epoch 2: loss = 4.634803771972656\nfor batch 104/326, epoch 2: loss = 4.482327461242676\nfor batch 105/326, epoch 2: loss = 4.5478034019470215\nfor batch 106/326, epoch 2: loss = 4.551131248474121\nfor batch 107/326, epoch 2: loss = 4.526933670043945\nfor batch 108/326, epoch 2: loss = 4.453609466552734\nfor batch 109/326, epoch 2: loss = 4.459863662719727\nfor batch 110/326, epoch 2: loss = 4.49229621887207\nfor batch 111/326, epoch 2: loss = 4.454338550567627\nfor batch 112/326, epoch 2: loss = 4.5682573318481445\nfor batch 113/326, epoch 2: loss = 4.361424922943115\nfor batch 114/326, epoch 2: loss = 4.5579729080200195\nfor batch 115/326, epoch 2: loss = 4.628593444824219\nfor batch 116/326, epoch 2: loss = 4.379364013671875\nfor batch 117/326, epoch 2: loss = 4.499397277832031\nfor batch 118/326, epoch 2: loss = 4.487864971160889\nfor batch 119/326, epoch 2: loss = 4.4556169509887695\nfor batch 120/326, epoch 2: loss = 4.5859880447387695\nfor batch 121/326, epoch 2: loss = 4.516241073608398\nfor batch 122/326, epoch 2: loss = 4.482852935791016\nfor batch 123/326, epoch 2: loss = 4.40553092956543\nfor batch 124/326, epoch 2: loss = 4.474069595336914\nfor batch 125/326, epoch 2: loss = 4.525165557861328\nfor batch 126/326, epoch 2: loss = 4.417505264282227\nfor batch 127/326, epoch 2: loss = 4.501557350158691\nfor batch 128/326, epoch 2: loss = 4.574785232543945\nfor batch 129/326, epoch 2: loss = 4.614174842834473\nfor batch 130/326, epoch 2: loss = 4.5137834548950195\nfor batch 131/326, epoch 2: loss = 4.511519908905029\nfor batch 132/326, epoch 2: loss = 4.676115036010742\nfor batch 133/326, epoch 2: loss = 4.488724708557129\nfor batch 134/326, epoch 2: loss = 4.529776573181152\nfor batch 135/326, epoch 2: loss = 4.3822455406188965\nfor batch 136/326, epoch 2: loss = 4.338657379150391\nfor batch 137/326, epoch 2: loss = 4.488223075866699\nfor batch 138/326, epoch 2: loss = 4.528851509094238\nfor batch 139/326, epoch 2: loss = 4.565146446228027\nfor batch 140/326, epoch 2: loss = 4.581552505493164\nfor batch 141/326, epoch 2: loss = 4.623839378356934\nfor batch 142/326, epoch 2: loss = 4.442444801330566\nfor batch 143/326, epoch 2: loss = 4.433780670166016\nfor batch 144/326, epoch 2: loss = 4.419394493103027\nfor batch 145/326, epoch 2: loss = 4.386229515075684\nfor batch 146/326, epoch 2: loss = 4.641928672790527\nfor batch 147/326, epoch 2: loss = 4.411717414855957\nfor batch 148/326, epoch 2: loss = 4.487196445465088\nfor batch 149/326, epoch 2: loss = 4.3980488777160645\nfor batch 150/326, epoch 2: loss = 4.691551208496094\nfor batch 151/326, epoch 2: loss = 4.449822902679443\nfor batch 152/326, epoch 2: loss = 4.388730525970459\nfor batch 153/326, epoch 2: loss = 4.559274673461914\nfor batch 154/326, epoch 2: loss = 4.498430252075195\nfor batch 155/326, epoch 2: loss = 4.455836772918701\nfor batch 156/326, epoch 2: loss = 4.483551025390625\nfor batch 157/326, epoch 2: loss = 4.446630954742432\nfor batch 158/326, epoch 2: loss = 4.483821392059326\nfor batch 159/326, epoch 2: loss = 4.505217552185059\nfor batch 160/326, epoch 2: loss = 4.568393230438232\nfor batch 161/326, epoch 2: loss = 4.527498245239258\nfor batch 162/326, epoch 2: loss = 4.561173439025879\nfor batch 163/326, epoch 2: loss = 4.378724098205566\nfor batch 164/326, epoch 2: loss = 4.540696144104004\nfor batch 165/326, epoch 2: loss = 4.406609535217285\nfor batch 166/326, epoch 2: loss = 4.442920684814453\nfor batch 167/326, epoch 2: loss = 4.31608772277832\nfor batch 168/326, epoch 2: loss = 4.6402587890625\nfor batch 169/326, epoch 2: loss = 4.478165626525879\nfor batch 170/326, epoch 2: loss = 4.460503578186035\nfor batch 171/326, epoch 2: loss = 4.603532314300537\nfor batch 172/326, epoch 2: loss = 4.442822456359863\nfor batch 173/326, epoch 2: loss = 4.395991325378418\nfor batch 174/326, epoch 2: loss = 4.432215690612793\nfor batch 175/326, epoch 2: loss = 4.460177421569824\nfor batch 176/326, epoch 2: loss = 4.373927116394043\nfor batch 177/326, epoch 2: loss = 4.4969048500061035\nfor batch 178/326, epoch 2: loss = 4.52194881439209\nfor batch 179/326, epoch 2: loss = 4.523712158203125\nfor batch 180/326, epoch 2: loss = 4.611096382141113\nfor batch 181/326, epoch 2: loss = 4.427423477172852\nfor batch 182/326, epoch 2: loss = 4.575261116027832\nfor batch 183/326, epoch 2: loss = 4.512485980987549\nfor batch 184/326, epoch 2: loss = 4.554701328277588\nfor batch 185/326, epoch 2: loss = 4.545181751251221\nfor batch 186/326, epoch 2: loss = 4.445521354675293\nfor batch 187/326, epoch 2: loss = 4.465675354003906\nfor batch 188/326, epoch 2: loss = 4.455652236938477\nfor batch 189/326, epoch 2: loss = 4.4105424880981445\nfor batch 190/326, epoch 2: loss = 4.584296226501465\nfor batch 191/326, epoch 2: loss = 4.613035678863525\nfor batch 192/326, epoch 2: loss = 4.401279926300049\nfor batch 193/326, epoch 2: loss = 4.573755264282227\nfor batch 194/326, epoch 2: loss = 4.670840263366699\nfor batch 195/326, epoch 2: loss = 4.527364253997803\nfor batch 196/326, epoch 2: loss = 4.508927345275879\nfor batch 197/326, epoch 2: loss = 4.526121139526367\nfor batch 198/326, epoch 2: loss = 4.525867938995361\nfor batch 199/326, epoch 2: loss = 4.741061210632324\nfor batch 200/326, epoch 2: loss = 4.534916877746582\nfor batch 201/326, epoch 2: loss = 4.4825029373168945\nfor batch 202/326, epoch 2: loss = 4.697146415710449\nfor batch 203/326, epoch 2: loss = 4.632150650024414\nfor batch 204/326, epoch 2: loss = 4.460689067840576\nfor batch 205/326, epoch 2: loss = 4.541364669799805\nfor batch 206/326, epoch 2: loss = 4.441809177398682\nfor batch 207/326, epoch 2: loss = 4.5460205078125\nfor batch 208/326, epoch 2: loss = 4.59818696975708\nfor batch 209/326, epoch 2: loss = 4.4613423347473145\nfor batch 210/326, epoch 2: loss = 4.534070014953613\nfor batch 211/326, epoch 2: loss = 4.671080589294434\nfor batch 212/326, epoch 2: loss = 4.438401222229004\nfor batch 213/326, epoch 2: loss = 4.723092079162598\nfor batch 214/326, epoch 2: loss = 4.490222930908203\nfor batch 215/326, epoch 2: loss = 4.719242095947266\nfor batch 216/326, epoch 2: loss = 4.4120073318481445\nfor batch 217/326, epoch 2: loss = 4.4145402908325195\nfor batch 218/326, epoch 2: loss = 4.566699028015137\nfor batch 219/326, epoch 2: loss = 4.494536399841309\nfor batch 220/326, epoch 2: loss = 4.550532341003418\nfor batch 221/326, epoch 2: loss = 4.608185291290283\nfor batch 222/326, epoch 2: loss = 4.51222038269043\nfor batch 223/326, epoch 2: loss = 4.436583995819092\nfor batch 224/326, epoch 2: loss = 4.492288589477539\nfor batch 225/326, epoch 2: loss = 4.491586208343506\nfor batch 226/326, epoch 2: loss = 4.4762115478515625\nfor batch 227/326, epoch 2: loss = 4.515475273132324\nfor batch 228/326, epoch 2: loss = 4.456560134887695\nfor batch 229/326, epoch 2: loss = 4.602691650390625\nfor batch 230/326, epoch 2: loss = 4.4002275466918945\nfor batch 231/326, epoch 2: loss = 4.551193714141846\nfor batch 232/326, epoch 2: loss = 4.493823528289795\nfor batch 233/326, epoch 2: loss = 4.463871955871582\nfor batch 234/326, epoch 2: loss = 4.473167896270752\nfor batch 235/326, epoch 2: loss = 4.423226833343506\nfor batch 236/326, epoch 2: loss = 4.629775047302246\nfor batch 237/326, epoch 2: loss = 4.503239631652832\nfor batch 238/326, epoch 2: loss = 4.344450950622559\nfor batch 239/326, epoch 2: loss = 4.543500900268555\nfor batch 240/326, epoch 2: loss = 4.514432430267334\nfor batch 241/326, epoch 2: loss = 4.6670074462890625\nfor batch 242/326, epoch 2: loss = 4.405413627624512\nfor batch 243/326, epoch 2: loss = 4.406682014465332\nfor batch 244/326, epoch 2: loss = 4.504570007324219\nfor batch 245/326, epoch 2: loss = 4.615769386291504\nfor batch 246/326, epoch 2: loss = 4.6218976974487305\nfor batch 247/326, epoch 2: loss = 4.504817008972168\nfor batch 248/326, epoch 2: loss = 4.440546989440918\nfor batch 249/326, epoch 2: loss = 4.536437034606934\nfor batch 250/326, epoch 2: loss = 4.4416069984436035\nfor batch 251/326, epoch 2: loss = 4.533924102783203\nfor batch 252/326, epoch 2: loss = 4.441657066345215\nfor batch 253/326, epoch 2: loss = 4.430323123931885\nfor batch 254/326, epoch 2: loss = 4.410649299621582\nfor batch 255/326, epoch 2: loss = 4.54926872253418\nfor batch 256/326, epoch 2: loss = 4.431008338928223\nfor batch 257/326, epoch 2: loss = 4.469487190246582\nfor batch 258/326, epoch 2: loss = 4.569072723388672\nfor batch 259/326, epoch 2: loss = 4.536848545074463\nfor batch 260/326, epoch 2: loss = 4.585805892944336\nfor batch 261/326, epoch 2: loss = 4.4973554611206055\nfor batch 262/326, epoch 2: loss = 4.543373107910156\nfor batch 263/326, epoch 2: loss = 4.622476100921631\nfor batch 264/326, epoch 2: loss = 4.568954944610596\nfor batch 265/326, epoch 2: loss = 4.4655561447143555\nfor batch 266/326, epoch 2: loss = 4.577791213989258\nfor batch 267/326, epoch 2: loss = 4.576930046081543\nfor batch 268/326, epoch 2: loss = 4.6421122550964355\nfor batch 269/326, epoch 2: loss = 4.487714767456055\nfor batch 270/326, epoch 2: loss = 4.469346046447754\nfor batch 271/326, epoch 2: loss = 4.4565229415893555\nfor batch 272/326, epoch 2: loss = 4.399048328399658\nfor batch 273/326, epoch 2: loss = 4.37867546081543\nfor batch 274/326, epoch 2: loss = 4.365960121154785\nfor batch 275/326, epoch 2: loss = 4.372074127197266\nfor batch 276/326, epoch 2: loss = 4.474783897399902\nfor batch 277/326, epoch 2: loss = 4.591289520263672\nfor batch 278/326, epoch 2: loss = 4.425246238708496\nfor batch 279/326, epoch 2: loss = 4.533932209014893\nfor batch 280/326, epoch 2: loss = 4.486493110656738\nfor batch 281/326, epoch 2: loss = 4.436367511749268\nfor batch 282/326, epoch 2: loss = 4.343493461608887\nfor batch 283/326, epoch 2: loss = 4.4664201736450195\nfor batch 284/326, epoch 2: loss = 4.499685287475586\nfor batch 285/326, epoch 2: loss = 4.5514116287231445\nfor batch 286/326, epoch 2: loss = 4.4499006271362305\nfor batch 287/326, epoch 2: loss = 4.512194633483887\nfor batch 288/326, epoch 2: loss = 4.488006591796875\nfor batch 289/326, epoch 2: loss = 4.65239143371582\nfor batch 290/326, epoch 2: loss = 4.524922847747803\nfor batch 291/326, epoch 2: loss = 4.5055389404296875\nfor batch 292/326, epoch 2: loss = 4.601494312286377\nfor batch 293/326, epoch 2: loss = 4.571615219116211\nfor batch 294/326, epoch 2: loss = 4.502986907958984\nfor batch 295/326, epoch 2: loss = 4.580142974853516\nfor batch 296/326, epoch 2: loss = 4.65383243560791\nfor batch 297/326, epoch 2: loss = 4.4615864753723145\nfor batch 298/326, epoch 2: loss = 4.409619331359863\nfor batch 299/326, epoch 2: loss = 4.567174911499023\nfor batch 300/326, epoch 2: loss = 4.464003562927246\nfor batch 301/326, epoch 2: loss = 4.589893341064453\nfor batch 302/326, epoch 2: loss = 4.4119391441345215\nfor batch 303/326, epoch 2: loss = 4.347193717956543\nfor batch 304/326, epoch 2: loss = 4.410341262817383\nfor batch 305/326, epoch 2: loss = 4.577188968658447\nfor batch 306/326, epoch 2: loss = 4.696475982666016\nfor batch 307/326, epoch 2: loss = 4.387331485748291\nfor batch 308/326, epoch 2: loss = 4.501981735229492\nfor batch 309/326, epoch 2: loss = 4.395968437194824\nfor batch 310/326, epoch 2: loss = 4.442634582519531\nfor batch 311/326, epoch 2: loss = 4.5275726318359375\nfor batch 312/326, epoch 2: loss = 4.662722587585449\nfor batch 313/326, epoch 2: loss = 4.433385372161865\nfor batch 314/326, epoch 2: loss = 4.400753974914551\nfor batch 315/326, epoch 2: loss = 4.497635364532471\nfor batch 316/326, epoch 2: loss = 4.4088335037231445\nfor batch 317/326, epoch 2: loss = 4.511898994445801\nfor batch 318/326, epoch 2: loss = 4.622261047363281\nfor batch 319/326, epoch 2: loss = 4.385425090789795\nfor batch 320/326, epoch 2: loss = 4.507535457611084\nfor batch 321/326, epoch 2: loss = 4.570072650909424\nfor batch 322/326, epoch 2: loss = 4.472348213195801\nfor batch 323/326, epoch 2: loss = 4.506567001342773\nfor batch 324/326, epoch 2: loss = 4.437658309936523\nfor batch 325/326, epoch 2: loss = 4.521042823791504\nfor batch 326/326, epoch 2: loss = 4.67441463470459\nfor batch 1/326, epoch 3: loss = 4.429073333740234\nfor batch 2/326, epoch 3: loss = 4.487490653991699\nfor batch 3/326, epoch 3: loss = 4.440634727478027\nfor batch 4/326, epoch 3: loss = 4.524096965789795\nfor batch 5/326, epoch 3: loss = 4.5476460456848145\nfor batch 6/326, epoch 3: loss = 4.668871879577637\nfor batch 7/326, epoch 3: loss = 4.395698547363281\nfor batch 8/326, epoch 3: loss = 4.562706470489502\nfor batch 9/326, epoch 3: loss = 4.45271110534668\nfor batch 10/326, epoch 3: loss = 4.363056659698486\nfor batch 11/326, epoch 3: loss = 4.552179336547852\nfor batch 12/326, epoch 3: loss = 4.506974697113037\nfor batch 13/326, epoch 3: loss = 4.547886848449707\nfor batch 14/326, epoch 3: loss = 4.5524067878723145\nfor batch 15/326, epoch 3: loss = 4.558506011962891\nfor batch 16/326, epoch 3: loss = 4.586259841918945\nfor batch 17/326, epoch 3: loss = 4.596016883850098\nfor batch 18/326, epoch 3: loss = 4.5255632400512695\nfor batch 19/326, epoch 3: loss = 4.559596061706543\nfor batch 20/326, epoch 3: loss = 4.427163124084473\nfor batch 21/326, epoch 3: loss = 4.529720783233643\nfor batch 22/326, epoch 3: loss = 4.728538513183594\nfor batch 23/326, epoch 3: loss = 4.412480354309082\nfor batch 24/326, epoch 3: loss = 4.558307647705078\nfor batch 25/326, epoch 3: loss = 4.522470474243164\nfor batch 26/326, epoch 3: loss = 4.3966264724731445\nfor batch 27/326, epoch 3: loss = 4.518253803253174\nfor batch 28/326, epoch 3: loss = 4.50292444229126\nfor batch 29/326, epoch 3: loss = 4.47103214263916\nfor batch 30/326, epoch 3: loss = 4.545211315155029\nfor batch 31/326, epoch 3: loss = 4.429795265197754\nfor batch 32/326, epoch 3: loss = 4.385105609893799\nfor batch 33/326, epoch 3: loss = 4.654715538024902\nfor batch 34/326, epoch 3: loss = 4.408133506774902\nfor batch 35/326, epoch 3: loss = 4.588253021240234\nfor batch 36/326, epoch 3: loss = 4.452308177947998\nfor batch 37/326, epoch 3: loss = 4.498657703399658\nfor batch 38/326, epoch 3: loss = 4.446928977966309\nfor batch 39/326, epoch 3: loss = 4.371447563171387\nfor batch 40/326, epoch 3: loss = 4.444177150726318\nfor batch 41/326, epoch 3: loss = 4.535487174987793\nfor batch 42/326, epoch 3: loss = 4.3943328857421875\nfor batch 43/326, epoch 3: loss = 4.3478193283081055\nfor batch 44/326, epoch 3: loss = 4.404934883117676\nfor batch 45/326, epoch 3: loss = 4.384785175323486\nfor batch 46/326, epoch 3: loss = 4.637116432189941\nfor batch 47/326, epoch 3: loss = 4.640959739685059\nfor batch 48/326, epoch 3: loss = 4.499371528625488\nfor batch 49/326, epoch 3: loss = 4.490175247192383\nfor batch 50/326, epoch 3: loss = 4.4145307540893555\nfor batch 51/326, epoch 3: loss = 4.567930221557617\nfor batch 52/326, epoch 3: loss = 4.476947784423828\nfor batch 53/326, epoch 3: loss = 4.378860950469971\nfor batch 54/326, epoch 3: loss = 4.5298919677734375\nfor batch 55/326, epoch 3: loss = 4.617127418518066\nfor batch 56/326, epoch 3: loss = 4.524036407470703\nfor batch 57/326, epoch 3: loss = 4.598199367523193\nfor batch 58/326, epoch 3: loss = 4.567408561706543\nfor batch 59/326, epoch 3: loss = 4.458757400512695\nfor batch 60/326, epoch 3: loss = 4.412955284118652\nfor batch 61/326, epoch 3: loss = 4.53018856048584\nfor batch 62/326, epoch 3: loss = 4.386228084564209\nfor batch 63/326, epoch 3: loss = 4.458703517913818\nfor batch 64/326, epoch 3: loss = 4.406475067138672\nfor batch 65/326, epoch 3: loss = 4.429409980773926\nfor batch 66/326, epoch 3: loss = 4.505411148071289\nfor batch 67/326, epoch 3: loss = 4.372004985809326\nfor batch 68/326, epoch 3: loss = 4.468473434448242\nfor batch 69/326, epoch 3: loss = 4.426144599914551\nfor batch 70/326, epoch 3: loss = 4.377869606018066\nfor batch 71/326, epoch 3: loss = 4.450723648071289\nfor batch 72/326, epoch 3: loss = 4.4568891525268555\nfor batch 73/326, epoch 3: loss = 4.499524116516113\nfor batch 74/326, epoch 3: loss = 4.4291157722473145\nfor batch 75/326, epoch 3: loss = 4.46126127243042\nfor batch 76/326, epoch 3: loss = 4.464436054229736\nfor batch 77/326, epoch 3: loss = 4.419737815856934\nfor batch 78/326, epoch 3: loss = 4.618203163146973\nfor batch 79/326, epoch 3: loss = 4.61698055267334\nfor batch 80/326, epoch 3: loss = 4.595160484313965\nfor batch 81/326, epoch 3: loss = 4.537910461425781\nfor batch 82/326, epoch 3: loss = 4.495512008666992\nfor batch 83/326, epoch 3: loss = 4.499411582946777\nfor batch 84/326, epoch 3: loss = 4.588924407958984\nfor batch 85/326, epoch 3: loss = 4.402806282043457\nfor batch 86/326, epoch 3: loss = 4.587206840515137\nfor batch 87/326, epoch 3: loss = 4.395576477050781\nfor batch 88/326, epoch 3: loss = 4.449005126953125\nfor batch 89/326, epoch 3: loss = 4.365510940551758\nfor batch 90/326, epoch 3: loss = 4.550254821777344\nfor batch 91/326, epoch 3: loss = 4.565138816833496\nfor batch 92/326, epoch 3: loss = 4.4943695068359375\nfor batch 93/326, epoch 3: loss = 4.3624114990234375\nfor batch 94/326, epoch 3: loss = 4.40761661529541\nfor batch 95/326, epoch 3: loss = 4.545005798339844\nfor batch 96/326, epoch 3: loss = 4.445642471313477\nfor batch 97/326, epoch 3: loss = 4.510073184967041\nfor batch 98/326, epoch 3: loss = 4.5757036209106445\nfor batch 99/326, epoch 3: loss = 4.471405982971191\nfor batch 100/326, epoch 3: loss = 4.521708011627197\nfor batch 101/326, epoch 3: loss = 4.487739562988281\nfor batch 102/326, epoch 3: loss = 4.515357971191406\nfor batch 103/326, epoch 3: loss = 4.524500846862793\nfor batch 104/326, epoch 3: loss = 4.48328161239624\nfor batch 105/326, epoch 3: loss = 4.4256911277771\nfor batch 106/326, epoch 3: loss = 4.503592491149902\nfor batch 107/326, epoch 3: loss = 4.467406272888184\nfor batch 108/326, epoch 3: loss = 4.513065338134766\nfor batch 109/326, epoch 3: loss = 4.437873363494873\nfor batch 110/326, epoch 3: loss = 4.6216044425964355\nfor batch 111/326, epoch 3: loss = 4.400618553161621\nfor batch 112/326, epoch 3: loss = 4.472814559936523\nfor batch 113/326, epoch 3: loss = 4.538712501525879\nfor batch 114/326, epoch 3: loss = 4.446981430053711\nfor batch 115/326, epoch 3: loss = 4.506350040435791\nfor batch 116/326, epoch 3: loss = 4.507531642913818\nfor batch 117/326, epoch 3: loss = 4.475481986999512\nfor batch 118/326, epoch 3: loss = 4.5114288330078125\nfor batch 119/326, epoch 3: loss = 4.445957183837891\nfor batch 120/326, epoch 3: loss = 4.638927459716797\nfor batch 121/326, epoch 3: loss = 4.415585994720459\nfor batch 122/326, epoch 3: loss = 4.439908027648926\nfor batch 123/326, epoch 3: loss = 4.661860942840576\nfor batch 124/326, epoch 3: loss = 4.324546813964844\nfor batch 125/326, epoch 3: loss = 4.496295928955078\nfor batch 126/326, epoch 3: loss = 4.441739559173584\nfor batch 127/326, epoch 3: loss = 4.496358871459961\nfor batch 128/326, epoch 3: loss = 4.445955276489258\nfor batch 129/326, epoch 3: loss = 4.51365852355957\nfor batch 130/326, epoch 3: loss = 4.58474063873291\nfor batch 131/326, epoch 3: loss = 4.380611896514893\nfor batch 132/326, epoch 3: loss = 4.627218246459961\nfor batch 133/326, epoch 3: loss = 4.6733832359313965\nfor batch 134/326, epoch 3: loss = 4.548372268676758\nfor batch 135/326, epoch 3: loss = 4.4376630783081055\nfor batch 136/326, epoch 3: loss = 4.525591850280762\nfor batch 137/326, epoch 3: loss = 4.461158275604248\nfor batch 138/326, epoch 3: loss = 4.582714080810547\nfor batch 139/326, epoch 3: loss = 4.354834079742432\nfor batch 140/326, epoch 3: loss = 4.486155033111572\nfor batch 141/326, epoch 3: loss = 4.514992713928223\nfor batch 142/326, epoch 3: loss = 4.390830993652344\nfor batch 143/326, epoch 3: loss = 4.452690124511719\nfor batch 144/326, epoch 3: loss = 4.414937973022461\nfor batch 145/326, epoch 3: loss = 4.539474964141846\nfor batch 146/326, epoch 3: loss = 4.685736656188965\nfor batch 147/326, epoch 3: loss = 4.508513450622559\nfor batch 148/326, epoch 3: loss = 4.688477516174316\nfor batch 149/326, epoch 3: loss = 4.459176063537598\nfor batch 150/326, epoch 3: loss = 4.634738922119141\nfor batch 151/326, epoch 3: loss = 4.480589866638184\nfor batch 152/326, epoch 3: loss = 4.567298412322998\nfor batch 153/326, epoch 3: loss = 4.383540630340576\nfor batch 154/326, epoch 3: loss = 4.404685020446777\nfor batch 155/326, epoch 3: loss = 4.677424430847168\nfor batch 156/326, epoch 3: loss = 4.525341033935547\nfor batch 157/326, epoch 3: loss = 4.596778869628906\nfor batch 158/326, epoch 3: loss = 4.43619966506958\nfor batch 159/326, epoch 3: loss = 4.593622207641602\nfor batch 160/326, epoch 3: loss = 4.517261505126953\nfor batch 161/326, epoch 3: loss = 4.510391712188721\nfor batch 162/326, epoch 3: loss = 4.599503517150879\nfor batch 163/326, epoch 3: loss = 4.581289291381836\nfor batch 164/326, epoch 3: loss = 4.477572441101074\nfor batch 165/326, epoch 3: loss = 4.326504707336426\nfor batch 166/326, epoch 3: loss = 4.648599624633789\nfor batch 167/326, epoch 3: loss = 4.617548942565918\nfor batch 168/326, epoch 3: loss = 4.399775981903076\nfor batch 169/326, epoch 3: loss = 4.441879749298096\nfor batch 170/326, epoch 3: loss = 4.525650978088379\nfor batch 171/326, epoch 3: loss = 4.3567609786987305\nfor batch 172/326, epoch 3: loss = 4.430130481719971\nfor batch 173/326, epoch 3: loss = 4.5507988929748535\nfor batch 174/326, epoch 3: loss = 4.4485273361206055\nfor batch 175/326, epoch 3: loss = 4.467252731323242\nfor batch 176/326, epoch 3: loss = 4.486504554748535\nfor batch 177/326, epoch 3: loss = 4.432072639465332\nfor batch 178/326, epoch 3: loss = 4.430418968200684\nfor batch 179/326, epoch 3: loss = 4.3978729248046875\nfor batch 180/326, epoch 3: loss = 4.5050458908081055\nfor batch 181/326, epoch 3: loss = 4.395508766174316\nfor batch 182/326, epoch 3: loss = 4.443290710449219\nfor batch 183/326, epoch 3: loss = 4.402473449707031\nfor batch 184/326, epoch 3: loss = 4.577051639556885\nfor batch 185/326, epoch 3: loss = 4.447277069091797\nfor batch 186/326, epoch 3: loss = 4.478726387023926\nfor batch 187/326, epoch 3: loss = 4.42392635345459\nfor batch 188/326, epoch 3: loss = 4.396022796630859\nfor batch 189/326, epoch 3: loss = 4.600513458251953\nfor batch 190/326, epoch 3: loss = 4.525658130645752\nfor batch 191/326, epoch 3: loss = 4.523033142089844\nfor batch 192/326, epoch 3: loss = 4.553916931152344\nfor batch 193/326, epoch 3: loss = 4.466928005218506\nfor batch 194/326, epoch 3: loss = 4.449149131774902\nfor batch 195/326, epoch 3: loss = 4.4007768630981445\nfor batch 196/326, epoch 3: loss = 4.536608695983887\nfor batch 197/326, epoch 3: loss = 4.46768045425415\nfor batch 198/326, epoch 3: loss = 4.513296127319336\nfor batch 199/326, epoch 3: loss = 4.527498245239258\nfor batch 200/326, epoch 3: loss = 4.468546390533447\nfor batch 201/326, epoch 3: loss = 4.593067169189453\nfor batch 202/326, epoch 3: loss = 4.534521102905273\nfor batch 203/326, epoch 3: loss = 4.564463138580322\nfor batch 204/326, epoch 3: loss = 4.595970153808594\nfor batch 205/326, epoch 3: loss = 4.602241516113281\nfor batch 206/326, epoch 3: loss = 4.307541847229004\nfor batch 207/326, epoch 3: loss = 4.452236175537109\nfor batch 208/326, epoch 3: loss = 4.531826972961426\nfor batch 209/326, epoch 3: loss = 4.54685115814209\nfor batch 210/326, epoch 3: loss = 4.557033538818359\nfor batch 211/326, epoch 3: loss = 4.646514415740967\nfor batch 212/326, epoch 3: loss = 4.549787521362305\nfor batch 213/326, epoch 3: loss = 4.445827484130859\nfor batch 214/326, epoch 3: loss = 4.509915351867676\nfor batch 215/326, epoch 3: loss = 4.4665961265563965\nfor batch 216/326, epoch 3: loss = 4.573184013366699\nfor batch 217/326, epoch 3: loss = 4.496487617492676\nfor batch 218/326, epoch 3: loss = 4.560599327087402\nfor batch 219/326, epoch 3: loss = 4.460168361663818\nfor batch 220/326, epoch 3: loss = 4.4135541915893555\nfor batch 221/326, epoch 3: loss = 4.448419570922852\nfor batch 222/326, epoch 3: loss = 4.5378007888793945\nfor batch 223/326, epoch 3: loss = 4.50450325012207\nfor batch 224/326, epoch 3: loss = 4.556187629699707\nfor batch 225/326, epoch 3: loss = 4.384783744812012\nfor batch 226/326, epoch 3: loss = 4.517360687255859\nfor batch 227/326, epoch 3: loss = 4.442993640899658\nfor batch 228/326, epoch 3: loss = 4.561918258666992\nfor batch 229/326, epoch 3: loss = 4.348306655883789\nfor batch 230/326, epoch 3: loss = 4.4233717918396\nfor batch 231/326, epoch 3: loss = 4.528692722320557\nfor batch 232/326, epoch 3: loss = 4.369029521942139\nfor batch 233/326, epoch 3: loss = 4.4733686447143555\nfor batch 234/326, epoch 3: loss = 4.617722034454346\nfor batch 235/326, epoch 3: loss = 4.463456153869629\nfor batch 236/326, epoch 3: loss = 4.369409561157227\nfor batch 237/326, epoch 3: loss = 4.383589744567871\nfor batch 238/326, epoch 3: loss = 4.525580406188965\nfor batch 239/326, epoch 3: loss = 4.403622627258301\nfor batch 240/326, epoch 3: loss = 4.614528656005859\nfor batch 241/326, epoch 3: loss = 4.511504173278809\nfor batch 242/326, epoch 3: loss = 4.5763444900512695\nfor batch 243/326, epoch 3: loss = 4.536118507385254\nfor batch 244/326, epoch 3: loss = 4.514497756958008\nfor batch 245/326, epoch 3: loss = 4.595038414001465\nfor batch 246/326, epoch 3: loss = 4.580083847045898\nfor batch 247/326, epoch 3: loss = 4.464369773864746\nfor batch 248/326, epoch 3: loss = 4.494039058685303\nfor batch 249/326, epoch 3: loss = 4.399611473083496\nfor batch 250/326, epoch 3: loss = 4.516353607177734\nfor batch 251/326, epoch 3: loss = 4.649038791656494\nfor batch 252/326, epoch 3: loss = 4.427467346191406\nfor batch 253/326, epoch 3: loss = 4.462331771850586\nfor batch 254/326, epoch 3: loss = 4.553633689880371\nfor batch 255/326, epoch 3: loss = 4.445426940917969\nfor batch 256/326, epoch 3: loss = 4.482665061950684\nfor batch 257/326, epoch 3: loss = 4.523538589477539\nfor batch 258/326, epoch 3: loss = 4.399420738220215\nfor batch 259/326, epoch 3: loss = 4.490229606628418\nfor batch 260/326, epoch 3: loss = 4.524877071380615\nfor batch 261/326, epoch 3: loss = 4.600093364715576\nfor batch 262/326, epoch 3: loss = 4.438661575317383\nfor batch 263/326, epoch 3: loss = 4.483375072479248\nfor batch 264/326, epoch 3: loss = 4.497161865234375\nfor batch 265/326, epoch 3: loss = 4.557762145996094\nfor batch 266/326, epoch 3: loss = 4.376367568969727\nfor batch 267/326, epoch 3: loss = 4.468191146850586\nfor batch 268/326, epoch 3: loss = 4.358043670654297\nfor batch 269/326, epoch 3: loss = 4.5824785232543945\nfor batch 270/326, epoch 3: loss = 4.484607696533203\nfor batch 271/326, epoch 3: loss = 4.605136394500732\nfor batch 272/326, epoch 3: loss = 4.582697868347168\nfor batch 273/326, epoch 3: loss = 4.514004707336426\nfor batch 274/326, epoch 3: loss = 4.478667259216309\nfor batch 275/326, epoch 3: loss = 4.49247932434082\nfor batch 276/326, epoch 3: loss = 4.543395042419434\nfor batch 277/326, epoch 3: loss = 4.543649196624756\nfor batch 278/326, epoch 3: loss = 4.565883636474609\nfor batch 279/326, epoch 3: loss = 4.573264122009277\nfor batch 280/326, epoch 3: loss = 4.414714813232422\nfor batch 281/326, epoch 3: loss = 4.580130577087402\nfor batch 282/326, epoch 3: loss = 4.4562225341796875\nfor batch 283/326, epoch 3: loss = 4.572789192199707\nfor batch 284/326, epoch 3: loss = 4.436733245849609\nfor batch 285/326, epoch 3: loss = 4.363046169281006\nfor batch 286/326, epoch 3: loss = 4.483570098876953\nfor batch 287/326, epoch 3: loss = 4.5216875076293945\nfor batch 288/326, epoch 3: loss = 4.569007873535156\nfor batch 289/326, epoch 3: loss = 4.478625297546387\nfor batch 290/326, epoch 3: loss = 4.332596778869629\nfor batch 291/326, epoch 3: loss = 4.423795700073242\nfor batch 292/326, epoch 3: loss = 4.5891218185424805\nfor batch 293/326, epoch 3: loss = 4.623075008392334\nfor batch 294/326, epoch 3: loss = 4.524336814880371\nfor batch 295/326, epoch 3: loss = 4.531176567077637\nfor batch 296/326, epoch 3: loss = 4.382207870483398\nfor batch 297/326, epoch 3: loss = 4.593995094299316\nfor batch 298/326, epoch 3: loss = 4.431454658508301\nfor batch 299/326, epoch 3: loss = 4.636046409606934\nfor batch 300/326, epoch 3: loss = 4.527581214904785\nfor batch 301/326, epoch 3: loss = 4.494200706481934\nfor batch 302/326, epoch 3: loss = 4.402825355529785\nfor batch 303/326, epoch 3: loss = 4.514029026031494\nfor batch 304/326, epoch 3: loss = 4.519865036010742\nfor batch 305/326, epoch 3: loss = 4.3923659324646\nfor batch 306/326, epoch 3: loss = 4.511346817016602\nfor batch 307/326, epoch 3: loss = 4.531428337097168\nfor batch 308/326, epoch 3: loss = 4.500870704650879\nfor batch 309/326, epoch 3: loss = 4.453762531280518\nfor batch 310/326, epoch 3: loss = 4.489311695098877\nfor batch 311/326, epoch 3: loss = 4.6265950202941895\nfor batch 312/326, epoch 3: loss = 4.636250019073486\nfor batch 313/326, epoch 3: loss = 4.620344161987305\nfor batch 314/326, epoch 3: loss = 4.527478218078613\nfor batch 315/326, epoch 3: loss = 4.516413688659668\nfor batch 316/326, epoch 3: loss = 4.437687873840332\nfor batch 317/326, epoch 3: loss = 4.490370750427246\nfor batch 318/326, epoch 3: loss = 4.506319046020508\nfor batch 319/326, epoch 3: loss = 4.442442417144775\nfor batch 320/326, epoch 3: loss = 4.477101802825928\nfor batch 321/326, epoch 3: loss = 4.610951900482178\nfor batch 322/326, epoch 3: loss = 4.443245887756348\nfor batch 323/326, epoch 3: loss = 4.473651885986328\nfor batch 324/326, epoch 3: loss = 4.63036584854126\nfor batch 325/326, epoch 3: loss = 4.4163312911987305\nfor batch 326/326, epoch 3: loss = 4.565977096557617\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def text_to_audio_array(text, lang='en'):\n#     audio_stream = io.BytesIO()\n#     tts = gTTS(text=text, lang=lang)\n#     tts.write_to_fp(audio_stream)\n    \n#     audio_stream.seek(0)\n    \n#     audio_segment = AudioSegment.from_file(audio_stream, format=\"mp3\")\n    \n#     wav_stream = io.BytesIO()\n#     audio_segment.export(wav_stream, format=\"wav\")\n#     wav_stream.seek(0)\n\n#     sample_rate, audio_data = wavfile.read(wav_stream)\n    \n#     return sample_rate, audio_data\n\n# text = \"Hello, how are you?\"\n# sample_rate, audio_data = text_to_audio_array(text, lang='en')\n\n# audio_tensor = torch.tensor(audio_data, dtype=torch.float32)\n# if sample_rate != 16000:\n#     resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n#     audio_tensor = resampler(audio_tensor)\n\n\n# audio_tensor = audio_tensor.unsqueeze(0)\n\n# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n# model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\n# inputs = processor(audio_tensor.squeeze(), sampling_rate=16000, return_tensors=\"pt\")\n\n# with torch.no_grad():\n#     features = model(**inputs).last_hidden_state\n\n# print(f\"Features Shape: {features.shape}\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class QADataset(Dataset):\n    def __init__(self, contexts, questions, answers, wav2vec_processor, wav2vec_model, tokenizer, lang='en'):\n        self.contexts = contexts\n        self.questions = questions\n        self.answers = answers\n        self.processor = wav2vec_processor\n        self.model = wav2vec_model\n        self.tokenizer = tokenizer\n        self.lang = lang\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, idx):\n        context = self.contexts[idx]\n        question = self.questions[idx]\n        answer = self.answers[idx]\n\n        # Tokenize and pad input\n        inputs = self.tokenizer(context, question, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n        \n        # Generate context audio features\n        context_audio_features = self.text_to_audio_features(context)\n        \n        # Create attention mask\n        attention_mask = inputs['attention_mask'].squeeze()  # Squeeze to remove extra dimensions\n\n        return {\n            'input_ids': inputs['input_ids'].squeeze(), \n            'attention_mask': attention_mask, \n            'context_audio_features': context_audio_features\n        }\n    def text_to_audio_array(self, text):\n        audio_stream = io.BytesIO()\n        tts = gTTS(text=text, lang=self.lang)\n        tts.write_to_fp(audio_stream)\n        \n        audio_stream.seek(0)\n        \n        audio_segment = AudioSegment.from_file(audio_stream, format=\"mp3\")\n        \n        wav_stream = io.BytesIO()\n        audio_segment.export(wav_stream, format=\"wav\")\n        wav_stream.seek(0)\n\n        sample_rate, audio_data = wavfile.read(wav_stream)\n        \n        return sample_rate, audio_data\n    \n    def text_to_audio_features(self, text):\n        sample_rate, audio_data = self.text_to_audio_array(text)\n        \n        audio_tensor = torch.tensor(audio_data, dtype=torch.float32)\n        if sample_rate != 16000:\n            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n            audio_tensor = resampler(audio_tensor)\n        \n        audio_tensor = audio_tensor.unsqueeze(0)\n        \n        inputs = self.processor(audio_tensor.squeeze(), sampling_rate=16000, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            inputs.to(device)\n            features = self.model(**inputs).last_hidden_state\n        \n        return features","metadata":{"execution":{"iopub.status.busy":"2024-08-13T16:10:00.532263Z","iopub.execute_input":"2024-08-13T16:10:00.533195Z","iopub.status.idle":"2024-08-13T16:10:00.545761Z","shell.execute_reply.started":"2024-08-13T16:10:00.533158Z","shell.execute_reply":"2024-08-13T16:10:00.545015Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import torch\nimport torchaudio\nimport numpy as np\n\nclass QADataset(Dataset):\n    def __init__(self, contexts, questions, answers, wav2vec_processor, wav2vec_model, tokenizer, lang='en', max_audio_length=16000):\n        self.contexts = contexts\n        self.questions = questions\n        self.answers = answers\n        self.processor = wav2vec_processor\n        self.model = wav2vec_model\n        self.tokenizer = tokenizer\n        self.lang = lang\n        self.max_audio_length = max_audio_length\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, idx):\n        context = self.contexts[idx]\n        question = self.questions[idx]\n        answer = self.answers[idx]\n\n        inputs = self.tokenizer(context, question, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n        \n        context_audio_features = self.text_to_audio_features(context)\n        \n        context_audio_features = self.pad_or_truncate_audio_features(context_audio_features)\n\n        attention_mask = inputs['attention_mask'].squeeze()  \n\n        return {\n            'inputs': inputs, \n            'attention_mask': attention_mask, \n            'context_audio_features': context_audio_features\n        }\n    \n    def text_to_audio_array(self, text):\n        audio_stream = io.BytesIO()\n        tts = gTTS(text=text, lang=self.lang)\n        tts.write_to_fp(audio_stream)\n        \n        audio_stream.seek(0)\n        \n        audio_segment = AudioSegment.from_file(audio_stream, format=\"mp3\")\n        \n        wav_stream = io.BytesIO()\n        audio_segment.export(wav_stream, format=\"wav\")\n        wav_stream.seek(0)\n\n        sample_rate, audio_data = wavfile.read(wav_stream)\n        \n        return sample_rate, audio_data\n    \n    def text_to_audio_features(self, text):\n        sample_rate, audio_data = self.text_to_audio_array(text)\n        \n        audio_tensor = torch.tensor(audio_data, dtype=torch.float32)\n        if sample_rate != 16000:\n            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n            audio_tensor = resampler(audio_tensor)\n        \n        audio_tensor = audio_tensor.unsqueeze(0)\n        \n        inputs = self.processor(audio_tensor.squeeze(), sampling_rate=16000, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            inputs = inputs.to(device)\n            features = self.model(**inputs).last_hidden_state\n        \n        return features\n\n    def pad_or_truncate_audio_features(self, features):\n        num_frames = features.size(1)\n        if num_frames > self.max_audio_length:\n            features = features[:, :self.max_audio_length, :]\n        elif num_frames < self.max_audio_length:\n            pad_length = self.max_audio_length - num_frames\n            pad = torch.zeros(features.size(0), pad_length, features.size(2), device=features.device)\n            features = torch.cat((features, pad), dim=1)\n        return features\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T16:17:02.505722Z","iopub.execute_input":"2024-08-13T16:17:02.506157Z","iopub.status.idle":"2024-08-13T16:17:02.525164Z","shell.execute_reply.started":"2024-08-13T16:17:02.506125Z","shell.execute_reply":"2024-08-13T16:17:02.524119Z"},"trusted":true},"outputs":[],"execution_count":47},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\nmodel1 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\ndataset = QADataset(contexts, questions, answers, processor, model,tokenizer)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T16:17:02.819085Z","iopub.execute_input":"2024-08-13T16:17:02.819477Z","iopub.status.idle":"2024-08-13T16:17:03.930236Z","shell.execute_reply.started":"2024-08-13T16:17:02.819446Z","shell.execute_reply":"2024-08-13T16:17:03.929329Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# # print(f\"Text Inputs: {sample['inputs']}\")\n# %time\n# import time\n# s = time.time()\n# sample = dataset[0]\n# print(f\"Context Audio Features Shape: {sample['context_audio_features'].shape}\")\n# e = time.time()\n\n# print(e - s)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T16:13:43.585002Z","iopub.execute_input":"2024-08-13T16:13:43.585651Z","iopub.status.idle":"2024-08-13T16:13:47.937209Z","shell.execute_reply.started":"2024-08-13T16:13:43.585621Z","shell.execute_reply":"2024-08-13T16:13:47.936235Z"},"trusted":true},"outputs":[{"name":"stdout","text":"CPU times: user 3 s, sys: 0 ns, total: 3 s\nWall time: 6.68 s\nContext Audio Features Shape: torch.Size([1, 16000, 768])\n4.345856666564941\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"\nclass MultimodalTransformer(nn.Module):\n    def __init__(self, text_model_name='bert-base-multilingual-cased', audio_feature_dim=768, hidden_dim=512):\n        super(MultimodalTransformer, self).__init__()\n        self.text_model = transformers.AutoModel.from_pretrained(text_model_name)\n        self.audio_linear = nn.Linear(audio_feature_dim, hidden_dim)\n        self.cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8)\n        self.fc = nn.Linear(hidden_dim, 1) \n\n    def forward(self, input_ids, attention_mask, audio_features):\n        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = text_outputs.last_hidden_state\n\n        audio_features = self.audio_linear(audio_features)\n        audio_features = audio_features.unsqueeze(0) \n\n        text_features = text_features.permute(1, 0, 2)  \n        attn_output, _ = self.cross_attention(text_features, audio_features, audio_features)\n        output = self.fc(attn_output.mean(dim=0))  \n\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T16:17:04.616155Z","iopub.execute_input":"2024-08-13T16:17:04.616581Z","iopub.status.idle":"2024-08-13T16:17:04.626226Z","shell.execute_reply.started":"2024-08-13T16:17:04.616548Z","shell.execute_reply":"2024-08-13T16:17:04.625184Z"},"trusted":true},"outputs":[],"execution_count":49},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport transformers\n\nbatch_size = 4\nnum_epochs = 1\nlearning_rate = 1e-5\n\ntrain_dataset = QADataset(\n    contexts=contexts, \n    questions=questions, \n    answers=answers, \n    wav2vec_processor=processor, \n    wav2vec_model=model1,\n    tokenizer = tokenizer\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = MultimodalTransformer().to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.MSELoss()  \n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    \n    for batch in train_loader:\n        inputs = batch['inputs']\n        context_audio_features = batch['context_audio_features']\n        inputs['input_ids'] = inputs['input_ids'].squeeze(1)\n        inputs['attention_mask'] = inputs['attention_mask'].squeeze(1)\n        inputs['token_type_ids'] = inputs['token_type_ids'].squeeze(1)\n        input_ids = inputs['input_ids'].to(device)\n        attention_mask = inputs['attention_mask'].to(device)\n        audio_features = context_audio_features.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, audio_features)\n    \n        targets = torch.zeros_like(outputs)  \n        loss = criterion(outputs, targets)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        print(loss.item())\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n\n\nprint(\"Training complete!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T16:19:37.326334Z","iopub.execute_input":"2024-08-13T16:19:37.326717Z","iopub.status.idle":"2024-08-13T16:20:05.394482Z","shell.execute_reply.started":"2024-08-13T16:19:37.326684Z","shell.execute_reply":"2024-08-13T16:20:05.393027Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[53], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m audio_features \u001b[38;5;241m=\u001b[39m context_audio_features\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(outputs)  \n\u001b[1;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[49], line 18\u001b[0m, in \u001b[0;36mMultimodalTransformer.forward\u001b[0;34m(self, input_ids, attention_mask, audio_features)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Cross-attention between text and audio features\u001b[39;00m\n\u001b[1;32m     17\u001b[0m text_features \u001b[38;5;241m=\u001b[39m text_features\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (seq_len, batch_size, hidden_dim)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(attn_output\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# Pooling over sequence\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5227\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tens_ops):\n\u001b[1;32m   5197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   5198\u001b[0m         multi_head_attention_forward,\n\u001b[1;32m   5199\u001b[0m         tens_ops,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5224\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   5225\u001b[0m     )\n\u001b[0;32m-> 5227\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m \u001b[43m_mha_shape_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5229\u001b[0m \u001b[38;5;66;03m# For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\u001b[39;00m\n\u001b[1;32m   5230\u001b[0m \u001b[38;5;66;03m# is batched, run the computation and before returning squeeze the\u001b[39;00m\n\u001b[1;32m   5231\u001b[0m \u001b[38;5;66;03m# batch dimension so that the output doesn't carry this temporary batch dimension.\u001b[39;00m\n\u001b[1;32m   5232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   5233\u001b[0m     \u001b[38;5;66;03m# unsqueeze if the input is unbatched\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5022\u001b[0m, in \u001b[0;36m_mha_shape_check\u001b[0;34m(query, key, value, key_padding_mask, attn_mask, num_heads)\u001b[0m\n\u001b[1;32m   5019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m   5020\u001b[0m     \u001b[38;5;66;03m# Batched Inputs\u001b[39;00m\n\u001b[1;32m   5021\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 5022\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m key\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \\\n\u001b[1;32m   5023\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor batched (3-D) `query`, expected `key` and `value` to be 3-D\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5024\u001b[0m          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D tensors respectively\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5026\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m key_padding_mask\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \\\n\u001b[1;32m   5027\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor batched (3-D) `query`, expected `key_padding_mask` to be `None` or 2-D\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5028\u001b[0m              \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey_padding_mask\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D tensor instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAssertionError\u001b[0m: For batched (3-D) `query`, expected `key` and `value` to be 3-D but found 5-D and 5-D tensors respectively"],"ename":"AssertionError","evalue":"For batched (3-D) `query`, expected `key` and `value` to be 3-D but found 5-D and 5-D tensors respectively","output_type":"error"}],"execution_count":53},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}