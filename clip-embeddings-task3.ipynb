{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9356647,"sourceType":"datasetVersion","datasetId":5543066}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"### This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-10T06:32:06.209536Z","iopub.execute_input":"2024-09-10T06:32:06.209843Z","iopub.status.idle":"2024-09-10T06:32:06.584898Z","shell.execute_reply.started":"2024-09-10T06:32:06.209801Z","shell.execute_reply":"2024-09-10T06:32:06.583750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentence_transformers","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-10T06:32:06.587108Z","iopub.execute_input":"2024-09-10T06:32:06.587595Z","iopub.status.idle":"2024-09-10T06:32:22.489737Z","shell.execute_reply.started":"2024-09-10T06:32:06.587552Z","shell.execute_reply":"2024-09-10T06:32:22.488597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport ast\n\n# Check if CUDA is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load the SentenceTransformer model and move it to the GPU\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nmodel = model.to(device)\n\n# Step 1: Load input sentences and entity knowledge strings from CSVs\ninput_df = pd.read_csv('/kaggle/input/business-json/Task_3_data.csv')  # CSV with columns 'sentence' and 'actual_entity'\nentity_df = pd.read_csv('/kaggle/input/business-json/Knowledge.csv')  # CSV with columns 'entity_name' and 'entity_description'\n\n# # Function to extract entity names\n# def extract_entity_name(entities_str):\n#     entities_list = ast.literal_eval(entities_str)\n#     # Return the first two mentions if they exist, otherwise None\n#     return entities_list\n\n# # Apply the function to the 'entities' column and expand into two new columns\n# input_df['entity_names'] = input_df['entity_titles'].apply(extract_entity_name).apply(pd.Series)\n\n# Display the updated DataFrame\nprint(input_df)\n\nasr_df = pd.read_csv('/kaggle/input/business-json/task_3_asr_out.csv')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-10T06:32:22.491476Z","iopub.execute_input":"2024-09-10T06:32:22.491889Z","iopub.status.idle":"2024-09-10T06:32:49.343656Z","shell.execute_reply.started":"2024-09-10T06:32:22.491840Z","shell.execute_reply":"2024-09-10T06:32:49.342652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(input_df.dtypes)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:32:49.345680Z","iopub.execute_input":"2024-09-10T06:32:49.346036Z","iopub.status.idle":"2024-09-10T06:32:49.352225Z","shell.execute_reply.started":"2024-09-10T06:32:49.346000Z","shell.execute_reply":"2024-09-10T06:32:49.351171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Top-K entity Linking","metadata":{}},{"cell_type":"code","source":"NER_df = pd.read_csv('/kaggle/input/business-json/Task_3_asr_NER.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:32:53.491208Z","iopub.execute_input":"2024-09-10T06:32:53.491707Z","iopub.status.idle":"2024-09-10T06:32:53.502190Z","shell.execute_reply.started":"2024-09-10T06:32:53.491665Z","shell.execute_reply":"2024-09-10T06:32:53.500965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_embeddings = model.encode(input_df['question'].tolist(), convert_to_tensor=True, device=device)\nentity_knowledge_embeddings = model.encode(entity_df['Knowledge'].tolist(), convert_to_tensor=True, device=device)\n# all_embeddings = [] \n\n# for index, row in asr_df.iterrows():\n#     sentences_str = row['transcriptions'] \n#     sentences = ast.literal_eval(sentences_str)\n#     row_embeddings = model.encode(sentences, convert_to_tensor=True, device=device)\n#     all_embeddings.append(row_embeddings)  # Append the embeddings to the list\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-10T06:32:54.760087Z","iopub.execute_input":"2024-09-10T06:32:54.760519Z","iopub.status.idle":"2024-09-10T06:32:55.869116Z","shell.execute_reply.started":"2024-09-10T06:32:54.760481Z","shell.execute_reply":"2024-09-10T06:32:55.868194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## After NER","metadata":{}},{"cell_type":"markdown","source":"### Full Knowledge","metadata":{}},{"cell_type":"code","source":"def calc(result_df, k, thresh):\n    f1_s = []\n    r_s = []\n    p_s = []\n    for i, row in result_df.iterrows():\n        count = row['bool_c']\n        act = row['bool_t']\n        \n        l = len(row['retrieved_sentence_ids'])\n        if (l == 0):\n            f1 = 0\n            recall = 0\n            pr = 0\n        else:\n            pr = count/l\n            recall = count/act\n            if (recall == 0 and pr == 0):\n                f1 = 0\n            else:\n                f1 = 2 * pr * recall/(pr+recall)\n\n        f1_s.append(f1)    \n        r_s.append(recall)\n        p_s.append(pr)\n\n#     print(f'average f1 score for top-{k} reults with threshold = {thresh} : {(sum(f1_s)/len(f1_s))}')\n#     print(f'average recall score for top-{k} reults with threshold = {thresh} : {(sum(r_s)/len(r_s))}')\n#     print(f'average precision score for top-{k} reults with threshold = {thresh} : {(sum(p_s)/len(p_s))}')\n    return (sum(f1_s)/len(f1_s)), (sum(r_s)/len(r_s)), (sum(p_s)/len(p_s))\n\ndef calc_ner(result_df, k, thresh):\n    f1_s = []\n    r_s = []\n    p_s = []\n    for i, row in result_df.iterrows():\n        count = row['bool_c']\n        act = row['bool_t']\n        \n        l = len(row['retrieved_sentence_ids'])\n        if (l == 0):\n            f1 = 0\n            recall = 0\n            pr = 0\n        else:\n            pr = count/l\n            recall = count/act\n            if (recall == 0 and pr == 0):\n                f1 = 0\n            else:\n                f1 = 2 * pr * recall/(pr+recall)\n\n        f1_s.append(f1)    \n        r_s.append(recall)\n        p_s.append(pr)\n\n#     print(f'average f1 score for top-{k} reults with threshold = {thresh} : {(sum(f1_s)/len(f1_s))}')\n#     print(f'average recall score for top-{k} reults with threshold = {thresh} : {(sum(r_s)/len(r_s))}')\n#     print(f'average precision score for top-{k} reults with threshold = {thresh} : {(sum(p_s)/len(p_s))}')\n    return (sum(f1_s)/len(f1_s)), (sum(r_s)/len(r_s)), (sum(p_s)/len(p_s))\n\n# Assume `knowledge_embeddings` contains embeddings of all entities\n# Assume `knowledge_entity_ids` contains the IDs corresponding to the knowledge embeddings\ndc = {}\nfor z in range(2,3):\n    for so in range(1,2):\n        top_k = 20  # Define how many top entities you want to retrieve\n        similarity_threshold = 0.25 * so  # Define the similarity threshold\n        similarity_threshold = round(similarity_threshold,2)\n        result_data = []\n\n        for i, row in input_df.iterrows():\n            # Step 1: Get the sentence embeddings for the current row from the asr_df\n            question_embedding = question_embeddings[i].unsqueeze(0).cpu().numpy()  # Shape: (num_sentences_in_row, embedding_dim)\n\n            # Step 2: Get the corresponding sentence IDs for this row\n            sentence_ids = ast.literal_eval(asr_df['Sentence_ids'].iloc[i])\n\n            entity_ids = ast.literal_eval(NER_df['top_entities'].iloc[i])\n            sim_scores = []\n            emt = []\n            for j in range(len(sentence_ids)):\n                # Get similarity scores for the current sentence\n        #         sentence_similarity_scores = similarity_scores[j]\n                sentence_id = sentence_ids[j]\n                entity_id = entity_ids[j]\n                emt.append(entity_id)\n                ind = entity_df[entity_df['Entity_ID'] == entity_id].index\n                embed = entity_knowledge_embeddings[ind]\n                similarity_scores = cosine_similarity(question_embedding,  embed.cpu().numpy()).flatten() \n                sim_scores.append(similarity_scores)\n                # Filter based on similarity threshold\n            filtered_indices = [idx for idx, score in enumerate(sim_scores) if score >= similarity_threshold]\n            top_indices = sorted(filtered_indices, key=lambda idx: sim_scores[idx], reverse=True)[:top_k]\n            pos_sentence_ids = ast.literal_eval(input_df['positive_sentence_ids'].iloc[i])\n#             emt = emt[top_indices]\n            count = 0\n            for e1 in pos_sentence_ids:\n                if e1 in top_indices:\n                    count+=1\n            result_data.append({\n                'q_id': row['QID'],\n                'sentence_ids': sentence_ids,\n                'retrieved_sentence_ids': top_indices,\n                'retrieved_sentence_entity_ids': [emt[idx] for idx in top_indices],\n                'similarity_scores': [sim_scores[idx][0] for idx in top_indices],\n                'ground_truth_positive_sentence_ids': pos_sentence_ids,\n                'ground_truth_positive_sentence_entity_ids': ast.literal_eval(row['entities']),\n                'all_sentences': ast.literal_eval(row['all_sentences']),\n                'answer_num': len(pos_sentence_ids),\n                'answer':row['answer'],\n                'bool_c' : count,\n                'bool_t': len(pos_sentence_ids)\n            })\n        #         # Sort by similarity score and get top k entities\n        #     top_indice = sorted(filtered_indices, key=lambda idx: sentence_similarity_scores[idx], reverse=True)[0]\n        # #         if i < 1: \n        # #             print(top_indice)\n        #         # Get the top entities and their corresponding IDs\n        #     top_entities_for_sentence = knowledge_entity_ids[top_indice]\n        #         top_scores_for_sentence = [sentence_similarity_scores[idx] for idx in top_indices]\n\n                # Store results for this sentence\n        #         sentence_ids.append(sentence_ids[j])\n\n            # List of sentence IDs corresponding to the embeddings\n\n        #     # Step 3: Calculate cosine similarity between sentence embeddings and knowledge embeddings\n        #     similarity_scores = cosine_similarity(question_embedding,  entity_knowledge_embeddings.cpu().numpy()).flatten()  # Shape: (embedding_dim, num_sentences_in_row)\n\n        #     filtered_indices = [idx for idx, score in enumerate(similarity_scores) if score >= similarity_threshold]\n\n        # #     # Step 6: Sort by similarity score and get top k sentences\n        #     top_indices = sorted(filtered_indices, key=lambda idx: similarity_scores[idx], reverse=True)[:top_k]\n        #     top_entities = #         top_scores_for_sentence = [sentence_similarity_scores[idx] for idx in top_indices]\n        #     # Get the top k sentences and their corresponding sentence IDs\n        #     top_sentence_ids = [sentence_ids[idx] for idx in top_indices]  # Corresponding sentence IDs\n        #     top_sentences = [ast.literal_eval(asr_df['transcriptions'].iloc[i])[idx] for idx in top_indices]  # Sentences corresponding to top indices\n        #     pos_sentence_ids = ast.literal_eval(input_df['positive_sentence_ids'].iloc[i])\n        #     count = 0\n        #     for e1 in pos_sentence_ids:\n        #         if e1 in top_sentence_ids:\n        #             count+=1\n        # #     print(pos_sentence_ids.all() in top_sentence_ids)\n        #     # Step 7: Store the result for this row\n        #     result_data.append({\n        #         'q_id': row['QID'],\n        #         'top_sentence_ids': top_sentence_ids,\n        #         'top_sentences': top_sentences,\n        #         'similarity_scores': [similarity_scores[idx] for idx in top_indices],\n\n        #     })\n            # Step 4: Process each sentence\n        #     top_sentence_ids = []\n        #     top_entities = []\n        #     top_scores = []\n\n        #     for j in range(len(sentence_ids)):\n        #         # Get similarity scores for the current sentence\n        #         sentence_similarity_scores = similarity_scores[j]\n\n        #         # Filter based on similarity threshold\n        #         filtered_indices = [idx for idx, score in enumerate(sentence_similarity_scores) if score >= similarity_threshold]\n\n        #         # Sort by similarity score and get top k entities\n        #         top_indice = sorted(filtered_indices, key=lambda idx: sentence_similarity_scores[idx], reverse=True)[0]\n        # #         if i < 1: \n        # #             print(top_indice)\n        #         # Get the top entities and their corresponding IDs\n        #         top_entities_for_sentence = knowledge_entity_ids[top_indice]\n        # #         top_scores_for_sentence = [sentence_similarity_scores[idx] for idx in top_indices]\n\n        #         # Store results for this sentence\n        # #         sentence_ids.append(sentence_ids[j])\n        #         top_entities.append(top_entities_for_sentence)\n        # #         top_scores.append(top_scores_for_sentence)\n\n        #     # Flatten the results and count matches for positive entities\n        # #     pos_sentence_ids = ast.literal_eval(input_df['positive_sentence_ids'].iloc[i])\n        # #     count = sum(e1 in top_entity for top_entities_for_sentence in top_entities for e1 in pos_sentence_ids)\n\n        #     # Step 7: Store the result for this row\n\n\n        # Optionally convert the result to a DataFrame\n        result_df = pd.DataFrame(result_data)\n        a, b, c = calc(result_df, top_k, similarity_threshold)\n        dc[top_k, similarity_threshold] = a,b,c\n        ojjj = 0\n        for i , row in result_df.iterrows():\n#                 kjsh = ast.literal_eval(row['top_sentence_ids'])\n            kjsh = row['retrieved_sentence_ids']\n            ojjj += len(kjsh)\n        print(f'For threshold = {similarity_threshold}: average_result_length = {ojjj/len(result_df)}')\n# Print the result\n# print(result_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:32:59.290346Z","iopub.execute_input":"2024-09-10T06:32:59.290740Z","iopub.status.idle":"2024-09-10T06:33:00.712606Z","shell.execute_reply.started":"2024-09-10T06:32:59.290702Z","shell.execute_reply":"2024-09-10T06:33:00.711567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.to_csv(\"Task3_Retrieved_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:00.714358Z","iopub.execute_input":"2024-09-10T06:33:00.714675Z","iopub.status.idle":"2024-09-10T06:33:00.728016Z","shell.execute_reply.started":"2024-09-10T06:33:00.714641Z","shell.execute_reply":"2024-09-10T06:33:00.727062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for it in dc.items():\n    print(it)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:00.728985Z","iopub.execute_input":"2024-09-10T06:33:00.729291Z","iopub.status.idle":"2024-09-10T06:33:00.734077Z","shell.execute_reply.started":"2024-09-10T06:33:00.729260Z","shell.execute_reply":"2024-09-10T06:33:00.733197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### NER after retrieval with full knowledge","metadata":{}},{"cell_type":"code","source":"new_df = pd.read_csv(\"/kaggle/working/Task3_Retrieved_results.csv\")\nentity_embeddings = model.encode(entity_df['Knowledge'].tolist(), convert_to_tensor=True, device=device)\nknowledge_entity_ids = entity_df['Entity_ID'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:01.702185Z","iopub.execute_input":"2024-09-10T06:33:01.702957Z","iopub.status.idle":"2024-09-10T06:33:02.116453Z","shell.execute_reply.started":"2024-09-10T06:33:01.702912Z","shell.execute_reply":"2024-09-10T06:33:02.115379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_embeddings = [] \n\nfor index, row in new_df.iterrows():\n    sentences_ids_str = row['retrieved_sentence_ids'] \n    sentence_ids = ast.literal_eval(sentences_ids_str)\n    sentences_str = asr_df['transcriptions'].iloc[index] \n    sentences = ast.literal_eval(sentences_str)\n    relevant = []\n    for j in range(len(sentences)):\n        if j in sentence_ids:\n            relevant.append(sentences[j])\n    row_embeddings = model.encode(relevant, convert_to_tensor=True, device=device)\n    all_embeddings.append(row_embeddings)  # Append the embeddings to the list","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-10T06:33:02.118075Z","iopub.execute_input":"2024-09-10T06:33:02.118464Z","iopub.status.idle":"2024-09-10T06:33:05.201765Z","shell.execute_reply.started":"2024-09-10T06:33:02.118426Z","shell.execute_reply":"2024-09-10T06:33:05.200672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_data = []","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:05.203829Z","iopub.execute_input":"2024-09-10T06:33:05.204217Z","iopub.status.idle":"2024-09-10T06:33:05.209210Z","shell.execute_reply.started":"2024-09-10T06:33:05.204177Z","shell.execute_reply":"2024-09-10T06:33:05.208036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_threshold = 0.05\n\n# Create a dictionary to store results, ensuring unique q_id entries\nresult_data = {}\n\nfor i, row in new_df.iterrows():\n    # Step 1: Get the sentence embeddings for the current row from the asr_df\n    sentence_embeddings = all_embeddings[i].cpu().numpy()  # Shape: (num_sentences_in_row, embedding_dim)\n    if len(sentence_embeddings) > 0:\n        # Step 2: Get the corresponding sentence IDs for this row\n        sentence_ids = ast.literal_eval(new_df['retrieved_sentence_ids'].iloc[i])  # List of sentence IDs corresponding to the embeddings\n\n        # Step 3: Calculate cosine similarity between sentence embeddings and knowledge embeddings\n        similarity_scores = cosine_similarity(sentence_embeddings, entity_embeddings.cpu().numpy())  # Shape: (embedding_dim, num_sentences_in_row)\n\n        # Step 4: Process each sentence\n        top_entities = []\n        \n        for j in range(len(sentence_ids)):\n            # Get similarity scores for the current sentence\n            sentence_similarity_scores = similarity_scores[j]\n\n            # Filter based on similarity threshold\n            filtered_indices = [idx for idx, score in enumerate(sentence_similarity_scores) if score >= similarity_threshold]\n            if filtered_indices:\n                # Sort by similarity score and get top entity\n                top_indice = sorted(filtered_indices, key=lambda idx: sentence_similarity_scores[idx], reverse=True)[0]\n\n                # Get the top entity for the sentence\n                top_entities_for_sentence = knowledge_entity_ids[top_indice]\n                top_entities.append(top_entities_for_sentence)\n\n    else:\n        top_entities = []\n        sentence_ids = []\n    # Step 7: Store the result for this q_id\n    if row['q_id'] not in result_data:\n        result_data[row['q_id']] = {\n            'q_id': row['q_id'],\n            'retrieved_sentence_ids': sentence_ids,\n            'top_entities': top_entities\n        }\n\n# Convert the result dictionary to a DataFrame\nresult_df = pd.DataFrame(list(result_data.values()))\n\n# Print the result\nprint(result_df['q_id'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:05.210697Z","iopub.execute_input":"2024-09-10T06:33:05.211097Z","iopub.status.idle":"2024-09-10T06:33:06.752413Z","shell.execute_reply.started":"2024-09-10T06:33:05.211060Z","shell.execute_reply":"2024-09-10T06:33:06.750974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.to_csv(\"Task3_CLIP_full_Knowledge_NER_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:06.756720Z","iopub.execute_input":"2024-09-10T06:33:06.757362Z","iopub.status.idle":"2024-09-10T06:33:06.766780Z","shell.execute_reply.started":"2024-09-10T06:33:06.757296Z","shell.execute_reply":"2024-09-10T06:33:06.765294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### NER after retrieval with partial knowledge","metadata":{}},{"cell_type":"code","source":"entity_df['Knowledge_20'] = entity_df['Knowledge'].apply(lambda x: x[:int(len(x) * 0.2)])\nentity_embeddings = model.encode(entity_df['Knowledge_20'].tolist(), convert_to_tensor=True, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:06.769215Z","iopub.execute_input":"2024-09-10T06:33:06.769836Z","iopub.status.idle":"2024-09-10T06:33:06.981423Z","shell.execute_reply.started":"2024-09-10T06:33:06.769772Z","shell.execute_reply":"2024-09-10T06:33:06.980323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_threshold = 0.05\n\n# Create a dictionary to store results, ensuring unique q_id entries\nresult_data = {}\n\nfor i, row in new_df.iterrows():\n    # Step 1: Get the sentence embeddings for the current row from the asr_df\n    sentence_embeddings = all_embeddings[i].cpu().numpy()  # Shape: (num_sentences_in_row, embedding_dim)\n    if len(sentence_embeddings) > 0:\n        # Step 2: Get the corresponding sentence IDs for this row\n        sentence_ids = ast.literal_eval(new_df['retrieved_sentence_ids'].iloc[i])  # List of sentence IDs corresponding to the embeddings\n\n        # Step 3: Calculate cosine similarity between sentence embeddings and knowledge embeddings\n        similarity_scores = cosine_similarity(sentence_embeddings, entity_embeddings.cpu().numpy())  # Shape: (embedding_dim, num_sentences_in_row)\n\n        # Step 4: Process each sentence\n        top_entities = []\n        \n        for j in range(len(sentence_ids)):\n            # Get similarity scores for the current sentence\n            sentence_similarity_scores = similarity_scores[j]\n\n            # Filter based on similarity threshold\n            filtered_indices = [idx for idx, score in enumerate(sentence_similarity_scores) if score >= similarity_threshold]\n            if filtered_indices:\n                # Sort by similarity score and get top entity\n                top_indice = sorted(filtered_indices, key=lambda idx: sentence_similarity_scores[idx], reverse=True)[0]\n\n                # Get the top entity for the sentence\n                top_entities_for_sentence = knowledge_entity_ids[top_indice]\n                top_entities.append(top_entities_for_sentence)\n\n    else:\n        top_entities = []\n        sentence_ids = []\n    # Step 7: Store the result for this q_id\n    if row['q_id'] not in result_data:\n        result_data[row['q_id']] = {\n            'q_id': row['q_id'],\n            'retrieved_sentence_ids': sentence_ids,\n            'top_entities': top_entities\n        }\n\n# Convert the result dictionary to a DataFrame\nresult_df = pd.DataFrame(list(result_data.values()))\n\n# Print the result\nprint(result_df['q_id'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:06.982814Z","iopub.execute_input":"2024-09-10T06:33:06.983168Z","iopub.status.idle":"2024-09-10T06:33:08.452926Z","shell.execute_reply.started":"2024-09-10T06:33:06.983129Z","shell.execute_reply":"2024-09-10T06:33:08.451941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.to_csv(\"Task3_CLIP_partial_Knowledge_NER_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:08.454527Z","iopub.execute_input":"2024-09-10T06:33:08.455233Z","iopub.status.idle":"2024-09-10T06:33:08.462554Z","shell.execute_reply.started":"2024-09-10T06:33:08.455186Z","shell.execute_reply":"2024-09-10T06:33:08.461387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### NER after retrieval with label (Entity_names)","metadata":{}},{"cell_type":"code","source":"entity_embeddings = model.encode(entity_df['Entity_name'].tolist(), convert_to_tensor=True, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:08.467933Z","iopub.execute_input":"2024-09-10T06:33:08.468821Z","iopub.status.idle":"2024-09-10T06:33:08.687434Z","shell.execute_reply.started":"2024-09-10T06:33:08.468758Z","shell.execute_reply":"2024-09-10T06:33:08.686434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_threshold = 0.05\n\n# Create a dictionary to store results, ensuring unique q_id entries\nresult_data = {}\n\nfor i, row in new_df.iterrows():\n    # Step 1: Get the sentence embeddings for the current row from the asr_df\n    sentence_embeddings = all_embeddings[i].cpu().numpy()  # Shape: (num_sentences_in_row, embedding_dim)\n    if len(sentence_embeddings) > 0:\n        # Step 2: Get the corresponding sentence IDs for this row\n        sentence_ids = ast.literal_eval(new_df['retrieved_sentence_ids'].iloc[i])  # List of sentence IDs corresponding to the embeddings\n\n        # Step 3: Calculate cosine similarity between sentence embeddings and knowledge embeddings\n        similarity_scores = cosine_similarity(sentence_embeddings, entity_embeddings.cpu().numpy())  # Shape: (embedding_dim, num_sentences_in_row)\n\n        # Step 4: Process each sentence\n        top_entities = []\n        \n        for j in range(len(sentence_ids)):\n            # Get similarity scores for the current sentence\n            sentence_similarity_scores = similarity_scores[j]\n\n            # Filter based on similarity threshold\n            filtered_indices = [idx for idx, score in enumerate(sentence_similarity_scores) if score >= similarity_threshold]\n            if filtered_indices:\n                # Sort by similarity score and get top entity\n                top_indice = sorted(filtered_indices, key=lambda idx: sentence_similarity_scores[idx], reverse=True)[0]\n\n                # Get the top entity for the sentence\n                top_entities_for_sentence = knowledge_entity_ids[top_indice]\n                top_entities.append(top_entities_for_sentence)\n\n    else:\n        top_entities = []\n        sentence_ids = []\n    # Step 7: Store the result for this q_id\n    if row['q_id'] not in result_data:\n        result_data[row['q_id']] = {\n            'q_id': row['q_id'],\n            'retrieved_sentence_ids': sentence_ids,\n            'top_entities': top_entities\n        }\n\n# Convert the result dictionary to a DataFrame\nresult_df = pd.DataFrame(list(result_data.values()))\n\n# Print the result\nprint(result_df['q_id'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:08.688726Z","iopub.execute_input":"2024-09-10T06:33:08.689057Z","iopub.status.idle":"2024-09-10T06:33:10.121394Z","shell.execute_reply.started":"2024-09-10T06:33:08.689021Z","shell.execute_reply":"2024-09-10T06:33:10.120085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.to_csv(\"Task3_CLIP_label_NER_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:10.127172Z","iopub.execute_input":"2024-09-10T06:33:10.128117Z","iopub.status.idle":"2024-09-10T06:33:10.136779Z","shell.execute_reply.started":"2024-09-10T06:33:10.128026Z","shell.execute_reply":"2024-09-10T06:33:10.135572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### NER on pos results with full_knowledge","metadata":{}},{"cell_type":"code","source":"all_embeddings = [] \nentity_embeddings = model.encode(entity_df['Knowledge'].tolist(), convert_to_tensor=True, device=device)\nfor index, row in input_df.iterrows():\n    sentences_ids_str = row['positive_sentence_ids'] \n    sentence_ids = ast.literal_eval(sentences_ids_str)\n    sentences_str = asr_df['transcriptions'].iloc[index] \n    sentences = ast.literal_eval(sentences_str)\n    relevant = []\n    for j in range(len(sentences)):\n        if j in sentence_ids:\n            relevant.append(sentences[j])\n    row_embeddings = model.encode(relevant, convert_to_tensor=True, device=device)\n    all_embeddings.append(row_embeddings)  # Append the embeddings to the list","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-10T06:33:10.138995Z","iopub.execute_input":"2024-09-10T06:33:10.140004Z","iopub.status.idle":"2024-09-10T06:33:13.691864Z","shell.execute_reply.started":"2024-09-10T06:33:10.139942Z","shell.execute_reply":"2024-09-10T06:33:13.690987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_threshold = 0.05\n\n# Create a dictionary to store results, ensuring unique q_id entries\nresult_data = {}\n\nfor i, row in input_df.iterrows():\n    # Step 1: Get the sentence embeddings for the current row from the asr_df\n    sentence_embeddings = all_embeddings[i].cpu().numpy()  # Shape: (num_sentences_in_row, embedding_dim)\n    if len(sentence_embeddings) > 0:\n        # Step 2: Get the corresponding sentence IDs for this row\n        sentence_ids = ast.literal_eval(input_df['positive_sentence_ids'].iloc[i])  # List of sentence IDs corresponding to the embeddings\n\n        # Step 3: Calculate cosine similarity between sentence embeddings and knowledge embeddings\n        similarity_scores = cosine_similarity(sentence_embeddings, entity_embeddings.cpu().numpy())  # Shape: (embedding_dim, num_sentences_in_row)\n\n        # Step 4: Process each sentence\n        top_entities = []\n        \n        for j in range(len(sentence_ids)):\n            # Get similarity scores for the current sentence\n            sentence_similarity_scores = similarity_scores[j]\n\n            # Filter based on similarity threshold\n            filtered_indices = [idx for idx, score in enumerate(sentence_similarity_scores) if score >= similarity_threshold]\n            if filtered_indices:\n                # Sort by similarity score and get top entity\n                top_indice = sorted(filtered_indices, key=lambda idx: sentence_similarity_scores[idx], reverse=True)[0]\n\n                # Get the top entity for the sentence\n                top_entities_for_sentence = knowledge_entity_ids[top_indice]\n                top_entities.append(top_entities_for_sentence)\n\n    else:\n        top_entities = []\n\n    # Step 7: Store the result for this q_id\n    if row['QID'] not in result_data:\n        result_data[row['QID']] = {\n            'q_id': row['QID'],\n            'positive_sentence_ids': sentence_ids,\n            'top_entities': top_entities\n        }\n\n# Convert the result dictionary to a DataFrame\nresult_df = pd.DataFrame(list(result_data.values()))\n\n# Print the result\nprint(result_df['q_id'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:13.693576Z","iopub.execute_input":"2024-09-10T06:33:13.693896Z","iopub.status.idle":"2024-09-10T06:33:14.803184Z","shell.execute_reply.started":"2024-09-10T06:33:13.693862Z","shell.execute_reply":"2024-09-10T06:33:14.802111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.to_csv(\"Task3_CLIP_full_knowledge_positive_sentences_NER_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:14.804437Z","iopub.execute_input":"2024-09-10T06:33:14.804782Z","iopub.status.idle":"2024-09-10T06:33:14.811393Z","shell.execute_reply.started":"2024-09-10T06:33:14.804750Z","shell.execute_reply":"2024-09-10T06:33:14.810553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### NER on pos results with partial Knowledge","metadata":{}},{"cell_type":"code","source":"entity_df['Knowledge_20'] = entity_df['Knowledge'].apply(lambda x: x[:int(len(x) * 0.2)])\nentity_embeddings = model.encode(entity_df['Knowledge_20'].tolist(), convert_to_tensor=True, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:14.812554Z","iopub.execute_input":"2024-09-10T06:33:14.813308Z","iopub.status.idle":"2024-09-10T06:33:14.995934Z","shell.execute_reply.started":"2024-09-10T06:33:14.813262Z","shell.execute_reply":"2024-09-10T06:33:14.994998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_threshold = 0.05\n\n# Create a dictionary to store results, ensuring unique q_id entries\nresult_data = {}\n\nfor i, row in input_df.iterrows():\n    # Step 1: Get the sentence embeddings for the current row from the asr_df\n    sentence_embeddings = all_embeddings[i].cpu().numpy()  # Shape: (num_sentences_in_row, embedding_dim)\n    if len(sentence_embeddings) > 0:\n        # Step 2: Get the corresponding sentence IDs for this row\n        sentence_ids = ast.literal_eval(input_df['positive_sentence_ids'].iloc[i])  # List of sentence IDs corresponding to the embeddings\n\n        # Step 3: Calculate cosine similarity between sentence embeddings and knowledge embeddings\n        similarity_scores = cosine_similarity(sentence_embeddings, entity_embeddings.cpu().numpy())  # Shape: (embedding_dim, num_sentences_in_row)\n\n        # Step 4: Process each sentence\n        top_entities = []\n        \n        for j in range(len(sentence_ids)):\n            # Get similarity scores for the current sentence\n            sentence_similarity_scores = similarity_scores[j]\n\n            # Filter based on similarity threshold\n            filtered_indices = [idx for idx, score in enumerate(sentence_similarity_scores) if score >= similarity_threshold]\n            if filtered_indices:\n                # Sort by similarity score and get top entity\n                top_indice = sorted(filtered_indices, key=lambda idx: sentence_similarity_scores[idx], reverse=True)[0]\n\n                # Get the top entity for the sentence\n                top_entities_for_sentence = knowledge_entity_ids[top_indice]\n                top_entities.append(top_entities_for_sentence)\n\n    else:\n        top_entities = []\n\n    # Step 7: Store the result for this q_id\n    if row['QID'] not in result_data:\n        result_data[row['QID']] = {\n            'q_id': row['QID'],\n            'positive_sentence_ids': sentence_ids,\n            'top_entities': top_entities\n        }\n\n# Convert the result dictionary to a DataFrame\nresult_df = pd.DataFrame(list(result_data.values()))\n\n# Print the result\nprint(result_df['q_id'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:14.997755Z","iopub.execute_input":"2024-09-10T06:33:14.998257Z","iopub.status.idle":"2024-09-10T06:33:16.094535Z","shell.execute_reply.started":"2024-09-10T06:33:14.998208Z","shell.execute_reply":"2024-09-10T06:33:16.093609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.to_csv(\"Task3_CLIP_partial_knowledge_positive_sentences_NER_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:16.096026Z","iopub.execute_input":"2024-09-10T06:33:16.096647Z","iopub.status.idle":"2024-09-10T06:33:16.103028Z","shell.execute_reply.started":"2024-09-10T06:33:16.096582Z","shell.execute_reply":"2024-09-10T06:33:16.102214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### NER on pos results with labels(entity_names)","metadata":{}},{"cell_type":"code","source":"entity_embeddings = model.encode(entity_df['Entity_name'].tolist(), convert_to_tensor=True, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:16.104258Z","iopub.execute_input":"2024-09-10T06:33:16.104654Z","iopub.status.idle":"2024-09-10T06:33:16.247242Z","shell.execute_reply.started":"2024-09-10T06:33:16.104607Z","shell.execute_reply":"2024-09-10T06:33:16.246318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_threshold = 0.05\n\n# Create a dictionary to store results, ensuring unique q_id entries\nresult_data = {}\n\nfor i, row in input_df.iterrows():\n    # Step 1: Get the sentence embeddings for the current row from the asr_df\n    sentence_embeddings = all_embeddings[i].cpu().numpy()  # Shape: (num_sentences_in_row, embedding_dim)\n    if len(sentence_embeddings) > 0:\n        # Step 2: Get the corresponding sentence IDs for this row\n        sentence_ids = ast.literal_eval(input_df['positive_sentence_ids'].iloc[i])  # List of sentence IDs corresponding to the embeddings\n\n        # Step 3: Calculate cosine similarity between sentence embeddings and knowledge embeddings\n        similarity_scores = cosine_similarity(sentence_embeddings, entity_embeddings.cpu().numpy())  # Shape: (embedding_dim, num_sentences_in_row)\n\n        # Step 4: Process each sentence\n        top_entities = []\n        \n        for j in range(len(sentence_ids)):\n            # Get similarity scores for the current sentence\n            sentence_similarity_scores = similarity_scores[j]\n\n            # Filter based on similarity threshold\n            filtered_indices = [idx for idx, score in enumerate(sentence_similarity_scores) if score >= similarity_threshold]\n            if filtered_indices:\n                # Sort by similarity score and get top entity\n                top_indice = sorted(filtered_indices, key=lambda idx: sentence_similarity_scores[idx], reverse=True)[0]\n\n                # Get the top entity for the sentence\n                top_entities_for_sentence = knowledge_entity_ids[top_indice]\n                top_entities.append(top_entities_for_sentence)\n\n    else:\n        top_entities = []\n\n    # Step 7: Store the result for this q_id\n    if row['QID'] not in result_data:\n        result_data[row['QID']] = {\n            'q_id': row['QID'],\n            'positive_sentence_ids': sentence_ids,\n            'top_entities': top_entities\n        }\n\n# Convert the result dictionary to a DataFrame\nresult_df = pd.DataFrame(list(result_data.values()))\n\n# Print the result\nprint(result_df['q_id'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:16.248606Z","iopub.execute_input":"2024-09-10T06:33:16.248897Z","iopub.status.idle":"2024-09-10T06:33:17.368289Z","shell.execute_reply.started":"2024-09-10T06:33:16.248865Z","shell.execute_reply":"2024-09-10T06:33:17.367350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.to_csv(\"Task3_CLIP_label_positive_sentences_NER_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:33:17.369532Z","iopub.execute_input":"2024-09-10T06:33:17.369858Z","iopub.status.idle":"2024-09-10T06:33:17.376573Z","shell.execute_reply.started":"2024-09-10T06:33:17.369823Z","shell.execute_reply":"2024-09-10T06:33:17.375735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# similarity_threshold = 0.05\n\n# for i, row in new_df.iterrows():\n#     # Step 1: Get the sentence embeddings for the current row from the asr_df\n#     sentence_embeddings = all_embeddings[i].cpu().numpy()  # Shape: (num_sentences_in_row, embedding_dim)\n#     if len(sentence_embeddings > 0):\n#         # Step 2: Get the corresponding sentence IDs for this row\n#         sentence_ids = ast.literal_eval(new_df['retrieved_sentence_ids'].iloc[i])  # List of sentence IDs corresponding to the embeddings\n\n#         # Step 3: Calculate cosine similarity between sentence embeddings and knowledge embeddings\n#         similarity_scores = cosine_similarity(sentence_embeddings,  entity_embeddings.cpu().numpy())  # Shape: (embedding_dim, num_sentences_in_row)\n\n#     #     print(similarity_scores.shape)\n#         # Step 4: Process each sentence\n#     #     top_sentence_ids = []\n#         top_entities = []\n#         top_scores = []\n\n#         for j in range(len(sentence_ids)):\n#             # Get similarity scores for the current sentence\n#             sentence_similarity_scores = similarity_scores[j]\n\n#             # Filter based on similarity threshold\n#             filtered_indices = [idx for idx, score in enumerate(sentence_similarity_scores) if score >= similarity_threshold]\n#             # Sort by similarity score and get top k entities\n#             top_indice = sorted(filtered_indices, key=lambda idx: sentence_similarity_scores[idx], reverse=True)[0]\n#     #         if i < 1: \n#     #             print(top_indice)\n#             # Get the top entities and their corresponding IDs\n#             top_entities_for_sentence = knowledge_entity_ids[top_indice]\n#     #         top_scores_for_sentence = [sentence_similarity_scores[idx] for idx in top_indices]\n\n#             # Store results for this sentence\n#     #         sentence_ids.append(sentence_ids[j])\n#             top_entities.append(top_entities_for_sentence)\n#     #         top_scores.append(top_scores_for_sentence)\n\n#         # Flatten the results and count matches for positive entities\n#     #     pos_sentence_ids = ast.literal_eval(input_df['positive_sentence_ids'].iloc[i])\n#     #     count = sum(e1 in top_entity for top_entities_for_sentence in top_entities for e1 in pos_sentence_ids)\n#     else:\n#         top_entities = []\n#     # Step 7: Store the result for this row\n#     result_data.append({\n#         'q_id': row['q_id'],\n#         'sentence_ids': sentence_ids,\n#         'top_entities': top_entities\n# #         'similarity_scores': top_scores,\n# #         'bool_c': count,\n# #         'bool_t': len(pos_sentence_ids)\n#     })\n\n# # Optionally convert the result to a DataFrame\n# result_df = pd.DataFrame(result_data)\n\n# # Print the result\n# print(result_df['q_id'].value_counts())","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-10T02:51:28.024314Z","iopub.execute_input":"2024-09-10T02:51:28.025140Z","iopub.status.idle":"2024-09-10T02:51:28.033987Z","shell.execute_reply.started":"2024-09-10T02:51:28.025087Z","shell.execute_reply":"2024-09-10T02:51:28.032681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result_df.to_csv(\"help.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:40:14.241484Z","iopub.execute_input":"2024-09-10T02:40:14.241935Z","iopub.status.idle":"2024-09-10T02:40:14.249260Z","shell.execute_reply.started":"2024-09-10T02:40:14.241894Z","shell.execute_reply":"2024-09-10T02:40:14.248169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Partial Knowledge","metadata":{}},{"cell_type":"code","source":"entity_df['Knowledge_20'] = entity_df['Knowledge'].apply(lambda x: x[:int(len(x) * 0.2)])\nquestion_embeddings = model.encode(input_df['question'].tolist(), convert_to_tensor=True, device=device)\nentity_knowledge_embeddings = model.encode(entity_df['Knowledge_20'].tolist(), convert_to_tensor=True, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:23:13.503339Z","iopub.execute_input":"2024-09-09T19:23:13.503734Z","iopub.status.idle":"2024-09-09T19:23:13.734679Z","shell.execute_reply.started":"2024-09-09T19:23:13.503697Z","shell.execute_reply":"2024-09-09T19:23:13.733735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Assume `knowledge_embeddings` contains embeddings of all entities\n# Assume `knowledge_entity_ids` contains the IDs corresponding to the knowledge embeddings\n\nfor z in range(2,3):\n    for so in range(10,30):\n        dc = {}\n        top_k = 20  # Define how many top entities you want to retrieve\n        similarity_threshold = 0.01 * so  # Define the similarity threshold\n        similarity_threshold = round(similarity_threshold,2)\n        result_data = []\n\n        for i, row in input_df.iterrows():\n            # Step 1: Get the sentence embeddings for the current row from the asr_df\n            question_embedding = question_embeddings[i].unsqueeze(0).cpu().numpy()  # Shape: (num_sentences_in_row, embedding_dim)\n\n            # Step 2: Get the corresponding sentence IDs for this row\n            sentence_ids = ast.literal_eval(asr_df['Sentence_ids'].iloc[i])\n\n            entity_ids = ast.literal_eval(NER_df['top_entities'].iloc[i])\n            sim_scores = []\n            emt = []\n            for j in range(len(sentence_ids)):\n                # Get similarity scores for the current sentence\n        #         sentence_similarity_scores = similarity_scores[j]\n                sentence_id = sentence_ids[j]\n                entity_id = entity_ids[j]\n                emt.append(entity_id)\n                ind = entity_df[entity_df['Entity_ID'] == entity_id].index\n                embed = entity_knowledge_embeddings[ind]\n                similarity_scores = cosine_similarity(question_embedding,  embed.cpu().numpy()).flatten() \n                sim_scores.append(similarity_scores)\n                # Filter based on similarity threshold\n            filtered_indices = [idx for idx, score in enumerate(sim_scores) if score >= similarity_threshold]\n            top_indices = sorted(filtered_indices, key=lambda idx: sim_scores[idx], reverse=True)[:top_k]\n            pos_sentence_ids = ast.literal_eval(input_df['positive_sentence_ids'].iloc[i])\n#             emt = emt[top_indices]\n            count = 0\n            for e1 in pos_sentence_ids:\n                if e1 in top_indices:\n                    count+=1\n            result_data.append({\n                'q_id': row['QID'],\n                'sentence_ids': sentence_ids,\n                'retrieved_sentence_ids': top_indices,\n                'retrieved_sentence_entity_ids': [emt[idx] for idx in top_indices],\n                'similarity_scores': [sim_scores[idx][0] for idx in top_indices],\n                'ground_truth_positive_sentence_ids': pos_sentence_ids,\n                'ground_truth_positive_sentence_entity_ids': ast.literal_eval(row['entities']),\n                'all_sentences': ast.literal_eval(row['all_sentences']),\n                'answer_num': len(pos_sentence_ids),\n                'answer':'answer',\n                'bool_c' : count,\n                'bool_t': len(pos_sentence_ids)\n            })\n\n        # Optionally convert the result to a DataFrame\n        result_df = pd.DataFrame(result_data)\n        a, b, c = calc(result_df, top_k, similarity_threshold)\n        dc[top_k, similarity_threshold] = a,b,c\n        ojjj = 0\n        for i , row in result_df.iterrows():\n#                 kjsh = ast.literal_eval(row['top_sentence_ids'])\n            kjsh = row['retrieved_sentence_ids']\n            ojjj += len(kjsh)\n        print(f'For threshold = {similarity_threshold}: average_result_length = {ojjj/len(result_df)}')\n# Print the result\n# print(result_df.head())\n        for it in dc.items():\n            print(it)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:23:35.091587Z","iopub.execute_input":"2024-09-09T19:23:35.092301Z","iopub.status.idle":"2024-09-09T19:24:00.641433Z","shell.execute_reply.started":"2024-09-09T19:23:35.092261Z","shell.execute_reply":"2024-09-09T19:24:00.640486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for it in dc.items():\n    print(it)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:11:57.584588Z","iopub.execute_input":"2024-09-09T19:11:57.584969Z","iopub.status.idle":"2024-09-09T19:11:57.589931Z","shell.execute_reply.started":"2024-09-09T19:11:57.584931Z","shell.execute_reply":"2024-09-09T19:11:57.588997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.to_csv(\"Task3_CLIP_partial_knowledge_Retrieved_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-08T16:16:54.284363Z","iopub.execute_input":"2024-09-08T16:16:54.284769Z","iopub.status.idle":"2024-09-08T16:16:54.295545Z","shell.execute_reply.started":"2024-09-08T16:16:54.284725Z","shell.execute_reply":"2024-09-08T16:16:54.294730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Entity Labels only","metadata":{}},{"cell_type":"code","source":"question_embeddings = model.encode(input_df['question'].tolist(), convert_to_tensor=True, device=device)\nentity_knowledge_embeddings = model.encode(entity_df['Entity_name'].tolist(), convert_to_tensor=True, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:19:10.854029Z","iopub.execute_input":"2024-09-09T19:19:10.854790Z","iopub.status.idle":"2024-09-09T19:19:11.042706Z","shell.execute_reply.started":"2024-09-09T19:19:10.854753Z","shell.execute_reply":"2024-09-09T19:19:11.041801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Assume `knowledge_embeddings` contains embeddings of all entities\n# Assume `knowledge_entity_ids` contains the IDs corresponding to the knowledge embeddings\n\nfor z in range(2,3):\n    for so in range(1,2):\n        dc = {}\n        top_k = 20  # Define how many top entities you want to retrieve\n        similarity_threshold = 0.15 * so  # Define the similarity threshold\n        similarity_threshold = round(similarity_threshold,2)\n        result_data = []\n\n        for i, row in input_df.iterrows():\n            # Step 1: Get the sentence embeddings for the current row from the asr_df\n            question_embedding = question_embeddings[i].unsqueeze(0).cpu().numpy()  # Shape: (num_sentences_in_row, embedding_dim)\n\n            # Step 2: Get the corresponding sentence IDs for this row\n            sentence_ids = ast.literal_eval(asr_df['Sentence_ids'].iloc[i])\n\n            entity_ids = ast.literal_eval(NER_df['top_entities'].iloc[i])\n            sim_scores = []\n            emt = []\n            for j in range(len(sentence_ids)):\n                # Get similarity scores for the current sentence\n        #         sentence_similarity_scores = similarity_scores[j]\n                sentence_id = sentence_ids[j]\n                entity_id = entity_ids[j]\n                emt.append(entity_id)\n                ind = entity_df[entity_df['Entity_ID'] == entity_id].index\n                embed = entity_knowledge_embeddings[ind]\n                similarity_scores = cosine_similarity(question_embedding,  embed.cpu().numpy()).flatten() \n                sim_scores.append(similarity_scores)\n                # Filter based on similarity threshold\n            filtered_indices = [idx for idx, score in enumerate(sim_scores) if score >= similarity_threshold]\n            top_indices = sorted(filtered_indices, key=lambda idx: sim_scores[idx], reverse=True)[:top_k]\n            pos_sentence_ids = ast.literal_eval(input_df['positive_sentence_ids'].iloc[i])\n#             emt = emt[top_indices]\n            count = 0\n            for e1 in pos_sentence_ids:\n                if e1 in top_indices:\n                    count+=1\n            result_data.append({\n                'q_id': row['QID'],\n                'sentence_ids': sentence_ids,\n                'retrieved_sentence_ids': top_indices,\n                'retrieved_sentence_entity_ids': [emt[idx] for idx in top_indices],\n                'similarity_scores': [sim_scores[idx][0] for idx in top_indices],\n                'ground_truth_positive_sentence_ids': pos_sentence_ids,\n                'ground_truth_positive_sentence_entity_ids': ast.literal_eval(row['entities']),\n                'all_sentences': ast.literal_eval(row['all_sentences']),\n                'answer_num': len(pos_sentence_ids),\n                'answer':'answer',\n                'bool_c' : count,\n                'bool_t': len(pos_sentence_ids)\n            })\n\n        # Optionally convert the result to a DataFrame\n        result_df = pd.DataFrame(result_data)\n        a, b, c = calc(result_df, top_k, similarity_threshold)\n        dc[top_k, similarity_threshold] = a,b,c\n        ojjj = 0\n        for i , row in result_df.iterrows():\n#                 kjsh = ast.literal_eval(row['top_sentence_ids'])\n            kjsh = row['retrieved_sentence_ids']\n            ojjj += len(kjsh)\n        print(f'For threshold = {similarity_threshold}: average_result_length = {ojjj/len(result_df)}')\n# Print the result\n# print(result_df.head())\n        for it in dc.items():\n            print(it)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:19:12.381450Z","iopub.execute_input":"2024-09-09T19:19:12.382218Z","iopub.status.idle":"2024-09-09T19:19:13.710703Z","shell.execute_reply.started":"2024-09-09T19:19:12.382177Z","shell.execute_reply":"2024-09-09T19:19:13.709742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:16:21.056137Z","iopub.execute_input":"2024-09-09T19:16:21.056854Z","iopub.status.idle":"2024-09-09T19:16:21.061578Z","shell.execute_reply.started":"2024-09-09T19:16:21.056812Z","shell.execute_reply":"2024-09-09T19:16:21.060575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.to_csv(\"Task3_CLIP_label_Retrieved_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-09T19:19:19.562609Z","iopub.execute_input":"2024-09-09T19:19:19.563472Z","iopub.status.idle":"2024-09-09T19:19:19.576489Z","shell.execute_reply.started":"2024-09-09T19:19:19.563429Z","shell.execute_reply":"2024-09-09T19:19:19.575750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# for i in range(2,4):\n#     for j in range(i+1):\n#         # Use '&' for element-wise comparison and wrap conditions in parentheses\n#         elem_rows = ((result_df['bool_c'] == j) & (result_df['bool_t'] == i)).sum()\n#         print(f'Total number of rows where predicted:{j} and total:{i} : {elem_rows}')\n\n\n# equal_rows = (result_df['bool_c'] == result_df['bool_t']).sum()\n\n# print(f'Total number of rows where values are equal: {equal_rows}')\n\n# less_rows = (result_df['bool_c'] < result_df['bool_t']).sum()\n\n# print(f'Total number of rows where values are less: {less_rows}')\n\n# print(f'values in bool_c: {result_df[\"bool_c\"].value_counts()}')","metadata":{"execution":{"iopub.status.busy":"2024-09-08T13:50:31.162165Z","iopub.status.idle":"2024-09-08T13:50:31.162734Z","shell.execute_reply.started":"2024-09-08T13:50:31.162439Z","shell.execute_reply":"2024-09-08T13:50:31.162468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Without NER","metadata":{}},{"cell_type":"code","source":"# print(all_embeddings[0].shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:38:17.947934Z","iopub.execute_input":"2024-09-08T11:38:17.948702Z","iopub.status.idle":"2024-09-08T11:38:17.952675Z","shell.execute_reply.started":"2024-09-08T11:38:17.948659Z","shell.execute_reply":"2024-09-08T11:38:17.951614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(input_df.columns)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:38:19.205802Z","iopub.execute_input":"2024-09-08T11:38:19.206661Z","iopub.status.idle":"2024-09-08T11:38:19.212182Z","shell.execute_reply.started":"2024-09-08T11:38:19.206617Z","shell.execute_reply":"2024-09-08T11:38:19.211089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# top_k = 6  # Define how many top sentences you want to retrieve\n# similarity_threshold = 0.05  # Define the similarity threshold\n\n# result_data = []\n\n# # Iterate over each row in input_df\n# for i, row in input_df.iterrows():\n#     # Step 1: Get the question embedding for the current row\n#     question_embedding = question_embeddings[i].unsqueeze(0).cpu().numpy()\n    \n#     # Step 2: Get the sentence embeddings for the current row from the asr_df\n#     # Assuming `all_embeddings` is the list of lists containing sentence embeddings for each row\n#     sentence_embeddings = all_embeddings[i].cpu().numpy()  # Shape: (num_sentences_in_row, embedding_dim)\n    \n#     # Step 3: Get the corresponding sentence IDs for this row\n#     sentence_ids = ast.literal_eval(asr_df['Sentence_ids'].iloc[i])  # List of sentence IDs corresponding to the embeddings\n\n#     # Step 4: Calculate cosine similarity between the question embedding and the sentence embeddings\n#     similarity_scores = cosine_similarity(question_embedding, sentence_embeddings).flatten()\n    \n#     # Step 5: Filter based on similarity threshold\n#     filtered_indices = [idx for idx, score in enumerate(similarity_scores) if score >= similarity_threshold]\n    \n#     # Step 6: Sort by similarity score and get top k sentences\n#     top_indices = sorted(filtered_indices, key=lambda idx: similarity_scores[idx], reverse=True)[:top_k]\n    \n#     # Get the top k sentences and their corresponding sentence IDs\n#     top_sentence_ids = [sentence_ids[idx] for idx in top_indices]  # Corresponding sentence IDs\n#     top_sentences = [ast.literal_eval(asr_df['transcriptions'].iloc[i])[idx] for idx in top_indices]  # Sentences corresponding to top indices\n#     pos_sentence_ids = ast.literal_eval(input_df['positive_sentence_ids'].iloc[i])\n#     count = 0\n#     for e1 in pos_sentence_ids:\n#         if e1 in top_sentence_ids:\n#             count+=1\n# #     print(pos_sentence_ids.all() in top_sentence_ids)\n#     # Step 7: Store the result for this row\n#     result_data.append({\n#         'q_id': row['QID'],\n#         'top_sentence_ids': top_sentence_ids,\n#         'top_sentences': top_sentences,\n#         'similarity_scores': [similarity_scores[idx] for idx in top_indices],\n#         'bool_c' : count,\n#         'bool_t': len(pos_sentence_ids)\n#     })\n\n# # Optionally convert the result to a DataFrame\n# result_df = pd.DataFrame(result_data)\n\n# # Print the result\n# print(result_df.head())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:02:10.053667Z","iopub.execute_input":"2024-09-08T11:02:10.053989Z","iopub.status.idle":"2024-09-08T11:02:10.210283Z","shell.execute_reply.started":"2024-09-08T11:02:10.053958Z","shell.execute_reply":"2024-09-08T11:02:10.209390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(2,4):\n#     for j in range(i+1):\n#         # Use '&' for element-wise comparison and wrap conditions in parentheses\n#         elem_rows = ((result_df['bool_c'] == j) & (result_df['bool_t'] == i)).sum()\n\n#         print(f'Total number of rows where predicted:{j} and total:{i} : {elem_rows}')\n\n\n# equal_rows = (result_df['bool_c'] == result_df['bool_t']).sum()\n\n# print(f'Total number of rows where values are equal: {equal_rows}')\n\n# less_rows = (result_df['bool_c'] < result_df['bool_t']).sum()\n\n# print(f'Total number of rows where values are less: {less_rows}')\n\n# print(f'values in bool_c: {result_df[\"bool_c\"].value_counts()}')","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:02:10.211434Z","iopub.execute_input":"2024-09-08T11:02:10.211725Z","iopub.status.idle":"2024-09-08T11:02:10.231988Z","shell.execute_reply.started":"2024-09-08T11:02:10.211693Z","shell.execute_reply":"2024-09-08T11:02:10.230987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NER first ","metadata":{}},{"cell_type":"code","source":"entity_embeddings = model.encode(entity_df['Knowledge'].tolist(), convert_to_tensor=True, device=device)\nknowledge_entity_ids = entity_df['Entity_ID'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:03:37.492200Z","iopub.execute_input":"2024-09-08T11:03:37.493057Z","iopub.status.idle":"2024-09-08T11:03:37.881246Z","shell.execute_reply.started":"2024-09-08T11:03:37.493016Z","shell.execute_reply":"2024-09-08T11:03:37.880284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport ast\n\n# top_k = 6  # Define how many top entities you want to retrieve\nsimilarity_threshold = 0.05  # Define the similarity threshold\n\nresult_data = []\n\n# Assume `knowledge_embeddings` contains embeddings of all entities\n# Assume `knowledge_entity_ids` contains the IDs corresponding to the knowledge embeddings\n\nfor i, row in input_df.iterrows():\n    # Step 1: Get the sentence embeddings for the current row from the asr_df\n    sentence_embeddings = all_embeddings[i].cpu().numpy()  # Shape: (num_sentences_in_row, embedding_dim)\n    \n    # Step 2: Get the corresponding sentence IDs for this row\n    sentence_ids = ast.literal_eval(asr_df['Sentence_ids'].iloc[i])  # List of sentence IDs corresponding to the embeddings\n    \n    # Step 3: Calculate cosine similarity between sentence embeddings and knowledge embeddings\n    similarity_scores = cosine_similarity(sentence_embeddings,  entity_embeddings.cpu().numpy())  # Shape: (embedding_dim, num_sentences_in_row)\n    \n#     print(similarity_scores.shape)\n    # Step 4: Process each sentence\n    top_sentence_ids = []\n    top_entities = []\n    top_scores = []\n    \n    for j in range(len(sentence_ids)):\n        # Get similarity scores for the current sentence\n        sentence_similarity_scores = similarity_scores[j]\n        \n        # Filter based on similarity threshold\n        filtered_indices = [idx for idx, score in enumerate(sentence_similarity_scores) if score >= similarity_threshold]\n        \n        # Sort by similarity score and get top k entities\n        top_indice = sorted(filtered_indices, key=lambda idx: sentence_similarity_scores[idx], reverse=True)[0]\n#         if i < 1: \n#             print(top_indice)\n        # Get the top entities and their corresponding IDs\n        top_entities_for_sentence = knowledge_entity_ids[top_indice]\n#         top_scores_for_sentence = [sentence_similarity_scores[idx] for idx in top_indices]\n        \n        # Store results for this sentence\n#         sentence_ids.append(sentence_ids[j])\n        top_entities.append(top_entities_for_sentence)\n#         top_scores.append(top_scores_for_sentence)\n    \n    # Flatten the results and count matches for positive entities\n#     pos_sentence_ids = ast.literal_eval(input_df['positive_sentence_ids'].iloc[i])\n#     count = sum(e1 in top_entity for top_entities_for_sentence in top_entities for e1 in pos_sentence_ids)\n    \n    # Step 7: Store the result for this row\n    result_data.append({\n        'q_id': row['QID'],\n        'sentence_ids': sentence_ids,\n        'top_entities': top_entities\n#         'similarity_scores': top_scores,\n#         'bool_c': count,\n#         'bool_t': len(pos_sentence_ids)\n    })\n\n# Optionally convert the result to a DataFrame\nresult_df = pd.DataFrame(result_data)\n\n# Print the result\nprint(result_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:17:33.274393Z","iopub.execute_input":"2024-09-08T11:17:33.274895Z","iopub.status.idle":"2024-09-08T11:17:38.170530Z","shell.execute_reply.started":"2024-09-08T11:17:33.274859Z","shell.execute_reply":"2024-09-08T11:17:38.169168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.to_csv(\"Task_3_asr_NER.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:18:35.551156Z","iopub.execute_input":"2024-09-08T11:18:35.551887Z","iopub.status.idle":"2024-09-08T11:18:35.559865Z","shell.execute_reply.started":"2024-09-08T11:18:35.551847Z","shell.execute_reply":"2024-09-08T11:18:35.559104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Full Knowledge","metadata":{}},{"cell_type":"code","source":"# Step 2: Encode the input sentences and entity knowledge strings using GPU\ninput1_embeddings = model.encode(asr_df['Sentence_1_transcript'].tolist(), convert_to_tensor=True, device=device)\ninput2_embeddings = model.encode(asr_df['Sentence_2_transcript'].tolist(), convert_to_tensor=True, device=device)\nentity_embeddings = model.encode(entity_df['Knowledge'].tolist(), convert_to_tensor=True, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T13:52:20.513461Z","iopub.execute_input":"2024-09-05T13:52:20.513839Z","iopub.status.idle":"2024-09-05T13:52:21.241202Z","shell.execute_reply.started":"2024-09-05T13:52:20.513793Z","shell.execute_reply":"2024-09-05T13:52:21.240262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize counters for ranking accuracy\ntop1_correct = 0\ntop5_correct = 0\ntop10_correct = 0\n\nres_data = []\n\n# Step 3: Calculate cosine similarity and evaluate rankings\nfor i, q_id in enumerate(input_df['QID']):\n    # Get the similarity scores for the current sentence with all entities\n    similarity_scores1 = cosine_similarity(input1_embeddings[i].unsqueeze(0).cpu().numpy(), entity_embeddings.cpu().numpy()).flatten()\n    similarity_scores2 = cosine_similarity(input2_embeddings[i].unsqueeze(0).cpu().numpy(), entity_embeddings.cpu().numpy()).flatten()\n    # Get the indices of entities sorted by similarity score (descending order)\n    ranked_entity_indices1 = similarity_scores1.argsort()[::-1]\n    ranked_entity_indices2 = similarity_scores2.argsort()[::-1]\n    # Get the ranked entity names\n    ranked_entity_names1 = entity_df['Entity_name'].iloc[ranked_entity_indices1]\n    ranked_entity_names2 = entity_df['Entity_name'].iloc[ranked_entity_indices2]\n\n    # Get the actual entity name for the current sentence\n    actual_entity_name1 = input_df['sentence_1_entity_name'].iloc[i]\n    actual_entity_name2 = input_df['sentence_2_entity_name'].iloc[i]\n    \n    res_data.append(\n    {\n        'q_id': q_id,\n        'linked_entity_sentnece_1': ranked_entity_names1.iloc[0],\n        'actual_entity_sentence_1' : actual_entity_name1,\n        'linked_entity_sentnece_2': ranked_entity_names2.iloc[0],\n        'actual_entity_sentence_2' : actual_entity_name2,\n    }\n    )\n    # Step 4: Check if the actual entity is within the top 1, 5, and 10\n    if actual_entity_name1 == ranked_entity_names1.iloc[0]:\n        top1_correct += 1\n    if actual_entity_name1 in ranked_entity_names1.iloc[:5].values:\n        top5_correct += 1\n    if actual_entity_name1 in ranked_entity_names1.iloc[:10].values:\n        top10_correct += 1\n        \n    if actual_entity_name2 == ranked_entity_names2.iloc[0]:\n        top1_correct += 1\n    if actual_entity_name2 in ranked_entity_names2.iloc[:5].values:\n        top5_correct += 1\n    if actual_entity_name2 in ranked_entity_names2.iloc[:10].values:\n        top10_correct += 1\n\n\n# Step 5: Calculate and print ranking accuracies\ntotal_sentences = len(input_df)\n\ntop1_accuracy = top1_correct / total_sentences * 50\ntop5_accuracy = top5_correct / total_sentences * 50\ntop10_accuracy = top10_correct / total_sentences * 50\n\nprint(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\nprint(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")\nprint(f\"Top-10 Accuracy: {top10_accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-09-05T13:52:21.243682Z","iopub.execute_input":"2024-09-05T13:52:21.243986Z","iopub.status.idle":"2024-09-05T13:52:23.712423Z","shell.execute_reply.started":"2024-09-05T13:52:21.243954Z","shell.execute_reply":"2024-09-05T13:52:23.710871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner_df = pd.DataFrame(res_data)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T13:52:23.714648Z","iopub.execute_input":"2024-09-05T13:52:23.715704Z","iopub.status.idle":"2024-09-05T13:52:23.725377Z","shell.execute_reply.started":"2024-09-05T13:52:23.715629Z","shell.execute_reply":"2024-09-05T13:52:23.723977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner_df.to_csv(\"Task2_asr_entity_linked_CLIP_full_knowledge_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-05T13:52:23.728236Z","iopub.execute_input":"2024-09-05T13:52:23.729571Z","iopub.status.idle":"2024-09-05T13:52:23.743875Z","shell.execute_reply.started":"2024-09-05T13:52:23.729501Z","shell.execute_reply":"2024-09-05T13:52:23.742275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Partial Knowledge","metadata":{}},{"cell_type":"code","source":"# Step 2: Encode the input sentences and entity knowledge strings using GPU\ninput1_embeddings = model.encode(asr_df['Sentence_1_transcript'].tolist(), convert_to_tensor=True, device=device)\ninput2_embeddings = model.encode(asr_df['Sentence_2_transcript'].tolist(), convert_to_tensor=True, device=device)\n\n# Calculate the substring for the first 20% of each knowledge string\nentity_df['Knowledge_20'] = entity_df['Knowledge'].apply(lambda x: x[:int(len(x) * 0.2)])\n\n# Encode the first 20% of the knowledge strings\nentity_embeddings = model.encode(entity_df['Knowledge_20'].tolist(), convert_to_tensor=True, device=device)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-05T13:52:23.745917Z","iopub.execute_input":"2024-09-05T13:52:23.746981Z","iopub.status.idle":"2024-09-05T13:52:24.331711Z","shell.execute_reply.started":"2024-09-05T13:52:23.746919Z","shell.execute_reply":"2024-09-05T13:52:24.33075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize counters for ranking accuracy\ntop1_correct = 0\ntop5_correct = 0\ntop10_correct = 0\n\nres_data = []\n\n# Step 3: Calculate cosine similarity and evaluate rankings\nfor i, q_id in enumerate(input_df['QID']):\n    # Get the similarity scores for the current sentence with all entities\n    similarity_scores1 = cosine_similarity(input1_embeddings[i].unsqueeze(0).cpu().numpy(), entity_embeddings.cpu().numpy()).flatten()\n    similarity_scores2 = cosine_similarity(input2_embeddings[i].unsqueeze(0).cpu().numpy(), entity_embeddings.cpu().numpy()).flatten()\n    # Get the indices of entities sorted by similarity score (descending order)\n    ranked_entity_indices1 = similarity_scores1.argsort()[::-1]\n    ranked_entity_indices2 = similarity_scores2.argsort()[::-1]\n    # Get the ranked entity names\n    ranked_entity_names1 = entity_df['Entity_name'].iloc[ranked_entity_indices1]\n    ranked_entity_names2 = entity_df['Entity_name'].iloc[ranked_entity_indices2]\n\n    # Get the actual entity name for the current sentence\n    actual_entity_name1 = input_df['sentence_1_entity_name'].iloc[i]\n    actual_entity_name2 = input_df['sentence_2_entity_name'].iloc[i]\n    \n    res_data.append(\n    {\n        'q_id': q_id,\n        'linked_entity_sentnece_1': ranked_entity_names1.iloc[0],\n        'actual_entity_sentence_1' : actual_entity_name1,\n        'linked_entity_sentnece_2': ranked_entity_names2.iloc[0],\n        'actual_entity_sentence_2' : actual_entity_name2,\n    }\n    )\n    # Step 4: Check if the actual entity is within the top 1, 5, and 10\n    if actual_entity_name1 == ranked_entity_names1.iloc[0]:\n        top1_correct += 1\n    if actual_entity_name1 in ranked_entity_names1.iloc[:5].values:\n        top5_correct += 1\n    if actual_entity_name1 in ranked_entity_names1.iloc[:10].values:\n        top10_correct += 1\n        \n    if actual_entity_name2 == ranked_entity_names2.iloc[0]:\n        top1_correct += 1\n    if actual_entity_name2 in ranked_entity_names2.iloc[:5].values:\n        top5_correct += 1\n    if actual_entity_name2 in ranked_entity_names2.iloc[:10].values:\n        top10_correct += 1\n\n\n# Step 5: Calculate and print ranking accuracies\ntotal_sentences = len(input_df)\n\ntop1_accuracy = top1_correct / total_sentences * 50\ntop5_accuracy = top5_correct / total_sentences * 50\ntop10_accuracy = top10_correct / total_sentences * 50\n\nprint(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\nprint(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")\nprint(f\"Top-10 Accuracy: {top10_accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-09-05T13:52:24.333126Z","iopub.execute_input":"2024-09-05T13:52:24.33343Z","iopub.status.idle":"2024-09-05T13:52:26.774663Z","shell.execute_reply.started":"2024-09-05T13:52:24.333397Z","shell.execute_reply":"2024-09-05T13:52:26.77351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner_df = pd.DataFrame(res_data)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T13:52:26.776278Z","iopub.execute_input":"2024-09-05T13:52:26.776957Z","iopub.status.idle":"2024-09-05T13:52:26.784902Z","shell.execute_reply.started":"2024-09-05T13:52:26.776905Z","shell.execute_reply":"2024-09-05T13:52:26.78356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner_df.to_csv(\"Task2_asr_entity_linked_CLIP_partial_knowledge_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-05T13:52:26.789632Z","iopub.execute_input":"2024-09-05T13:52:26.790461Z","iopub.status.idle":"2024-09-05T13:52:26.801366Z","shell.execute_reply.started":"2024-09-05T13:52:26.790414Z","shell.execute_reply":"2024-09-05T13:52:26.800115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Labels Only","metadata":{}},{"cell_type":"code","source":"# Step 2: Encode the input sentences and entity knowledge strings using GPU\ninput1_embeddings = model.encode(asr_df['Sentence_1_transcript'].tolist(), convert_to_tensor=True, device=device)\ninput2_embeddings = model.encode(asr_df['Sentence_2_transcript'].tolist(), convert_to_tensor=True, device=device)\nentity_embeddings = model.encode(entity_df['Entity_name'].tolist(), convert_to_tensor=True, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T13:52:26.803024Z","iopub.execute_input":"2024-09-05T13:52:26.803832Z","iopub.status.idle":"2024-09-05T13:52:27.40843Z","shell.execute_reply.started":"2024-09-05T13:52:26.803789Z","shell.execute_reply":"2024-09-05T13:52:27.407411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize counters for ranking accuracy\ntop1_correct = 0\ntop5_correct = 0\ntop10_correct = 0\n\nres_data = []\n\n# Step 3: Calculate cosine similarity and evaluate rankings\nfor i, q_id in enumerate(input_df['QID']):\n    # Get the similarity scores for the current sentence with all entities\n    similarity_scores1 = cosine_similarity(input1_embeddings[i].unsqueeze(0).cpu().numpy(), entity_embeddings.cpu().numpy()).flatten()\n    similarity_scores2 = cosine_similarity(input2_embeddings[i].unsqueeze(0).cpu().numpy(), entity_embeddings.cpu().numpy()).flatten()\n    # Get the indices of entities sorted by similarity score (descending order)\n    ranked_entity_indices1 = similarity_scores1.argsort()[::-1]\n    ranked_entity_indices2 = similarity_scores2.argsort()[::-1]\n    # Get the ranked entity names\n    ranked_entity_names1 = entity_df['Entity_name'].iloc[ranked_entity_indices1]\n    ranked_entity_names2 = entity_df['Entity_name'].iloc[ranked_entity_indices2]\n\n    # Get the actual entity name for the current sentence\n    actual_entity_name1 = input_df['sentence_1_entity_name'].iloc[i]\n    actual_entity_name2 = input_df['sentence_2_entity_name'].iloc[i]\n    \n    res_data.append(\n    {\n        'q_id': q_id,\n        'linked_entity_sentnece_1': ranked_entity_names1.iloc[0],\n        'actual_entity_sentence_1' : actual_entity_name1,\n        'linked_entity_sentnece_2': ranked_entity_names2.iloc[0],\n        'actual_entity_sentence_2' : actual_entity_name2,\n    }\n    )\n    # Step 4: Check if the actual entity is within the top 1, 5, and 10\n    if actual_entity_name1 == ranked_entity_names1.iloc[0]:\n        top1_correct += 1\n    if actual_entity_name1 in ranked_entity_names1.iloc[:5].values:\n        top5_correct += 1\n    if actual_entity_name1 in ranked_entity_names1.iloc[:10].values:\n        top10_correct += 1\n        \n    if actual_entity_name2 == ranked_entity_names2.iloc[0]:\n        top1_correct += 1\n    if actual_entity_name2 in ranked_entity_names2.iloc[:5].values:\n        top5_correct += 1\n    if actual_entity_name2 in ranked_entity_names2.iloc[:10].values:\n        top10_correct += 1\n\n\n# Step 5: Calculate and print ranking accuracies\ntotal_sentences = len(input_df)\n\ntop1_accuracy = top1_correct / total_sentences * 50\ntop5_accuracy = top5_correct / total_sentences * 50\ntop10_accuracy = top10_correct / total_sentences * 50\n\nprint(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\nprint(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")\nprint(f\"Top-10 Accuracy: {top10_accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-09-05T13:52:27.410118Z","iopub.execute_input":"2024-09-05T13:52:27.41052Z","iopub.status.idle":"2024-09-05T13:52:29.877729Z","shell.execute_reply.started":"2024-09-05T13:52:27.410476Z","shell.execute_reply":"2024-09-05T13:52:29.876437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner_df = pd.DataFrame(res_data)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T13:52:29.879533Z","iopub.execute_input":"2024-09-05T13:52:29.880655Z","iopub.status.idle":"2024-09-05T13:52:29.889322Z","shell.execute_reply.started":"2024-09-05T13:52:29.880587Z","shell.execute_reply":"2024-09-05T13:52:29.887962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner_df.to_csv(\"Task2_asr_entity_linked_CLIP_label_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-05T13:52:29.891228Z","iopub.execute_input":"2024-09-05T13:52:29.892179Z","iopub.status.idle":"2024-09-05T13:52:29.905998Z","shell.execute_reply.started":"2024-09-05T13:52:29.892116Z","shell.execute_reply":"2024-09-05T13:52:29.904696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}