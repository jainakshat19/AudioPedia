{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9273248,"sourceType":"datasetVersion","datasetId":5543066}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-29T10:25:19.149790Z","iopub.execute_input":"2024-08-29T10:25:19.150806Z","iopub.status.idle":"2024-08-29T10:25:19.717687Z","shell.execute_reply.started":"2024-08-29T10:25:19.150746Z","shell.execute_reply":"2024-08-29T10:25:19.715941Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install torch transformers","metadata":{"execution":{"iopub.status.busy":"2024-08-29T10:31:42.238524Z","iopub.execute_input":"2024-08-29T10:31:42.239924Z","iopub.status.idle":"2024-08-29T10:31:59.468553Z","shell.execute_reply.started":"2024-08-29T10:31:42.239840Z","shell.execute_reply":"2024-08-29T10:31:59.466895Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0+cpu)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom transformers import Wav2Vec2Model, Wav2Vec2Processor\nimport os\nimport torch.nn.functional as F\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-29T10:39:52.015148Z","iopub.execute_input":"2024-08-29T10:39:52.015668Z","iopub.status.idle":"2024-08-29T10:39:52.022508Z","shell.execute_reply.started":"2024-08-29T10:39:52.015615Z","shell.execute_reply":"2024-08-29T10:39:52.020788Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:57:58.269798Z","iopub.execute_input":"2024-08-29T11:57:58.270330Z","iopub.status.idle":"2024-08-29T11:57:58.277721Z","shell.execute_reply.started":"2024-08-29T11:57:58.270282Z","shell.execute_reply":"2024-08-29T11:57:58.276189Z"},"trusted":true},"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"entity_audio_dir = \"/kaggle/input/business-json/entity_aud_files/kaggle/working/entity_aud_files\"\ndataset_audio_dir = '/kaggle/input/business-json/aud_files/kaggle/working/TTS/aud_files'","metadata":{"execution":{"iopub.status.busy":"2024-08-29T10:38:44.536551Z","iopub.execute_input":"2024-08-29T10:38:44.537041Z","iopub.status.idle":"2024-08-29T10:38:44.543351Z","shell.execute_reply.started":"2024-08-29T10:38:44.536998Z","shell.execute_reply":"2024-08-29T10:38:44.541856Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Load Wav2Vec2\nwav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\nwav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\n# Load ImageBind (hypothetical code, replace with the actual model)\n# imagebind_model = ImageBindModel.from_pretrained(\"path_to_imagebind_model\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T10:37:38.654186Z","iopub.execute_input":"2024-08-29T10:37:38.654656Z","iopub.status.idle":"2024-08-29T10:37:42.670481Z","shell.execute_reply.started":"2024-08-29T10:37:38.654615Z","shell.execute_reply":"2024-08-29T10:37:42.669491Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"818cd362a65d470c88b5352f2d0ff880"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cca148b7ce7f4a6f952b5ef17ab95862"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef2afcd1da1f4a28956c5dbef46ae5f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ae0a72c274b4027ba6ebfbe7ce1d165"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38047a6da3af48c48771bdfda5070387"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aebc3943adf4e2d8a7b95ea5d7a0702"}},"metadata":{}},{"name":"stderr","text":"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Function to truncate audio to 3.5 seconds\ndef truncate_audio(audio, sample_rate, duration=3.5):\n    max_length = int(duration * sample_rate)\n    if audio.size(1) > max_length:\n        audio = audio[:, :max_length]\n    return audio\n\n# Function to get Wav2Vec2 embeddings\ndef get_wav2vec2_embeddings(audio, processor, model, max_length):\n    audio = audio.squeeze(0)\n    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True, truncation=True,max_length = max_length)\n    with torch.no_grad():\n        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n    return embeddings\n\n\n# # Function to get ImageBind embeddings (hypothetical)\n# def get_imagebind_embeddings(audio, model):\n#     # Replace with actual ImageBind model code\n#     embeddings = model.get_audio_embeddings(audio)\n#     return embeddings\n\n# Function to calculate cosine similarity\ndef cosine_similarity(emb1, emb2):\n    return F.cosine_similarity(emb1, emb2)\n\n# Function to rank entities based on similarity\ndef rank_entities(dataset_embeddings, entity_embeddings):\n    rankings = {}\n    for audio_filename, audio_emb in dataset_embeddings.items():\n        similarities = []\n        for entity_filename, entity_emb in entity_embeddings.items():\n            similarity = cosine_similarity(audio_emb, entity_emb)\n            similarities.append((entity_filename, similarity.item()))\n        \n        rankings[audio_filename] = sorted(similarities, key=lambda x: x[1], reverse=True)\n    return rankings\n\n\n# Function to calculate Top-N accuracy\ndef calculate_top_n_accuracy(rankings, sentence_df, entity_df, n=1):\n    correct = 0\n    total = len(sentence_df)\n\n    for idx, row in sentence_df.iterrows():\n        sentence_audio = row['audio_filename']\n        # Check if 'entities' is a list of dictionaries and contains at least one entity\n        if isinstance(row['entities'], list) and len(row['entities']) > 0:\n            actual_entity_name = row['entities'][0]['mention']\n            actual_entity = entity_map.get(actual_entity_name, None)\n            # If actual_entity is not found in entity_map, skip this entry\n            if actual_entity is None:\n                continue\n\n            # Get top-n ranked entities for this sentence\n            top_n = [entity for entity, _ in rankings.get(sentence_audio, [])[:n]]\n            # Check if the actual entity is within the top-n predictions\n            if actual_entity in top_n:\n                correct += 1\n\n    return correct / total\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:49:34.448764Z","iopub.execute_input":"2024-08-29T11:49:34.449187Z","iopub.status.idle":"2024-08-29T11:49:34.467103Z","shell.execute_reply.started":"2024-08-29T11:49:34.449148Z","shell.execute_reply":"2024-08-29T11:49:34.465625Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# Process entity audio files\nentity_embeddings_wav2vec = {}\n# entity_embeddings_imagebind = {}\n\nfile_list = os.listdir(entity_audio_dir)\n\nfor filename in tqdm(file_list, desc=\"Processing audio files\", unit=\"file\"):\n    try:\n        file_path = os.path.join(entity_audio_dir, filename)\n        audio, sample_rate = torchaudio.load(file_path)\n        audio = truncate_audio(audio, sample_rate)\n        max_length = int(3.5 * 16000) \n        wav2vec_embeddings = get_wav2vec2_embeddings(audio, wav2vec_processor, wav2vec_model, max_length)\n        # imagebind_embeddings = get_imagebind_embeddings(audio, imagebind_model)\n        \n        entity_embeddings_wav2vec[filename] = wav2vec_embeddings\n        # entity_embeddings_imagebind[filename] = imagebind_embeddings\n    except Exception as e:\n        print(f\"Error processing {filename}: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-29T10:46:30.823555Z","iopub.execute_input":"2024-08-29T10:46:30.824958Z","iopub.status.idle":"2024-08-29T10:50:27.034075Z","shell.execute_reply.started":"2024-08-29T10:46:30.824895Z","shell.execute_reply":"2024-08-29T10:50:27.032904Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Processing audio files: 100%|██████████| 500/500 [03:56<00:00,  2.12file/s]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Process dataset audio files\ndataset_embeddings_wav2vec = {}\n# dataset_embeddings_imagebind = {}\nfile_list = os.listdir(dataset_audio_dir)\nfor filename in tqdm(file_list, desc=\"Processing audio files\", unit=\"file\"):\n    try:\n        file_path = os.path.join(dataset_audio_dir, filename)\n        audio, sample_rate = torchaudio.load(file_path)\n        max_length = (3*16000)\n        wav2vec_embeddings = get_wav2vec2_embeddings(audio, wav2vec_processor, wav2vec_model, max_length)\n    #     imagebind_embeddings = get_imagebind_embeddings(audio, imagebind_model)\n\n        dataset_embeddings_wav2vec[filename] = wav2vec_embeddings\n    #     dataset_embeddings_imagebind[filename] = imagebind_embeddings\n    except Exception as e:\n        print(f\"Error processing {filename}: {e}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T10:50:30.093013Z","iopub.execute_input":"2024-08-29T10:50:30.094275Z","iopub.status.idle":"2024-08-29T11:25:56.741122Z","shell.execute_reply.started":"2024-08-29T10:50:30.094189Z","shell.execute_reply":"2024-08-29T11:25:56.739920Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Processing audio files: 100%|██████████| 2626/2626 [35:26<00:00,  1.23file/s]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import ast\n\n\nsentence_df = pd.read_csv('/kaggle/input/business-json/Test (1).csv') \nentity_df = pd.read_csv('/kaggle/input/business-json/entity_aud_files.csv')\n\n# Convert 'entities' column from string to list of dictionaries\nsentence_df['entities'] = sentence_df['entities'].apply(lambda x: ast.literal_eval(x))\n\n# Map entity_name to ID for easy lookup\nentity_map = dict(zip(entity_df['entity_name'], entity_df['ID']))","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:39:55.912488Z","iopub.execute_input":"2024-08-29T11:39:55.913028Z","iopub.status.idle":"2024-08-29T11:39:56.013790Z","shell.execute_reply.started":"2024-08-29T11:39:55.912983Z","shell.execute_reply":"2024-08-29T11:39:56.012553Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Rank entities for Wav2Vec2 and ImageBind embeddings\nrankings_wav2vec = rank_entities(dataset_embeddings_wav2vec, entity_embeddings_wav2vec)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Calculate Top-N accuracies for Wav2Vec2\ntop1_acc_wav2vec = calculate_top_n_accuracy(rankings_wav2vec, sentence_df, entity_df, n=1)\ntop5_acc_wav2vec = calculate_top_n_accuracy(rankings_wav2vec, sentence_df, entity_df, n=5)\ntop10_acc_wav2vec = calculate_top_n_accuracy(rankings_wav2vec, sentence_df, entity_df, n=10)\n\n# Print accuracy results\nprint(f\"Wav2Vec2 Top-1 Accuracy: {top1_acc_wav2vec:.2f}\")\nprint(f\"Wav2Vec2 Top-5 Accuracy: {top5_acc_wav2vec:.2f}\")\nprint(f\"Wav2Vec2 Top-10 Accuracy: {top10_acc_wav2vec:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T11:49:37.584573Z","iopub.execute_input":"2024-08-29T11:49:37.585095Z","iopub.status.idle":"2024-08-29T11:49:38.239412Z","shell.execute_reply.started":"2024-08-29T11:49:37.585051Z","shell.execute_reply":"2024-08-29T11:49:38.237923Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Wav2Vec2 Top-1 Accuracy: 0.00\nWav2Vec2 Top-5 Accuracy: 0.00\nWav2Vec2 Top-10 Accuracy: 0.00\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"\n# rankings_imagebind = rank_entities(dataset_embeddings_imagebind, entity_embeddings_imagebind)\n\n\n# top1_acc_imagebind = top_n_accuracy(rankings_imagebind, entity_embeddings_imagebind.keys(), n=1)\n# top5_acc_imagebind = top_n_accuracy(rankings_imagebind, entity_embeddings_imagebind.keys(), n=5)\n# top10_acc_imagebind = top_n_accuracy(rankings_imagebind, entity_embeddings_imagebind.keys(), n=10)\n\n# print(f\"ImageBind Top-1 Accuracy: {top1_acc_imagebind:.2f}\")\n# print(f\"ImageBind Top-5 Accuracy: {top5_acc_imagebind:.2f}\")\n# print(f\"ImageBind Top-10 Accuracy: {top10_acc_imagebind:.2f}\")","metadata":{},"outputs":[],"execution_count":null}]}