{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-24T18:26:50.973198Z","iopub.execute_input":"2024-08-24T18:26:50.973640Z","iopub.status.idle":"2024-08-24T18:26:51.458505Z","shell.execute_reply.started":"2024-08-24T18:26:50.973586Z","shell.execute_reply":"2024-08-24T18:26:51.457356Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\n!pip install -r requirements.txt\n!pip install -e hf-dev-train/transformers-main\n!pip install -e peft-main\n!pip install torchlibrosa\n!pip install gdown\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T18:18:24.833904Z","iopub.execute_input":"2024-08-24T18:18:24.834359Z","iopub.status.idle":"2024-08-24T18:21:40.891403Z","shell.execute_reply.started":"2024-08-24T18:18:24.834317Z","shell.execute_reply":"2024-08-24T18:21:40.889277Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\nCollecting torch==1.13.1\n  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (1801.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchvision==0.14.1\n  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp310-cp310-linux_x86_64.whl (24.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==0.13.1\n  Downloading https://download.pytorch.org/whl/cu117/torchaudio-0.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (4.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==1.13.1) (4.12.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.14.1) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.14.1) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.14.1) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (2024.7.4)\nInstalling collected packages: torch, torchvision, torchaudio\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.0+cpu\n    Uninstalling torch-2.4.0+cpu:\n      Successfully uninstalled torch-2.4.0+cpu\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.19.0+cpu\n    Uninstalling torchvision-0.19.0+cpu:\n      Successfully uninstalled torchvision-0.19.0+cpu\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.4.0+cpu\n    Uninstalling torchaudio-2.4.0+cpu:\n      Successfully uninstalled torchaudio-2.4.0+cpu\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.4.0 requires torch>=2.1.0, but you have torch 1.13.1+cu117 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-1.13.1+cu117 torchaudio-0.13.1+cu117 torchvision-0.14.1+cu117\n\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: hf-dev-train/transformers-main is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with bzr+http, bzr+https, bzr+ssh, bzr+sftp, bzr+ftp, bzr+lp, bzr+file, git+http, git+https, git+ssh, git+git, git+file, hg+file, hg+http, hg+https, hg+ssh, hg+static-http, svn+ssh, svn+http, svn+https, svn+svn, svn+file).\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: peft-main is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with bzr+http, bzr+https, bzr+ssh, bzr+sftp, bzr+ftp, bzr+lp, bzr+file, git+http, git+https, git+ssh, git+git, git+file, hg+file, hg+http, hg+https, hg+ssh, hg+static-http, svn+ssh, svn+http, svn+https, svn+svn, svn+file).\u001b[0m\u001b[31m\n\u001b[0mCollecting torchlibrosa\n  Downloading torchlibrosa-0.1.0-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchlibrosa) (1.26.4)\nRequirement already satisfied: librosa>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchlibrosa) (0.10.2.post1)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->torchlibrosa) (3.0.1)\nRequirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->torchlibrosa) (1.14.0)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->torchlibrosa) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->torchlibrosa) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->torchlibrosa) (5.1.1)\nRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->torchlibrosa) (0.58.1)\nRequirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->torchlibrosa) (0.12.1)\nRequirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->torchlibrosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->torchlibrosa) (0.4.0)\nRequirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->torchlibrosa) (4.12.2)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->torchlibrosa) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.8.0->torchlibrosa) (1.0.8)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from lazy-loader>=0.1->librosa>=0.8.0->torchlibrosa) (21.3)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa>=0.8.0->torchlibrosa) (0.41.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa>=0.8.0->torchlibrosa) (3.11.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa>=0.8.0->torchlibrosa) (2.32.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa>=0.8.0->torchlibrosa) (3.5.0)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa>=0.8.0->torchlibrosa) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.8.0->torchlibrosa) (2.22)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->lazy-loader>=0.1->librosa>=0.8.0->torchlibrosa) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->torchlibrosa) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->torchlibrosa) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->torchlibrosa) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->torchlibrosa) (2024.7.4)\nDownloading torchlibrosa-0.1.0-py3-none-any.whl (11 kB)\nInstalling collected packages: torchlibrosa\nSuccessfully installed torchlibrosa-0.1.0\nCollecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.7.4)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())","metadata":{"execution":{"iopub.status.busy":"2024-08-24T18:21:40.895447Z","iopub.execute_input":"2024-08-24T18:21:40.896027Z","iopub.status.idle":"2024-08-24T18:21:42.627535Z","shell.execute_reply.started":"2024-08-24T18:21:40.895968Z","shell.execute_reply":"2024-08-24T18:21:42.626118Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"False\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nos.makedirs('/kaggle/temp/GAMA-project/tmp', exist_ok=True)\nos.chdir('/kaggle/temp/GAMA-project/tmp')","metadata":{"execution":{"iopub.status.busy":"2024-08-24T18:21:46.679542Z","iopub.execute_input":"2024-08-24T18:21:46.680109Z","iopub.status.idle":"2024-08-24T18:21:46.686693Z","shell.execute_reply.started":"2024-08-24T18:21:46.680062Z","shell.execute_reply":"2024-08-24T18:21:46.684927Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\n!gdown --fuzzy https://drive.google.com/file/d/1bWIyJOOzlrQbuIZPYTwH8au8ywrATe7K/view?usp=drive_link\n!unzip Llama-2-7b-chat-hf-qformer.zip\n\n\nos.environ['BASE_MODEL'] = '/kaggle/temp/GAMA-project/tmp/Llama-2-7b-chat-hf-qformer'\n\n!gdown --fuzzy https://drive.google.com/file/d/1Rfwf9CGubPZ1lHX9yIhIjZjwB3IGX1cy/view?usp=drive_link  \n!gdown --fuzzy https://drive.google.com/file/d/1VnAkb45iO9WIQGzONqmynb2kTRjHsZlG/view?usp=drive_link  \n!unzip '*.zip'  \n\n\n!gdown --fuzzy https://drive.google.com/file/d/1zQAg3Qu3Cv0n6dQgnplj5X3zj8L83Aug/view?usp=drive_link\n!tar -xvf *.tar \n\n!gdown --fuzzy https://drive.google.com/file/d/1tMd5g5ysTQ8VHSYrgvss7A7qvT7gBlEO/view?usp=drive_link\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T18:22:00.667020Z","iopub.execute_input":"2024-08-24T18:22:00.667451Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1bWIyJOOzlrQbuIZPYTwH8au8ywrATe7K\nFrom (redirected): https://drive.google.com/uc?id=1bWIyJOOzlrQbuIZPYTwH8au8ywrATe7K&confirm=t&uuid=86ddcb66-bcf3-4a7e-83dc-9eaf30a644a2\nTo: /kaggle/working/~/GAMA-project/tmp/Llama-2-7b-chat-hf-qformer.zip\n100%|███████████████████████████████████████| 11.4G/11.4G [01:25<00:00, 133MB/s]\nArchive:  Llama-2-7b-chat-hf-qformer.zip\n   creating: Llama-2-7b-chat-hf-qformer/\n  inflating: __MACOSX/._Llama-2-7b-chat-hf-qformer  \n  inflating: Llama-2-7b-chat-hf-qformer/Qformer.json  \n  inflating: Llama-2-7b-chat-hf-qformer/USE_POLICY.md  \n  inflating: Llama-2-7b-chat-hf-qformer/tokenizer_config.json  \n  inflating: Llama-2-7b-chat-hf-qformer/special_tokens_map.json  \n  inflating: Llama-2-7b-chat-hf-qformer/model-00001-of-00002.safetensors  ","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nimport os\n\ndefault_cuda_devices = \"0\"\nif len(sys.argv) > 1:\n    argument = sys.argv[1]\n    if argument == '4':\n        argument = default_cuda_devices\nelse:\n    argument = default_cuda_devices\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = argument\nimport numpy as np\nimport os\nimport torchaudio\nimport fire\nimport json\nimport torch\nfrom tqdm import tqdm\nimport time\nimport torchvision\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    get_peft_model_state_dict,\n    prepare_model_for_int8_training,\n    set_peft_model_state_dict,\n)\nfrom transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer, LlamaConfig\n\nfrom utils.prompter import Prompter\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndef int16_to_float32_torch(x):\n    return (x / 32767.0).type(torch.float32)\n\ndef float32_to_int16_torch(x):\n    x = torch.clamp(x, min=-1., max=1.)\n    return (x * 32767.).type(torch.int16)\n\ndef get_mel(audio_data):\n        # mel shape: (n_mels, T)\n    mel_tf = torchaudio.transforms.MelSpectrogram(\n            sample_rate=48000,\n            n_fft=1024,\n            win_length=1024,\n            hop_length=480,\n            center=True,\n            pad_mode=\"reflect\",\n            power=2.0,\n            norm=None,\n            onesided=True,\n            n_mels=64,\n            f_min=50,\n            f_max=14000\n    ).to(audio_data.device)\n        \n    mel = mel_tf(audio_data)\n\n        # we use log mel spectrogram as input\n    mel = torchaudio.transforms.AmplitudeToDB(top_db=None)(mel)\n\n    return mel.T  # (T, n_mels)\n\n\ndef get_audio_features(sample, audio_data, max_len, data_truncating, data_filling, require_grad=False):\n        grad_fn = suppress if require_grad else torch.no_grad\n        with grad_fn():\n            if len(audio_data) > max_len:\n                if data_truncating == \"rand_trunc\":\n                    longer = torch.tensor([True])\n                elif data_truncating == \"fusion\":\n                    # fusion\n                    mel = get_mel(audio_data)\n                    # split to three parts\n                    chunk_frames = max_len // 480 + 1  # the +1 related to how the spectrogram is computed\n                    total_frames = mel.shape[0]\n                    if chunk_frames == total_frames:\n                        # there is a corner case where the audio length is\n                        # larger than max_len but smaller than max_len+hop_size.\n                        # In this case, we just use the whole audio.\n                        mel_fusion = torch.stack([mel, mel, mel, mel], dim=0)\n                        sample[\"mel_fusion\"] = mel_fusion\n                        longer = torch.tensor([False])\n                    else:\n                        ranges = np.array_split(list(range(0, total_frames - chunk_frames + 1)), 3)\n                        # print('total_frames-chunk_frames:', total_frames-chunk_frames,\n                        #       'len(audio_data):', len(audio_data),\n                        #       'chunk_frames:', chunk_frames,\n                        #       'total_frames:', total_frames)\n                        if len(ranges[1]) == 0:\n                            # if the audio is too short, we just use the first chunk\n                            ranges[1] = [0]\n                        if len(ranges[2]) == 0:\n                            # if the audio is too short, we just use the first chunk\n                            ranges[2] = [0]\n                        # randomly choose index for each part\n                        idx_front = np.random.choice(ranges[0])\n                        idx_middle = np.random.choice(ranges[1])\n                        idx_back = np.random.choice(ranges[2])\n                        # select mel\n                        mel_chunk_front = mel[idx_front:idx_front + chunk_frames, :]\n                        mel_chunk_middle = mel[idx_middle:idx_middle + chunk_frames, :]\n                        mel_chunk_back = mel[idx_back:idx_back + chunk_frames, :]\n\n                        # shrink the mel\n                        mel_shrink = torchvision.transforms.Resize(size=[chunk_frames, 64])(mel[None])[0]\n                        # logging.info(f\"mel_shrink.shape: {mel_shrink.shape}\")\n\n                        # stack\n                        mel_fusion = torch.stack([mel_shrink, mel_chunk_front, mel_chunk_middle, mel_chunk_back], dim=0)\n                        sample[\"mel_fusion\"] = mel_fusion #.unsqueeze(0)\n                        longer = torch.tensor([True])\n                else:\n                    raise NotImplementedError(\n                        f\"data_truncating {data_truncating} not implemented\"\n                    )\n                # random crop to max_len (for compatibility)\n                overflow = len(audio_data) - max_len\n                idx = np.random.randint(0, overflow + 1)\n                audio_data = audio_data[idx: idx + max_len]\n\n            else:  # padding if too short\n                if len(audio_data) < max_len:  # do nothing if equal\n                    if data_filling == \"repeatpad\":\n                        n_repeat = int(max_len / len(audio_data))\n                        audio_data = audio_data.repeat(n_repeat)\n                        # audio_data = audio_data.unsqueeze(0).unsqueeze(0).unsqueeze(0)\n                        # audio_data = F.interpolate(audio_data,size=max_len,mode=\"bicubic\")[0,0,0]\n                        audio_data = F.pad(\n                            audio_data,\n                            (0, max_len - len(audio_data)),\n                            mode=\"constant\",\n                            value=0,\n                        )\n                    elif data_filling == \"pad\":\n                        audio_data = F.pad(\n                            audio_data,\n                            (0, max_len - len(audio_data)),\n                            mode=\"constant\",\n                            value=0,\n                        )\n                    elif data_filling == \"repeat\":\n                        n_repeat = int(max_len / len(audio_data))\n                        audio_data = audio_data.repeat(n_repeat + 1)[:max_len]\n                    else:\n                        raise NotImplementedError(\n                            f\"data_filling {data_filling} not implemented\"\n                        )\n                if data_truncating == 'fusion':\n                    mel = get_mel(audio_data)\n                    mel_fusion = torch.stack([mel, mel, mel, mel], dim=0)\n                    sample[\"mel_fusion\"] = mel_fusion\n                longer = torch.tensor([False])\n\n        sample[\"longer\"] = longer\n        sample[\"waveform\"] = audio_data\n        sample[\"mel_fusion\"] = sample[\"mel_fusion\"].unsqueeze(0)\n        # print(sample[\"mel_fusion\"].shape)\n        # print(\"---------------------\")\n        return sample\n\n\n    \ndef load_audio(filename):\n    waveform, sr = torchaudio.load(filename)\n    if sr != 16000:\n        waveform = torchaudio.functional.resample(waveform, sr, 16000)\n        sr = 16000\n    waveform = waveform - waveform.mean()\n    fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr,\n                                              use_energy=False, window_type='hanning',\n                                              num_mel_bins=128, dither=0.0, frame_shift=10)\n    target_length = 1024\n    n_frames = fbank.shape[0]\n    p = target_length - n_frames\n    if p > 0:\n        m = torch.nn.ZeroPad2d((0, 0, 0, p))\n        fbank = m(fbank)\n    elif p < 0:\n        fbank = fbank[0:target_length, :]\n    # normalize the fbank\n    fbank = (fbank + 5.081) / 4.4849\n    return fbank\n\n\n\ndef main(\n    base_model: str = \"/fs/nexus-projects/brain_project/Llama-2-7b-chat-hf-qformer\",\n    prompt_template: str = \"alpaca_short\",  # The prompt template to use, will default to alpaca.\n):\n    base_model = base_model or os.environ.get(\"BASE_MODEL\", \"\")\n    assert (\n        base_model\n    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n\n    prompter = Prompter(prompt_template)\n    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n\n    # model = LlamaForCausalLM.from_pretrained(base_model, device_map=\"auto\")\n    model = LlamaForCausalLM.from_pretrained(base_model, device_map=\"auto\") #, torch_dtype=torch.bfloat16\n\n\n    config = LoraConfig(\n        r=8,\n        lora_alpha=16,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=0.0,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    model = get_peft_model(model, config)\n    temp, top_p, top_k = 0.1, 0.95, 500\n    # change it to your model path\n    eval_mdl_path = '/fs/gamma-projects/audio/gama/new_data/stage5/checkpoint-2500/pytorch_model.bin'\n    state_dict = torch.load(eval_mdl_path, map_location='cpu')\n    msg = model.load_state_dict(state_dict, strict=False)\n\n    model.is_parallelizable = True\n    model.model_parallel = True\n\n    # unwind broken decapoda-research config\n    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n    model.config.bos_token_id = 1\n    model.config.eos_token_id = 2\n\n    model.eval()\n    file = open('/fs/nexus-projects/brain_project/acl_sk_24/GAMA_Benchmark_new.json','r')\n    file = json.load(file)\n    res = []\n    for i in tqdm(file):\n        tmp = {}\n        for j in i['instruction_output']:\n            audio_path = i['audio_id']\n            instruction = j['instruction']\n            prompt = prompter.generate_prompt(instruction, None)\n            inputs = tokenizer(prompt, return_tensors=\"pt\")\n            input_ids = inputs[\"input_ids\"].to(device)\n            if audio_path != 'empty':\n                cur_audio_input = load_audio(audio_path).unsqueeze(0)\n                if torch.cuda.is_available() == False:\n                    pass\n                else:\n                    cur_audio_input = cur_audio_input.to(device)\n            else:\n                cur_audio_input = None\n\n            generation_config = GenerationConfig(\n                do_sample=True,\n                temperature=temp,\n                top_p=top_p,\n                top_k=top_k,\n                repetition_penalty=1.1,\n                max_new_tokens=400,\n                bos_token_id=model.config.bos_token_id,\n                eos_token_id=model.config.eos_token_id,\n                pad_token_id=model.config.pad_token_id,\n                num_return_sequences=1\n            )\n\n            # Without streaming\n\n            with torch.no_grad():\n                generation_output = model.generate(\n                    input_ids=input_ids.to(device),\n                    audio_input=cur_audio_input,\n                    generation_config=generation_config,\n                    return_dict_in_generate=True,\n                    output_scores=True,\n                    max_new_tokens=400,\n                )\n            s = generation_output.sequences[0]\n            output = tokenizer.decode(s)[6:-4]\n            output = output[len(prompt):]\n            # print('----------------------')\n            # print(output)\n            tmp['audio_id'] = audio_path\n            tmp['instruction'] = instruction\n            tmp['scene_caption'] = i['caption']\n            tmp['prediction'] = output\n            tmp['timestamp_events'] = i['timestamp_events']\n            tmp['ref'] = j[\"output\"]\n            res.append(tmp)\n    with open(\"stage5_answers_qformer_all.json\", \"w\") as res_file:\n        json.dump(res, res_file, indent=4)\n\nif __name__ == \"__main__\":\n    fire.Fire(main)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd ~/GAMA-project\nmkdir tmp\ncd tmp\n\ngdown --fuzzy https://drive.google.com/file/d/1bWIyJOOzlrQbuIZPYTwH8au8ywrATe7K/view?usp=drive_link\nunzip Llama-2-7b-chat-hf-qformer.zip\nexport BASE_MODEL=\"/ps/scratch/kchhatre/Work/Abhirama_collab/GAMA/tmp_dwnloads/Llama-2-7b-chat-hf-qformer\"\n\nstage5 gama model:\ngdown --fuzzy https://drive.google.com/file/d/1Rfwf9CGubPZ1lHX9yIhIjZjwB3IGX1cy/view?usp=drive_link # epoch 2\ngdown --fuzzy https://drive.google.com/file/d/1VnAkb45iO9WIQGzONqmynb2kTRjHsZlG/view?usp=drive_link # epoch 1\nunzip\n\ncomp AR audios:\ngdown --fuzzy https://drive.google.com/file/d/1zQAg3Qu3Cv0n6dQgnplj5X3zj8L83Aug/view?usp=drive_link\nextract tar\n\njson file: \ngdown --fuzzy https://drive.google.com/file/d/1tMd5g5ysTQ8VHSYrgvss7A7qvT7gBlEO/view?usp=drive_link\n\ntypical datapoint:\n[\n    {\n        \"audio_id\": \"Y0SSy52rc1BM.wav\",\n        \"input\": \"\",\n        \"dataset\": \"Audioset_Strong\",\n        \"task\": \"open-ended question\",\n        \"instruction_output\": [\n            {\n                \"instruction\": \"Given the presence of choir and music, infer the possible occasion or event taking place. Use the auditory observations to corroborate with the possible visual cues.\",\n                \"output\": \"The choir and music suggest a celebratory event, possibly a festive occasion or performance event, and the visual cues corroborate with a dance or party setting.\"}]}]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}