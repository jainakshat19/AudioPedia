{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9273248,"sourceType":"datasetVersion","datasetId":5543066}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-04T18:16:09.673107Z","iopub.execute_input":"2024-09-04T18:16:09.673419Z","iopub.status.idle":"2024-09-04T18:16:10.039227Z","shell.execute_reply.started":"2024-09-04T18:16:09.673385Z","shell.execute_reply":"2024-09-04T18:16:10.038413Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone -b feature/add_hf https://github.com/nielsrogge/ImageBind.git\n%cd ImageBind","metadata":{"execution":{"iopub.status.busy":"2024-09-04T18:16:10.041126Z","iopub.execute_input":"2024-09-04T18:16:10.041641Z","iopub.status.idle":"2024-09-04T18:16:11.763670Z","shell.execute_reply.started":"2024-09-04T18:16:10.041586Z","shell.execute_reply":"2024-09-04T18:16:11.762580Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'ImageBind'...\nremote: Enumerating objects: 125, done.\u001b[K\nremote: Counting objects: 100% (85/85), done.\u001b[K\nremote: Compressing objects: 100% (46/46), done.\u001b[K\nremote: Total 125 (delta 57), reused 39 (delta 39), pack-reused 40 (from 1)\u001b[K\nReceiving objects: 100% (125/125), 2.64 MiB | 30.02 MiB/s, done.\nResolving deltas: 100% (59/59), done.\n/kaggle/working/ImageBind\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install .","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-04T18:16:11.765009Z","iopub.execute_input":"2024-09-04T18:16:11.765324Z","iopub.status.idle":"2024-09-04T18:19:00.459266Z","shell.execute_reply.started":"2024-09-04T18:16:11.765290Z","shell.execute_reply":"2024-09-04T18:19:00.458050Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Processing /kaggle/working/ImageBind\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d (from imagebind==0.1.0)\n  Cloning https://github.com/facebookresearch/pytorchvideo.git (to revision 28fe037d212663c6a24f373b94cc5d478c8c1a1d) to /tmp/pip-install-j0iln_oc/pytorchvideo_a4e8380b57c94c708d06650a47ecd723\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorchvideo.git /tmp/pip-install-j0iln_oc/pytorchvideo_a4e8380b57c94c708d06650a47ecd723\n  Running command git rev-parse -q --verify 'sha^28fe037d212663c6a24f373b94cc5d478c8c1a1d'\n  Running command git fetch -q https://github.com/facebookresearch/pytorchvideo.git 28fe037d212663c6a24f373b94cc5d478c8c1a1d\n  Running command git checkout -q 28fe037d212663c6a24f373b94cc5d478c8c1a1d\n  Resolved https://github.com/facebookresearch/pytorchvideo.git to commit 28fe037d212663c6a24f373b94cc5d478c8c1a1d\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting torch==1.13.0 (from imagebind==0.1.0)\n  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (23 kB)\nCollecting torchvision==0.14.0 (from imagebind==0.1.0)\n  Downloading torchvision-0.14.0-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\nCollecting torchaudio==0.13.0 (from imagebind==0.1.0)\n  Downloading torchaudio-0.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (1.0 kB)\nCollecting timm==0.6.7 (from imagebind==0.1.0)\n  Downloading timm-0.6.7-py3-none-any.whl.metadata (33 kB)\nCollecting ftfy (from imagebind==0.1.0)\n  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from imagebind==0.1.0) (2024.5.15)\nCollecting einops (from imagebind==0.1.0)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting fvcore (from imagebind==0.1.0)\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting eva-decord==0.6.1 (from imagebind==0.1.0)\n  Downloading eva_decord-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (449 bytes)\nCollecting iopath (from imagebind==0.1.0)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.10/site-packages (from imagebind==0.1.0) (1.26.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from imagebind==0.1.0) (3.7.5)\nCollecting types-regex (from imagebind==0.1.0)\n  Downloading types_regex-2024.7.24.20240726-py3-none-any.whl.metadata (1.6 kB)\nCollecting mayavi (from imagebind==0.1.0)\n  Downloading mayavi-4.8.2.tar.gz (7.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: cartopy in /opt/conda/lib/python3.10/site-packages (from imagebind==0.1.0) (0.23.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==1.13.0->imagebind==0.1.0) (4.12.2)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.0->imagebind==0.1.0)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.0->imagebind==0.1.0)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.0->imagebind==0.1.0)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.0->imagebind==0.1.0)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.14.0->imagebind==0.1.0) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.14.0->imagebind==0.1.0) (9.5.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->imagebind==0.1.0) (70.0.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->imagebind==0.1.0) (0.43.0)\nRequirement already satisfied: shapely>=1.7 in /opt/conda/lib/python3.10/site-packages (from cartopy->imagebind==0.1.0) (1.8.5.post1)\nRequirement already satisfied: packaging>=20 in /opt/conda/lib/python3.10/site-packages (from cartopy->imagebind==0.1.0) (21.3)\nRequirement already satisfied: pyshp>=2.3 in /opt/conda/lib/python3.10/site-packages (from cartopy->imagebind==0.1.0) (2.3.1)\nRequirement already satisfied: pyproj>=3.3.1 in /opt/conda/lib/python3.10/site-packages (from cartopy->imagebind==0.1.0) (3.6.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imagebind==0.1.0) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imagebind==0.1.0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imagebind==0.1.0) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imagebind==0.1.0) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imagebind==0.1.0) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imagebind==0.1.0) (2.9.0.post0)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->imagebind==0.1.0) (0.2.13)\nCollecting yacs>=0.1.6 (from fvcore->imagebind==0.1.0)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore->imagebind==0.1.0) (6.0.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore->imagebind==0.1.0) (4.66.4)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.10/site-packages (from fvcore->imagebind==0.1.0) (2.4.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore->imagebind==0.1.0) (0.9.0)\nCollecting portalocker (from iopath->imagebind==0.1.0)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nCollecting apptools (from mayavi->imagebind==0.1.0)\n  Downloading apptools-5.3.0-py3-none-any.whl.metadata (4.3 kB)\nCollecting envisage (from mayavi->imagebind==0.1.0)\n  Downloading envisage-7.0.3-py3-none-any.whl.metadata (5.2 kB)\nCollecting pyface>=6.1.1 (from mayavi->imagebind==0.1.0)\n  Downloading pyface-8.0.0-py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.10/site-packages (from mayavi->imagebind==0.1.0) (2.18.0)\nCollecting traits>=6.0.0 (from mayavi->imagebind==0.1.0)\n  Downloading traits-6.4.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\nCollecting traitsui>=7.0.0 (from mayavi->imagebind==0.1.0)\n  Downloading traitsui-8.0.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: vtk in /opt/conda/lib/python3.10/site-packages (from mayavi->imagebind==0.1.0) (9.3.1)\nCollecting av (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->imagebind==0.1.0)\n  Downloading av-13.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nCollecting parameterized (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->imagebind==0.1.0)\n  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->imagebind==0.1.0) (3.3)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from pyproj>=3.3.1->cartopy->imagebind==0.1.0) (2024.7.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->imagebind==0.1.0) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.14.0->imagebind==0.1.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.14.0->imagebind==0.1.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.14.0->imagebind==0.1.0) (1.26.18)\nDownloading eva_decord-0.6.1-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading timm-0.6.7-py3-none-any.whl (509 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchaudio-0.13.0-cp310-cp310-manylinux1_x86_64.whl (4.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.14.0-cp310-cp310-manylinux1_x86_64.whl (24.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading types_regex-2024.7.24.20240726-py3-none-any.whl (5.6 kB)\nDownloading pyface-8.0.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading traits-6.4.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading traitsui-8.0.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading apptools-5.3.0-py3-none-any.whl (230 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.0/230.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading av-13.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading envisage-7.0.3-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.9/268.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\nDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nBuilding wheels for collected packages: imagebind, fvcore, iopath, mayavi, pytorchvideo\n  Building wheel for imagebind (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for imagebind: filename=imagebind-0.1.0-py3-none-any.whl size=27972 sha256=4e5dcd6c4d368bd677e26843f28d1b2fcbf4af550febd08fdbbc58e42941773a\n  Stored in directory: /tmp/pip-ephem-wheel-cache-jk5wbe6y/wheels/02/d7/3a/6e110914e8d3f8a2d32c26aee7970cdb86ff5e17fab71e1c78\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=d7364272fffbaa7c9f934846fea3da4bcd58e101bb713ceae4a8a24b462c1b8b\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=ce7df07827d393bb476cc061d1b38a9db938d39db5be8ed77135f9927e72cddb\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n  Building wheel for mayavi (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for mayavi: filename=mayavi-4.8.2-cp310-cp310-linux_x86_64.whl size=16692940 sha256=f685f4e37aed428cfc31c37bd6b4b3b6f83ec738266dc0ae2fdbd832554a0fe8\n  Stored in directory: /root/.cache/pip/wheels/60/52/8c/d16aeced951729965d3a787b59424bfc235372fafd1130c56e\n  Building wheel for pytorchvideo (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=211198 sha256=b566d13a5d492e212a35469e56cdf6de4a89244be268035089187f911b97e152\n  Stored in directory: /root/.cache/pip/wheels/a8/a0/a9/b2f1582cd6198b0425b645bdcce413a15f58d9cc3beee721d0\nSuccessfully built imagebind fvcore iopath mayavi pytorchvideo\nInstalling collected packages: yacs, types-regex, traits, portalocker, parameterized, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, ftfy, eva-decord, einops, av, pyface, nvidia-cudnn-cu11, iopath, apptools, traitsui, torch, fvcore, torchvision, torchaudio, pytorchvideo, envisage, timm, mayavi, imagebind\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.0\n    Uninstalling torch-2.4.0:\n      Successfully uninstalled torch-2.4.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.19.0\n    Uninstalling torchvision-0.19.0:\n      Successfully uninstalled torchvision-0.19.0\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.4.0\n    Uninstalling torchaudio-2.4.0:\n      Successfully uninstalled torchaudio-2.4.0\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.8\n    Uninstalling timm-1.0.8:\n      Successfully uninstalled timm-1.0.8\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.4.0 requires torch>=2.1.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed apptools-5.3.0 av-13.0.0 einops-0.8.0 envisage-7.0.3 eva-decord-0.6.1 ftfy-6.2.3 fvcore-0.1.5.post20221221 imagebind-0.1.0 iopath-0.1.10 mayavi-4.8.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 parameterized-0.9.0 portalocker-2.10.1 pyface-8.0.0 pytorchvideo-0.1.5 timm-0.6.7 torch-1.13.0 torchaudio-0.13.0 torchvision-0.14.0 traits-6.4.3 traitsui-8.0.0 types-regex-2024.7.24.20240726 yacs-0.1.8\n","output_type":"stream"}]},{"cell_type":"code","source":"from imagebind import data\nimport torch\nfrom imagebind.models import imagebind_model\nfrom imagebind.models.imagebind_model import ModalityType\nfrom imagebind.models.imagebind_model import ImageBindModel\n\ntext_list=[\"A dog.\", \"A car\", \"A bird\"]\nimage_paths=[\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\naudio_paths=[\".assets/dog_audio.wav\", \".assets/car_audio.wav\", \".assets/bird_audio.wav\"]\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = ImageBindModel.from_pretrained(\"nielsr/imagebind-huge\")\nmodel.eval()\nmodel.to(device)\n\n# # Load data\n# inputs = {\n#     ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n#     ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n#     ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n# }\n\n# with torch.no_grad():\n#     embeddings = model(inputs)\n\n# print(\n#     \"Vision x Text: \",\n#     torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1),\n# )\n# print(\n#     \"Audio x Text: \",\n#     torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1),\n# )\n# print(\n#     \"Vision x Audio: \",\n#     torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n# )\n\n# Expected output:\n#\n# Vision x Text:\n# tensor([[9.9761e-01, 2.3694e-03, 1.8612e-05],\n#         [3.3836e-05, 9.9994e-01, 2.4118e-05],\n#         [4.7997e-05, 1.3496e-02, 9.8646e-01]])\n#\n# Audio x Text:\n# tensor([[1., 0., 0.],\n#         [0., 1., 0.],\n#         [0., 0., 1.]])\n#\n# Vision x Audio:\n# tensor([[0.8070, 0.1088, 0.0842],\n#         [0.1036, 0.7884, 0.1079],\n#         [0.0018, 0.0022, 0.9960]])\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-04T18:19:00.462411Z","iopub.execute_input":"2024-09-04T18:19:00.463206Z","iopub.status.idle":"2024-09-04T18:19:50.024948Z","shell.execute_reply.started":"2024-09-04T18:19:00.463171Z","shell.execute_reply":"2024-09-04T18:19:50.023929Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33438e623b1d447ca718f85b79cf636c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.80G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be898ca96e844a82b61859d075a1c84f"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"ImageBindModel(\n  (modality_preprocessors): ModuleDict(\n    (vision): RGBDTPreprocessor(\n      (cls_token): tensor((1, 1, 1280), requires_grad=True)\n      \n      (rgbt_stem): PatchEmbedGeneric(\n        (proj): Sequential(\n          (0): PadIm2Video()\n          (1): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n        )\n      )\n      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n        (pos_embed): tensor((1, 257, 1280), requires_grad=True)\n        \n      )\n    )\n    (text): TextPreprocessor(\n      (pos_embed): tensor((1, 77, 1024), requires_grad=True)\n      (mask): tensor((77, 77), requires_grad=False)\n      \n      (token_embedding): Embedding(49408, 1024)\n    )\n    (audio): AudioPreprocessor(\n      (cls_token): tensor((1, 1, 768), requires_grad=True)\n      \n      (rgbt_stem): PatchEmbedGeneric(\n        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10), bias=False)\n        (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n        (pos_embed): tensor((1, 229, 768), requires_grad=True)\n        \n      )\n    )\n    (depth): RGBDTPreprocessor(\n      (cls_token): tensor((1, 1, 384), requires_grad=True)\n      \n      (depth_stem): PatchEmbedGeneric(\n        (proj): Conv2d(1, 384, kernel_size=(16, 16), stride=(16, 16), bias=False)\n        (norm_layer): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      )\n      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n        (pos_embed): tensor((1, 197, 384), requires_grad=True)\n        \n      )\n    )\n    (thermal): ThermalPreprocessor(\n      (cls_token): tensor((1, 1, 768), requires_grad=True)\n      \n      (rgbt_stem): PatchEmbedGeneric(\n        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n        (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n        (pos_embed): tensor((1, 197, 768), requires_grad=True)\n        \n      )\n    )\n    (imu): IMUPreprocessor(\n      (pos_embed): tensor((1, 251, 512), requires_grad=True)\n      (cls_token): tensor((1, 1, 512), requires_grad=True)\n      \n      (imu_stem): PatchEmbedGeneric(\n        (proj): Linear(in_features=48, out_features=512, bias=False)\n        (norm_layer): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (modality_trunks): ModuleDict(\n    (vision): SimpleTransformer(\n      (pre_transformer_layer): Sequential(\n        (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        (1): EinOpsRearrange()\n      )\n      (blocks): Sequential(\n        (0): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (1): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (2): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (3): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (4): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (5): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (6): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (7): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (8): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (9): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (10): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (11): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (12): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (13): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (14): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (15): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (16): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (17): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (18): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (19): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (20): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (21): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (22): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (23): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (24): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (25): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (26): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (27): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (28): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (29): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (30): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n        (31): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n      (post_transformer_layer): EinOpsRearrange()\n    )\n    (text): SimpleTransformer(\n      (pre_transformer_layer): Sequential(\n        (0): Identity()\n        (1): EinOpsRearrange()\n      )\n      (blocks): Sequential(\n        (0): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (1): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (2): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (3): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (4): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (5): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (6): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (7): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (8): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (9): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (10): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (11): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (12): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (13): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (14): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (15): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (16): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (17): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (18): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (19): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (20): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (21): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (22): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n        (23): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n      (post_transformer_layer): EinOpsRearrange()\n    )\n    (audio): SimpleTransformer(\n      (pre_transformer_layer): Sequential(\n        (0): Identity()\n        (1): EinOpsRearrange()\n      )\n      (blocks): Sequential(\n        (0): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (1): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.009)\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (2): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.018)\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (3): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.027)\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (4): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.036)\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (5): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.045)\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (6): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.055)\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (7): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.064)\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (8): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.073)\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (9): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.082)\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (10): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.091)\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (11): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.100)\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n      (post_transformer_layer): EinOpsRearrange()\n    )\n    (depth): SimpleTransformer(\n      (pre_transformer_layer): Sequential(\n        (0): Identity()\n        (1): EinOpsRearrange()\n      )\n      (blocks): Sequential(\n        (0): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n        )\n        (1): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n        )\n        (2): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n        )\n        (3): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n        )\n        (4): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n        )\n        (5): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n        )\n        (6): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n        )\n        (7): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n        )\n        (8): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n        )\n        (9): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n        )\n        (10): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n        )\n        (11): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n      (post_transformer_layer): EinOpsRearrange()\n    )\n    (thermal): SimpleTransformer(\n      (pre_transformer_layer): Sequential(\n        (0): Identity()\n        (1): EinOpsRearrange()\n      )\n      (blocks): Sequential(\n        (0): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (1): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (2): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (3): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (4): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (5): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (6): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (7): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (8): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (9): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (10): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n        (11): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n      (post_transformer_layer): EinOpsRearrange()\n    )\n    (imu): SimpleTransformer(\n      (pre_transformer_layer): Sequential(\n        (0): Identity()\n        (1): EinOpsRearrange()\n      )\n      (blocks): Sequential(\n        (0): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (drop_path): Identity()\n          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n        )\n        (1): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.140)\n          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n        )\n        (2): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.280)\n          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n        )\n        (3): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.420)\n          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n        )\n        (4): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.560)\n          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n        )\n        (5): BlockWithMasking(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (drop_path): DropPath(drop_prob=0.700)\n          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (act): GELU(approximate='none')\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (drop): Dropout(p=0.0, inplace=False)\n          )\n          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n      (post_transformer_layer): EinOpsRearrange()\n    )\n  )\n  (modality_heads): ModuleDict(\n    (vision): Sequential(\n      (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n      (1): SelectElement()\n      (2): Linear(in_features=1280, out_features=1024, bias=False)\n    )\n    (text): SelectEOSAndProject(\n      (proj): Sequential(\n        (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (1): Linear(in_features=1024, out_features=1024, bias=False)\n      )\n    )\n    (audio): Sequential(\n      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (1): SelectElement()\n      (2): Linear(in_features=768, out_features=1024, bias=False)\n    )\n    (depth): Sequential(\n      (0): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (1): SelectElement()\n      (2): Linear(in_features=384, out_features=1024, bias=False)\n    )\n    (thermal): Sequential(\n      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (1): SelectElement()\n      (2): Linear(in_features=768, out_features=1024, bias=False)\n    )\n    (imu): Sequential(\n      (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n      (1): SelectElement()\n      (2): Dropout(p=0.5, inplace=False)\n      (3): Linear(in_features=512, out_features=1024, bias=False)\n    )\n  )\n  (modality_postprocessors): ModuleDict(\n    (vision): Normalize()\n    (text): Sequential(\n      (0): Normalize()\n      (1): LearnableLogitScaling(logit_scale_init=14.285714285714285,learnable=True, max_logit_scale=100)\n    )\n    (audio): Sequential(\n      (0): Normalize()\n      (1): LearnableLogitScaling(logit_scale_init=20.0,learnable=False, max_logit_scale=100)\n    )\n    (depth): Sequential(\n      (0): Normalize()\n      (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n    )\n    (thermal): Sequential(\n      (0): Normalize()\n      (1): LearnableLogitScaling(logit_scale_init=10.0,learnable=False, max_logit_scale=100)\n    )\n    (imu): Sequential(\n      (0): Normalize()\n      (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"entity_audio_dir = \"/kaggle/input/business-json/entity_aud_files/kaggle/working/entity_aud_files\"\nsentence_audio_dir = '/kaggle/input/business-json/aud_files/kaggle/working/TTS/aud_files'","metadata":{"execution":{"iopub.status.busy":"2024-09-04T18:19:50.026505Z","iopub.execute_input":"2024-09-04T18:19:50.026918Z","iopub.status.idle":"2024-09-04T18:19:50.031890Z","shell.execute_reply.started":"2024-09-04T18:19:50.026852Z","shell.execute_reply":"2024-09-04T18:19:50.031025Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import ast\nsentence_df = pd.read_csv('/kaggle/input/business-json/Test (1).csv') \nentity_df = pd.read_csv('/kaggle/input/business-json/entity_aud_files.csv')\nent_knowledge_df = pd.read_csv('/kaggle/input/business-json/Knowledge.csv')\nsentence_df['entities'] = sentence_df['entities'].apply(lambda x: ast.literal_eval(x))","metadata":{"execution":{"iopub.status.busy":"2024-09-04T18:19:50.033390Z","iopub.execute_input":"2024-09-04T18:19:50.033776Z","iopub.status.idle":"2024-09-04T18:19:50.592812Z","shell.execute_reply.started":"2024-09-04T18:19:50.033717Z","shell.execute_reply":"2024-09-04T18:19:50.591655Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# import os\n# import torch\n# from imagebind.models import imagebind_model\n# from imagebind.models.imagebind_model import ModalityType\n# from imagebind.models.imagebind_model import ImageBindModel\n# from imagebind import data\n# import pandas as pd\n# import ast\n\n\n# entity_audio_paths = sorted([os.path.join(entity_audio_dir, f) for f in os.listdir(entity_audio_dir) if f.endswith('.wav')])\n# sentence_audio_paths = sorted([os.path.join(sentence_audio_dir, f) for f in os.listdir(sentence_audio_dir) if f.endswith('.wav')])\n\n# entity_names = entity_df['entity_name'].tolist()\n\n# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# # Load the data for text (entity names) and audio (entity and sentence audios)\n# text_inputs = data.load_and_transform_text(entity_names, device)\n# entity_audio_inputs = data.load_and_transform_audio_data(entity_audio_paths, device)\n# sentence_audio_inputs = data.load_and_transform_audio_data(sentence_audio_paths, device)\n\n# # Get embeddings\n# with torch.no_grad():\n#     embeddings = model({\n#         ModalityType.TEXT: text_inputs,\n#         ModalityType.AUDIO: torch.cat([entity_audio_inputs, sentence_audio_inputs], dim=0),\n#     })\n\n# # Separate entity and sentence embeddings\n# entity_embeddings = embeddings[ModalityType.TEXT]\n# audio_embeddings = embeddings[ModalityType.AUDIO]\n\n# # Split audio embeddings into entity and sentence embeddings\n# entity_audio_embeddings = audio_embeddings[:len(entity_audio_paths)]\n# sentence_audio_embeddings = audio_embeddings[len(entity_audio_paths):]\n\n# # Compute the ranking based on the formula: audio_features @ entity_text_features.T\n# ranking_matrix = torch.matmul(sentence_audio_embeddings, entity_embeddings.T)\n\n# # Rank entities based on the similarity scores\n# rankings = torch.argsort(ranking_matrix, dim=-1, descending=True)\n\n# # Function to calculate Top-N accuracy\n# def calculate_top_n_accuracy(rankings, sentence_df, entity_df, n=1):\n#     correct = 0\n#     total = len(sentence_df)\n\n#     # Create a map from entity name to entity ID\n#     entity_map = dict(zip(entity_df['entity_name'], entity_df['ID']))\n\n#     # Convert 'entities' column from string to list of dictionaries\n#     sentence_df['entities'] = sentence_df['entities'].apply(lambda x: ast.literal_eval(x))\n\n#     for idx, row in sentence_df.iterrows():\n#         actual_entity_name = row['entities'][0]['mention']  # Get actual entity name\n#         actual_entity_id = entity_map.get(actual_entity_name)  # Get corresponding ID\n#         predicted_ranking = rankings[idx].tolist()  # Get the ranked indices for the sentence\n\n#         # Get the actual entity's position in the ranking\n#         actual_rank = predicted_ranking.index(entity_df[entity_df['ID'] == actual_entity_id].index[0])\n\n#         # Check if the actual entity is within the top-n\n#         if actual_rank < n:\n#             correct += 1\n\n#     return correct / total\n\n# # Calculate Top-1, Top-5, and Top-10 accuracy\n# top1_acc = calculate_top_n_accuracy(rankings, sentence_df, entity_df, n=1)\n# top5_acc = calculate_top_n_accuracy(rankings, sentence_df, entity_df, n=5)\n# top10_acc = calculate_top_n_accuracy(rankings, sentence_df, entity_df, n=10)\n\n# print(f\"Top-1 Accuracy: {top1_acc:.2f}\")\n# print(f\"Top-5 Accuracy: {top5_acc:.2f}\")\n# print(f\"Top-10 Accuracy: {top10_acc:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-04T18:19:50.596362Z","iopub.execute_input":"2024-09-04T18:19:50.596669Z","iopub.status.idle":"2024-09-04T18:19:50.607094Z","shell.execute_reply.started":"2024-09-04T18:19:50.596635Z","shell.execute_reply":"2024-09-04T18:19:50.605998Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# import gc\n# torch.cuda.empty_cache()\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-09-04T18:19:50.608479Z","iopub.execute_input":"2024-09-04T18:19:50.608923Z","iopub.status.idle":"2024-09-04T18:19:50.763078Z","shell.execute_reply.started":"2024-09-04T18:19:50.608861Z","shell.execute_reply":"2024-09-04T18:19:50.761804Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom imagebind.models.imagebind_model import ModalityType, ImageBindModel\nfrom imagebind import data\nimport pandas as pd\nfrom tqdm import tqdm\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Function to process audio files in batches\ndef process_audio_in_batches(audio_paths, text_inputs, batch_size=16):\n    all_embeddings = []\n    txt_embeddings = []\n    for i in tqdm(range(0, len(audio_paths), batch_size), desc=\"Processing Batches\"):\n        batch_audio_paths = audio_paths[i:i + batch_size]\n        audio_inputs = data.load_and_transform_audio_data(batch_audio_paths, device)\n        if i == 0:\n            with torch.no_grad():\n                embeddings = model({\n                    ModalityType.TEXT: text_inputs,\n                    ModalityType.AUDIO: audio_inputs,\n                })\n\n            all_embeddings.append(embeddings[ModalityType.AUDIO])\n\n            txt_embeddings.append(embeddings[ModalityType.TEXT])\n        else:\n            with torch.no_grad():\n                embeddings = model({\n                    ModalityType.AUDIO: audio_inputs,\n                })\n\n            all_embeddings.append(embeddings[ModalityType.AUDIO])\n            \n        # Clear CUDA cache to free up memory\n        torch.cuda.empty_cache()\n    \n    return torch.cat(all_embeddings, dim = 0), txt_embeddings\n\n# Get paths for entity and sentence audio files\nentity_audio_paths = sorted([os.path.join(entity_audio_dir, f) for f in os.listdir(entity_audio_dir) if f.endswith('.wav')])\nsentence_audio_paths = sorted([os.path.join(sentence_audio_dir, f) for f in os.listdir(sentence_audio_dir) if f.endswith('.wav')])\n\nentity_names = entity_df['entity_name'].tolist()\nentity_knowledge = ent_knowledge_df['Knowledge'].tolist()\n# Load the data for text (entity names)\ntext_inputs = data.load_and_transform_text(entity_knowledge, device)\n\n# Process entity audio files in batches\n# entity_audio_embeddings,_ = process_audio_in_batches(entity_audio_paths, text_inputs)\n\n# Process sentence audio files in batches\nsentence_audio_embeddings, entity_embeddings  = process_audio_in_batches(sentence_audio_paths, text_inputs)\n\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-04T18:30:21.235786Z","iopub.execute_input":"2024-09-04T18:30:21.236188Z","iopub.status.idle":"2024-09-04T18:32:44.466599Z","shell.execute_reply.started":"2024-09-04T18:30:21.236151Z","shell.execute_reply":"2024-09-04T18:32:44.465628Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Processing Batches: 100%|██████████| 165/165 [02:22<00:00,  1.16it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Compute the ranking based on the formula: audio_features @ entity_text_features.T\n# ranking_matrix = torch.matmul(sentence_audio_embeddings, entity_audio_embeddings.T)\n\n# # Rank entities based on the similarity scores\n# rankings = torch.argsort(ranking_matrix, dim=-1, descending=True)\n\n# # Function to calculate Top-N accuracy\n# def calculate_top_n_accuracy(rankings, sentence_df, entity_df, n=1):\n#     correct = 0\n#     total = len(sentence_df)\n\n#     # Create a map from entity name to entity ID\n#     entity_map = dict(zip(entity_df['entity_name'], entity_df['ID']))\n\n#     # Convert 'entities' column from string to list of dictionaries\n\n\n#     for idx, row in sentence_df.iterrows():\n#         actual_entity_name = row['entities'][0]['mention']  # Get actual entity name\n#         actual_entity_id = entity_map.get(actual_entity_name)  # Get corresponding ID\n#         predicted_ranking = rankings[idx].tolist()  # Get the ranked indices for the sentence\n\n#         # Get the actual entity's position in the ranking\n#         actual_rank = predicted_ranking.index(entity_df[entity_df['ID'] == actual_entity_id].index[0])\n#         print(actual_rank)\n#         # Check if the actual entity is within the top-n \n#         if actual_rank < n: \n#             correct += 1\n            \n            \n#     return correct / total\n        \n        \n# top1_acc = calculate_top_n_accuracy(rankings, sentence_df, entity_df, n=1) \n# top5_acc = calculate_top_n_accuracy(rankings, sentence_df, entity_df, n=5) \n# top10_acc = calculate_top_n_accuracy(rankings, sentence_df, entity_df, n=10)\n\n# print(f\"Top-1 Accuracy: {top1_acc:.2f}\") \n# print(f\"Top-5 Accuracy: {top5_acc:.2f}\") \n# print(f\"Top-10 Accuracy: {top10_acc:.2f}\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-04T18:21:26.841398Z","iopub.status.idle":"2024-09-04T18:21:26.841775Z","shell.execute_reply.started":"2024-09-04T18:21:26.841588Z","shell.execute_reply":"2024-09-04T18:21:26.841607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Compute the ranking based on the formula: audio_features @ entity_text_features.T\nranking_matrix = torch.matmul(sentence_audio_embeddings, entity_embeddings[0].T)\n\n# Rank entities based on the similarity scores\nrankings = torch.argsort(ranking_matrix, dim=-1, descending=True)\n\n# Function to calculate Top-N accuracy\ndef calculate_top_n_accuracy(rankings, sentence_df, entity_df, n=1):\n    correct = 0\n    total = len(sentence_df)\n\n    # Create a map from entity name to entity ID\n    entity_map = dict(zip(entity_df['entity_name'], entity_df['ID']))\n\n    for idx, row in sentence_df.iterrows():\n        actual_entity_name = row['entities'][0]['mention']  # Get actual entity name\n        actual_entity_id = entity_map.get(actual_entity_name)  # Get corresponding ID\n        predicted_ranking = rankings[idx].tolist()  # Get the ranked indices for the sentence\n\n        # Get the actual entity's position in the ranking\n        actual_rank = predicted_ranking.index(ent_knowledge_df[entity_df['ID'] == actual_entity_id].index[0])\n\n        # Check if the actual entity is within the top-n\n        if actual_rank < n:\n            correct += 1\n\n    return correct / total\n\n# Calculate Top-1, Top-5, and Top-10 accuracy\ntop1_acc = calculate_top_n_accuracy(rankings, sentence_df, entity_df, n=1)\ntop5_acc = calculate_top_n_accuracy(rankings, sentence_df, entity_df, n=5)\ntop10_acc = calculate_top_n_accuracy(rankings, sentence_df, entity_df, n=10)\n\nprint(f\"Top-1 Accuracy: {top1_acc:.2f}\")\nprint(f\"Top-5 Accuracy: {top5_acc:.2f}\")\nprint(f\"Top-10 Accuracy: {top10_acc:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T18:32:44.468897Z","iopub.execute_input":"2024-09-04T18:32:44.469324Z","iopub.status.idle":"2024-09-04T18:32:49.635131Z","shell.execute_reply.started":"2024-09-04T18:32:44.469276Z","shell.execute_reply":"2024-09-04T18:32:49.634137Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Top-1 Accuracy: 0.00\nTop-5 Accuracy: 0.01\nTop-10 Accuracy: 0.02\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}